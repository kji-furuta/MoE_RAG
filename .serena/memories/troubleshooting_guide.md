# ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚¬ã‚¤ãƒ‰

## ðŸš¨ ã‚ˆãã‚ã‚‹å•é¡Œã¨è§£æ±ºæ–¹æ³•

### 1. GPUé–¢é€£ã®å•é¡Œ

#### CUDA out of memory ã‚¨ãƒ©ãƒ¼
```bash
# ç—‡çŠ¶
RuntimeError: CUDA out of memory. Tried to allocate X GB

# è§£æ±ºæ–¹æ³•
# 1. GPUãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢
docker exec ai-ft-container python -c "import torch; torch.cuda.empty_cache()"

# 2. ä»–ã®ãƒ—ãƒ­ã‚»ã‚¹ã‚’ç¢ºèªã—ã¦çµ‚äº†
nvidia-smi
kill -9 [PID]

# 3. ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’æ¸›ã‚‰ã™
# configs/model_config.yaml ã§ batch_size ã‚’èª¿æ•´

# 4. gradient_checkpointing ã‚’æœ‰åŠ¹åŒ–
# training_args ã« gradient_checkpointing=True ã‚’è¿½åŠ 

# 5. é‡å­åŒ–ã‚’ä½¿ç”¨
# QLoRA (4bit) ã¾ãŸã¯ 8bit é‡å­åŒ–ã‚’ä½¿ç”¨
```

#### GPUãŒèªè­˜ã•ã‚Œãªã„
```bash
# ç¢ºèªã‚³ãƒžãƒ³ãƒ‰
docker exec ai-ft-container nvidia-smi

# è§£æ±ºæ–¹æ³•
# 1. Docker ã® GPU ã‚µãƒãƒ¼ãƒˆç¢ºèª
docker run --rm --gpus all nvidia/cuda:12.6.0-base-ubuntu22.04 nvidia-smi

# 2. nvidia-container-runtime ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
sudo apt-get update
sudo apt-get install -y nvidia-container-runtime

# 3. Docker daemon å†èµ·å‹•
sudo systemctl restart docker
```

### 2. ãƒ¡ãƒ¢ãƒªä¸è¶³

#### ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒ¢ãƒªä¸è¶³
```bash
# ç—‡çŠ¶
MemoryError ã¾ãŸã¯ Killed ãƒ—ãƒ­ã‚»ã‚¹

# è§£æ±ºæ–¹æ³•
# 1. ã‚¹ãƒ¯ãƒƒãƒ—è¿½åŠ 
sudo dd if=/dev/zero of=/swapfile bs=1G count=32
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile

# 2. Docker ãƒ¡ãƒ¢ãƒªåˆ¶é™ã®ç¢ºèª
docker update ai-ft-container --memory="32g" --memory-swap="-1"

# 3. ä¸è¦ãªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢
echo 3 | sudo tee /proc/sys/vm/drop_caches
```

### 3. ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼

#### ModuleNotFoundError
```bash
# ç—‡çŠ¶
ModuleNotFoundError: No module named 'XXX'

# è§£æ±ºæ–¹æ³•
# 1. ãƒ‘ã‚¹ã®ç¢ºèª
docker exec ai-ft-container python -c "import sys; print('\n'.join(sys.path))"

# 2. å¿…è¦ãªãƒ‘ã‚¹ã‚’è¿½åŠ 
docker exec ai-ft-container bash -c "export PYTHONPATH=/workspace:$PYTHONPATH"

# 3. ä¾å­˜é–¢ä¿‚ã®å†ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
docker exec ai-ft-container pip install -r requirements.txt --force-reinstall

# 4. ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢
docker exec ai-ft-container find . -type d -name __pycache__ -exec rm -r {} +
```

### 4. ãƒãƒ¼ãƒˆç«¶åˆ

#### Address already in use
```bash
# ç—‡çŠ¶
[Errno 98] Address already in use

# è§£æ±ºæ–¹æ³•
# 1. ä½¿ç”¨ä¸­ã®ãƒ—ãƒ­ã‚»ã‚¹ã‚’ç‰¹å®š
sudo lsof -i :8050
sudo netstat -tlnp | grep 8050

# 2. ãƒ—ãƒ­ã‚»ã‚¹ã‚’çµ‚äº†
sudo fuser -k 8050/tcp

# 3. Docker ã‚³ãƒ³ãƒ†ãƒŠã®ç¢ºèª
docker ps | grep 8050
docker stop [CONTAINER_ID]
```

### 5. QdrantæŽ¥ç¶šã‚¨ãƒ©ãƒ¼

#### Connection refused to Qdrant
```bash
# ç—‡çŠ¶
Failed to connect to Qdrant at localhost:6333

# è§£æ±ºæ–¹æ³•
# 1. Qdrant ã‚³ãƒ³ãƒ†ãƒŠã®çŠ¶æ…‹ç¢ºèª
docker ps | grep qdrant
docker logs qdrant-container

# 2. Qdrant å†èµ·å‹•
docker restart qdrant-container

# 3. ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç¢ºèª
docker network ls
docker network inspect ai-ft-network

# 4. ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
curl http://localhost:6333/collections
```

### 6. ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼

#### HuggingFace ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼
```bash
# ç—‡çŠ¶
OSError: Can't load model from 'model_name'

# è§£æ±ºæ–¹æ³•
# 1. HF_TOKEN ã®ç¢ºèª
docker exec ai-ft-container env | grep HF_TOKEN

# 2. ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç¢ºèª
docker exec ai-ft-container ls -la /workspace/hf_cache

# 3. æ‰‹å‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
docker exec ai-ft-container python -c "
from transformers import AutoModel
model = AutoModel.from_pretrained('model_name', cache_dir='/workspace/hf_cache')
"

# 4. ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ¼ãƒ‰ã®è¨­å®š
export TRANSFORMERS_OFFLINE=1
```

### 7. è¨“ç·´ãŒé€²ã¾ãªã„

#### Loss ãŒä¸‹ãŒã‚‰ãªã„
```bash
# ç¢ºèªäº‹é …
# 1. å­¦ç¿’çŽ‡ã®ç¢ºèª
grep learning_rate configs/*.yaml

# 2. ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª
head -n 5 data/train.jsonl

# 3. ãƒ¢ãƒ‡ãƒ«ã®å‹¾é…ç¢ºèª
# training_utils.py ã«ä»¥ä¸‹ã‚’è¿½åŠ :
for name, param in model.named_parameters():
    if param.grad is not None:
        print(f"{name}: {param.grad.norm().item()}")

# è§£æ±ºæ–¹æ³•
# - å­¦ç¿’çŽ‡ã‚’èª¿æ•´ (1e-5 ã€œ 1e-3)
# - warmup_steps ã‚’å¢—ã‚„ã™
# - ãƒ‡ãƒ¼ã‚¿ã®å“è³ªã‚’ç¢ºèª
```

### 8. Docker ãƒ“ãƒ«ãƒ‰ã‚¨ãƒ©ãƒ¼

#### ã‚¤ãƒ¡ãƒ¼ã‚¸ãƒ“ãƒ«ãƒ‰å¤±æ•—
```bash
# ç—‡çŠ¶
docker build failed

# è§£æ±ºæ–¹æ³•
# 1. ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãªã—ã§ãƒ“ãƒ«ãƒ‰
docker build --no-cache -t ai-ft-image .

# 2. ãƒ“ãƒ«ãƒ‰ãƒ­ã‚°ã®è©³ç´°ç¢ºèª
docker build --progress=plain -t ai-ft-image .

# 3. Docker ã‚·ã‚¹ãƒ†ãƒ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
docker system prune -a --volumes

# 4. ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ç¢ºèª
df -h
docker system df
```

### 9. WebSocket æŽ¥ç¶šã‚¨ãƒ©ãƒ¼

#### WebSocket connection failed
```bash
# ç—‡çŠ¶
WebSocket connection to ws://localhost:8050/ws/... failed

# è§£æ±ºæ–¹æ³•
# 1. nginx/ãƒ—ãƒ­ã‚­ã‚·è¨­å®šç¢ºèª
# WebSocket ã®ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’è¨±å¯

# 2. CORS è¨­å®šç¢ºèª
# main_unified.py ã§ origins ã‚’ç¢ºèª

# 3. ãƒ•ã‚¡ã‚¤ã‚¢ã‚¦ã‚©ãƒ¼ãƒ«ç¢ºèª
sudo ufw status
sudo ufw allow 8050
```

### 10. ãƒ­ã‚°é–¢é€£ã®å•é¡Œ

#### ãƒ­ã‚°ãŒå‡ºåŠ›ã•ã‚Œãªã„
```bash
# è§£æ±ºæ–¹æ³•
# 1. ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã®ç¢ºèª
docker exec ai-ft-container python -c "
import logging
print(logging.getLogger().level)
"

# 2. ãƒ­ã‚°è¨­å®šã®ç¢ºèª
grep -r "logging" src/utils/logger.py

# 3. æ¨™æº–ã‚¨ãƒ©ãƒ¼å‡ºåŠ›ã®ç¢ºèª
docker logs ai-ft-container 2>&1

# 4. ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®æ¨©é™ç¢ºèª
docker exec ai-ft-container ls -la logs/
```

## ðŸ” ãƒ‡ãƒãƒƒã‚°ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯

### 1. ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ‡ãƒãƒƒã‚°
```python
# ã‚³ãƒ¼ãƒ‰ã«è¿½åŠ 
import pdb; pdb.set_trace()

# ã¾ãŸã¯
import ipdb; ipdb.set_trace()  # ã‚ˆã‚Šé«˜æ©Ÿèƒ½
```

### 2. ãƒªãƒ¢ãƒ¼ãƒˆãƒ‡ãƒãƒƒã‚°ï¼ˆVSCodeï¼‰
```json
// .vscode/launch.json
{
    "version": "0.2.0",
    "configurations": [
        {
            "name": "Python: Remote Attach",
            "type": "python",
            "request": "attach",
            "connect": {
                "host": "localhost",
                "port": 5678
            },
            "pathMappings": [
                {
                    "localRoot": "${workspaceFolder}",
                    "remoteRoot": "/workspace"
                }
            ]
        }
    ]
}
```

### 3. ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°
```python
# ãƒ¡ãƒ¢ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°
from memory_profiler import profile

@profile
def memory_intensive_function():
    # å‡¦ç†

# æ™‚é–“ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°
import cProfile
cProfile.run('train_model()')
```

## ðŸ“ž ã‚µãƒãƒ¼ãƒˆæƒ…å ±

### ãƒ­ã‚°åŽé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
```bash
#!/bin/bash
# collect_debug_info.sh

echo "=== System Info ===" > debug_report.txt
uname -a >> debug_report.txt
echo -e "\n=== Docker Info ===" >> debug_report.txt
docker version >> debug_report.txt
echo -e "\n=== GPU Info ===" >> debug_report.txt
nvidia-smi >> debug_report.txt
echo -e "\n=== Container Logs ===" >> debug_report.txt
docker logs ai-ft-container --tail 1000 >> debug_report.txt
echo -e "\n=== Python Packages ===" >> debug_report.txt
docker exec ai-ft-container pip freeze >> debug_report.txt

echo "Debug report saved to debug_report.txt"
```