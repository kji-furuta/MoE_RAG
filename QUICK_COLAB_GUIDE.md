# Google Colabã§ã®å¤‰æ›æ‰‹é †ï¼ˆãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ç‰ˆï¼‰

## âš ï¸ ãƒ¡ãƒ¢ãƒªè¦ä»¶ã¨åˆ¶é™

### GPU ãƒ¡ãƒ¢ãƒªè¦ä»¶
- **DeepSeek-R1-Distill-Qwen-32B**: 
  - FP16: ç´„64GBå¿…è¦
  - INT8: ç´„32GBå¿…è¦
  - INT4: ç´„16-20GBå¿…è¦
- **Google Colab A100 (40GB)**: INT4é‡å­åŒ–ã§å‹•ä½œå¯èƒ½

### æ¨å¥¨ç’°å¢ƒ
- Google Colab Pro+ (A100 40GB)
- å®Ÿè¡Œæ™‚é–“: ç´„1-2æ™‚é–“
- Google Driveç©ºãå®¹é‡: 50GBä»¥ä¸Š

## ğŸ“‹ äº‹å‰æº–å‚™ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ã§å®Ÿè¡Œï¼‰

### 1. LoRAã‚¢ãƒ€ãƒ—ã‚¿ã®åœ§ç¸®
```bash
cd /home/kjifu/MoE_RAG/outputs
tar -czf lora_20250830_223432.tar.gz lora_20250830_223432/
# ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: ç´„833MB
```

### 2. Google Driveã¸ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
1. [Google Drive](https://drive.google.com) ã‚’é–‹ã
2. ã€Œãƒã‚¤ãƒ‰ãƒ©ã‚¤ãƒ–ã€ã®ãƒ«ãƒ¼ãƒˆã« `lora_20250830_223432.tar.gz` ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
3. ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†ã‚’ç¢ºèªï¼ˆç´„10åˆ†ï¼‰

## ğŸš€ Google Colabã§ã®å®Ÿè¡Œ

### Step 1: æ–°ã—ã„ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ä½œæˆ
1. [Google Colab](https://colab.research.google.com/) ã«ã‚¢ã‚¯ã‚»ã‚¹
2. ã€Œæ–°ã—ã„ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã€ã‚’ã‚¯ãƒªãƒƒã‚¯
3. **é‡è¦**: ãƒ©ãƒ³ã‚¿ã‚¤ãƒ  â†’ ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã®ã‚¿ã‚¤ãƒ—ã‚’å¤‰æ›´ â†’ **A100 GPU** ã‚’é¸æŠ

### Step 2: å¤‰æ›ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å®Ÿè¡Œ

ä»¥ä¸‹ã®ã‚»ãƒ«ã‚’é †ç•ªã«å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼š

#### ã‚»ãƒ«1: GPUç¢ºèªã¨ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
```python
# GPUç¢ºèª
!nvidia-smi

# å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆ4bité‡å­åŒ–å¯¾å¿œç‰ˆï¼‰
!pip install -q transformers==4.44.0 peft accelerate bitsandbytes
!pip install -q sentencepiece protobuf
!pip install -q auto-gptq optimum  # 4bité‡å­åŒ–ç”¨

# llama.cppã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
!git clone https://github.com/ggerganov/llama.cpp
!cd llama.cpp && make clean && make LLAMA_CUDA=1 -j8
```

#### ã‚»ãƒ«2: Google Driveãƒã‚¦ãƒ³ãƒˆã¨ãƒ•ã‚¡ã‚¤ãƒ«æº–å‚™
```python
from google.colab import drive
import os
import torch
import gc

# Google Driveã‚’ãƒã‚¦ãƒ³ãƒˆ
drive.mount('/content/drive')

# LoRAã‚¢ãƒ€ãƒ—ã‚¿ã‚’è§£å‡
!tar -xzf /content/drive/MyDrive/lora_20250830_223432.tar.gz -C /content/
print("LoRA adapter extracted to /content/lora_20250830_223432")
```

#### ã‚»ãƒ«3: ãƒ¢ãƒ‡ãƒ«ã®ãƒãƒ¼ã‚¸ï¼ˆ4bité‡å­åŒ–ç‰ˆï¼‰
```python
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel
import torch
import gc

# ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
torch.cuda.empty_cache()
gc.collect()

# 4bité‡å­åŒ–è¨­å®š
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

print("Loading base model with 4-bit quantization...")
print("Expected memory usage: ~16-20GB")
try:
    base_model = AutoModelForCausalLM.from_pretrained(
        "cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese",
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True,
        low_cpu_mem_usage=True,
        max_memory={0: "39GB"}  # A100ã®æœ€å¤§ãƒ¡ãƒ¢ãƒªã‚’æŒ‡å®š
    )
    print("âœ… Base model loaded successfully with 4-bit quantization")
except Exception as e:
    print(f"âŒ Error loading model: {e}")
    print("Trying alternative approach...")
    # ä»£æ›¿ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼šCPU offloadingã‚’ä½¿ç”¨
    base_model = AutoModelForCausalLM.from_pretrained(
        "cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese",
        load_in_4bit=True,
        device_map="auto",
        offload_folder="/content/offload",
        trust_remote_code=True
    )

print("Loading LoRA adapter...")
model = PeftModel.from_pretrained(
    base_model,
    "/content/lora_20250830_223432",
    torch_dtype=torch.float16
)

print("Merging LoRA with base model...")
print("Note: This may take 20-30 minutes with 4-bit quantization")
model = model.merge_and_unload()

# é‡å­åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã™ã‚‹å‰ã«ã€FP16ã«å¤‰æ›
print("Converting to FP16 for saving...")
model = model.to(torch.float16)

print("Saving merged model...")
model.save_pretrained(
    "/content/merged_model",
    torch_dtype=torch.float16,
    safe_serialization=True,
    max_shard_size="2GB"
)

# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã‚‚ä¿å­˜
tokenizer = AutoTokenizer.from_pretrained(
    "cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese",
    trust_remote_code=True
)
tokenizer.save_pretrained("/content/merged_model")

# ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
del model, base_model
torch.cuda.empty_cache()
gc.collect()

print("âœ… Merge complete!")
```

#### ã‚»ãƒ«4: GGUFå¤‰æ›ã¨é‡å­åŒ–
```python
# GGUFå½¢å¼ã«å¤‰æ›
!cd llama.cpp && python convert-hf-to-gguf.py \
    /content/merged_model \
    --outfile /content/model-f16.gguf \
    --outtype f16

print("âœ… GGUF conversion complete!")

# Q4_K_Mé‡å­åŒ–ï¼ˆæ¨å¥¨ï¼‰
!cd llama.cpp && ./quantize \
    /content/model-f16.gguf \
    /content/deepseek-finetuned-q4_k_m.gguf \
    Q4_K_M

import os
size_gb = os.path.getsize("/content/deepseek-finetuned-q4_k_m.gguf") / (1024**3)
print(f"âœ… Quantization complete! File size: {size_gb:.2f} GB")
```

#### ã‚»ãƒ«5: Modelfileä½œæˆã¨ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
```python
# Modelfileä½œæˆ
modelfile_content = """FROM ./deepseek-finetuned-q4_k_m.gguf

PARAMETER temperature 0.1
PARAMETER top_p 0.95
PARAMETER top_k 40
PARAMETER repeat_penalty 1.1
PARAMETER num_predict 2048
PARAMETER stop "<|endoftext|>"
PARAMETER stop "</s>"
PARAMETER stop "<|im_end|>"

SYSTEM "ã‚ãªãŸã¯æ—¥æœ¬ã®é“è·¯è¨­è¨ˆã®å°‚é–€å®¶ã§ã™ã€‚é“è·¯æ§‹é€ ä»¤ã¨è¨­è¨ˆåŸºæº–ã«åŸºã¥ã„ã¦æ­£ç¢ºãªæŠ€è¡“çš„å›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚"

TEMPLATE """{{ if .System }}<|im_start|>system
{{ .System }}<|im_end|>
{{ end }}{{ if .Prompt }}<|im_start|>user
{{ .Prompt }}<|im_end|>
<|im_start|>assistant
{{ end }}"""
"""

with open("/content/Modelfile", "w") as f:
    f.write(modelfile_content)

# Google Driveã«ä¿å­˜
import shutil
output_dir = "/content/drive/MyDrive/deepseek_gguf_output"
os.makedirs(output_dir, exist_ok=True)

print("Copying files to Google Drive...")
shutil.copy("/content/deepseek-finetuned-q4_k_m.gguf", output_dir)
shutil.copy("/content/Modelfile", output_dir)

print(f"âœ… Files saved to Google Drive: {output_dir}")
print(f"   - deepseek-finetuned-q4_k_m.gguf ({size_gb:.2f} GB)")
print(f"   - Modelfile")
```

## ğŸ“¥ ãƒ­ãƒ¼ã‚«ãƒ«ã¸ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¨è¨­å®š

### 1. Google Driveã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
1. Google Driveã® `deepseek_gguf_output` ãƒ•ã‚©ãƒ«ãƒ€ã‚’é–‹ã
2. ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼š
   - `deepseek-finetuned-q4_k_m.gguf` (ç´„18-20GB)
   - `Modelfile`

### 2. WSL2ã¸ã®è»¢é€
```bash
# Windowsã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ«ãƒ€ã‹ã‚‰è»¢é€
cp /mnt/c/Users/[ãƒ¦ãƒ¼ã‚¶ãƒ¼å]/Downloads/deepseek-finetuned-q4_k_m.gguf ~/
cp /mnt/c/Users/[ãƒ¦ãƒ¼ã‚¶ãƒ¼å]/Downloads/Modelfile ~/
```

### 3. Dockerã‚³ãƒ³ãƒ†ãƒŠã¸ã‚³ãƒ”ãƒ¼
```bash
docker cp ~/deepseek-finetuned-q4_k_m.gguf ai-ft-container:/workspace/
docker cp ~/Modelfile ai-ft-container:/workspace/
```

### 4. Ollamaã«ç™»éŒ²
```bash
docker exec ai-ft-container ollama create deepseek-finetuned -f /workspace/Modelfile
```

### 5. å‹•ä½œç¢ºèª
```bash
# Ollamaã§ç›´æ¥ãƒ†ã‚¹ãƒˆ
docker exec ai-ft-container ollama run deepseek-finetuned "è¨­è¨ˆé€Ÿåº¦100km/hã®æœ€å°æ›²ç·šåŠå¾„ã¯ï¼Ÿ"
# æœŸå¾…ã•ã‚Œã‚‹å›ç­”: 460m

# RAGã‚·ã‚¹ãƒ†ãƒ ã§ãƒ†ã‚¹ãƒˆ
curl -X POST "http://localhost:8050/rag/query" \
     -H "Content-Type: application/json" \
     -d '{"query": "è¨­è¨ˆé€Ÿåº¦100km/hã®æœ€å°æ›²ç·šåŠå¾„ã¯ï¼Ÿ"}'
```

### 6. RAGè¨­å®šæ›´æ–°
```yaml
# src/rag/config/rag_config.yaml
llm:
  use_ollama_fallback: true
  ollama_model: deepseek-finetuned  # å¤‰æ›´
  ollama_host: http://localhost:11434
```

## â±ï¸ å‡¦ç†æ™‚é–“ã®ç›®å®‰
- GPUç¢ºèªã¨ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—: 5åˆ†
- ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ï¼ˆ4bité‡å­åŒ–ï¼‰: 15-20åˆ†
- LoRAãƒãƒ¼ã‚¸: 20-30åˆ†
- GGUFå¤‰æ›: 10-15åˆ†
- é‡å­åŒ–: 10-15åˆ†
- **åˆè¨ˆ: ç´„60-85åˆ†**

## ğŸ’° ã‚³ã‚¹ãƒˆ
- Google Colab Pro+: $49.99/æœˆï¼ˆA100ä½¿ç”¨å¯èƒ½ï¼‰
- 1å›ã®å¤‰æ›ã§ç´„1-2æ™‚é–“ã®GPUä½¿ç”¨

## âš ï¸ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ãƒ¡ãƒ¢ãƒªä¸è¶³ã‚¨ãƒ©ãƒ¼ãŒå‡ºãŸå ´åˆ

#### ã‚ªãƒ—ã‚·ãƒ§ãƒ³1: ã‚ˆã‚Šç©æ¥µçš„ãªé‡å­åŒ–
```python
# 4bité‡å­åŒ–ã§ã‚‚ãƒ¡ãƒ¢ãƒªä¸è¶³ã®å ´åˆ
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,  # ãƒ€ãƒ–ãƒ«é‡å­åŒ–ã‚’æœ‰åŠ¹åŒ–
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16  # bfloat16ã®ä»£ã‚ã‚Šã«float16ã‚’ä½¿ç”¨
)
```

#### ã‚ªãƒ—ã‚·ãƒ§ãƒ³2: ãƒãƒƒãƒå‡¦ç†ã§ãƒãƒ¼ã‚¸
```python
# ãƒ¬ã‚¤ãƒ¤ãƒ¼ã”ã¨ã«ãƒãƒ¼ã‚¸ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ã ãŒæ™‚é–“ãŒã‹ã‹ã‚‹ï¼‰
import gc
import torch
from tqdm import tqdm

# LoRAã‚¢ãƒ€ãƒ—ã‚¿ã‚’ãƒ¬ã‚¤ãƒ¤ãƒ¼ã”ã¨ã«ãƒãƒ¼ã‚¸
for name, module in tqdm(model.named_modules()):
    if hasattr(module, 'merge_and_unload'):
        module.merge_and_unload()
        torch.cuda.empty_cache()
        gc.collect()
```

#### ã‚ªãƒ—ã‚·ãƒ§ãƒ³3: CPUã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ä½¿ç”¨
```python
base_model = AutoModelForCausalLM.from_pretrained(
    "cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese",
    load_in_4bit=True,
    device_map="auto",
    offload_folder="/content/offload",
    offload_state_dict=True,
    trust_remote_code=True
)
```

### ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒã‚¯ãƒ©ãƒƒã‚·ãƒ¥ã—ãŸå ´åˆ
```python
# ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ãƒªã‚¹ã‚¿ãƒ¼ãƒˆå¾Œã€ä»¥ä¸‹ã‚’å®Ÿè¡Œ
import os
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'
import gc
import torch
torch.cuda.empty_cache()
gc.collect()
```

### llama.cpp ã®ãƒ“ãƒ«ãƒ‰ã‚¨ãƒ©ãƒ¼
```bash
# CUDAãªã—ã§ãƒ“ãƒ«ãƒ‰
!cd llama.cpp && make clean && make
```

### å¤‰æ›ãŒé€”ä¸­ã§æ­¢ã¾ã£ãŸå ´åˆ
- ãƒ©ãƒ³ã‚¿ã‚¤ãƒ  â†’ ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ç®¡ç† â†’ çµ‚äº†
- æ–°ã—ã„ã‚»ãƒƒã‚·ãƒ§ãƒ³ã§æœ€åˆã‹ã‚‰ã‚„ã‚Šç›´ã—

## ğŸ“ é‡è¦ãªæ³¨æ„äº‹é …
1. **å¿…ãšA100 GPUã‚’é¸æŠ**ã—ã¦ãã ã•ã„ï¼ˆç„¡æ–™ç‰ˆã§ã¯ä¸å¯ï¼‰
2. **Google Driveã®å®¹é‡**ã‚’äº‹å‰ã«ç¢ºèªï¼ˆ40GBä»¥ä¸Šå¿…è¦ï¼‰
3. **ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ**ã«æ³¨æ„ï¼ˆ90åˆ†åˆ¶é™ãŒã‚ã‚‹å ´åˆã‚ã‚Šï¼‰
4. **ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¯åˆ†å‰²**ã—ã¦è¡Œã†ã“ã¨ã‚‚å¯èƒ½