#!/usr/bin/env python3
"""
AI Fine-tuning Toolkit Web API - Unified Implementation
çµ±åˆã•ã‚ŒãŸWebã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹å®Ÿè£…
"""

from fastapi import FastAPI, HTTPException, UploadFile, File, BackgroundTasks, Query, WebSocket, Form
from fastapi.staticfiles import StaticFiles
from fastapi.responses import HTMLResponse, PlainTextResponse, JSONResponse, StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
import json
import os
import uuid
from pathlib import Path
import logging
import torch
import psutil
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer
from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training
import yaml
from datetime import datetime, timezone, timedelta

# æ—¥æœ¬æ™‚é–“ï¼ˆJSTï¼‰ã®è¨­å®š
JST = timezone(timedelta(hours=9))
import asyncio
from concurrent.futures import ThreadPoolExecutor
import traceback
from typing import Optional
import sys
import time
from pathlib import Path as PathlibPath

# Import model utilities
from app.model_utils import (
    load_model_and_tokenizer,
    handle_model_loading_error,
    get_output_directory,
    load_training_config,
    create_quantization_config,
    load_tokenizer,
    get_device_map
)
import io
import random

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# RAG system imports
try:
    sys.path.insert(0, str(PathlibPath(__file__).parent.parent))
    from loguru import logger as rag_logger
    from src.rag.core.query_engine import RoadDesignQueryEngine, QueryResult
    from src.rag.indexing.metadata_manager import MetadataManager
    RAG_AVAILABLE = True
    logger.info("RAG system components loaded successfully")
except ImportError as e:
    RAG_AVAILABLE = False
    logger.warning(f"RAG system not available: {e}")

# Prometheusãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from app.monitoring import metrics_collector, get_prometheus_metrics
    logger.info("Prometheusãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã—ãŸ")
    METRICS_AVAILABLE = True
except Exception as e:
    logger.warning(f"ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—: {e}")
    metrics_collector = None
    METRICS_AVAILABLE = False

# FastAPIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³åˆæœŸåŒ–
app = FastAPI(
    title="AI Fine-tuning Toolkit",
    description="æ—¥æœ¬èªLLMãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨Webã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹",
    version="2.0.0"
)

# CORSè¨­å®šï¼ˆæœ¬ç•ªç’°å¢ƒã§ã¯åˆ¶é™ã™ã¹ãï¼‰
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:8050", "http://127.0.0.1:8050"],  # åˆ¶é™ã‚’è¿½åŠ 
    allow_credentials=True,
    allow_methods=["GET", "POST"],
    allow_headers=["*"],
)

# é™çš„ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å‹•çš„æ¤œå‡º
def find_static_directory():
    """é™çš„ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ¤œç´¢"""
    current_file = Path(__file__)
    static_path = current_file.parent / "static"
    
    if static_path.exists() and static_path.is_dir():
        return str(static_path)
    
    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‹ã‚‰ã®ç›¸å¯¾ãƒ‘ã‚¹
    project_root = Path(os.getcwd())
    possible_paths = [
        project_root / "static",  # å„ªå…ˆåº¦ã‚’ä¸Šã’ã‚‹
        project_root / "app" / "static",
        project_root / "static"
    ]
    
    for path in possible_paths:
        if path.exists() and path.is_dir():
            return str(path)
    
    return str(project_root / "static")  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’/workspace/staticã«å¤‰æ›´

static_dir = find_static_directory()
print(f"Using static directory: {static_dir}")

# é™çš„ãƒ•ã‚¡ã‚¤ãƒ«ã®è¨­å®š
app.mount("/static", StaticFiles(directory=static_dir), name="static")

# ç¶™ç¶šå­¦ç¿’ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from app.continual_learning.continual_learning_ui import create_continual_learning_router, websocket_endpoint
    continual_learning_router = create_continual_learning_router()
    app.include_router(continual_learning_router)
    logger.info("ç¶™ç¶šå­¦ç¿’ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’æ­£å¸¸ã«ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ")
except Exception as e:
    logger.warning(f"ç¶™ç¶šå­¦ç¿’ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã‚’ã‚¹ã‚­ãƒƒãƒ—: {str(e)}")
    # ç¶™ç¶šå­¦ç¿’ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒèª­ã¿è¾¼ã‚ãªã„å ´åˆã¯ã€åŸºæœ¬çš„ãªAPIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’ç›´æ¥å®šç¾©
    pass

# ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«å®šç¾©
class ModelInfo(BaseModel):
    name: str
    description: str
    size: str
    status: str

class TrainingRequest(BaseModel):
    model_name: str
    training_data: List[str]
    training_method: str = "lora"  # lora, qlora, full
    lora_config: Dict[str, Any]
    training_config: Dict[str, Any]

class GenerationRequest(BaseModel):
    model_path: str
    prompt: str
    max_length: int = 2048
    temperature: float = 0.7
    top_p: float = 0.9

class TrainingStatus(BaseModel):
    task_id: str
    status: str
    progress: float
    message: str
    model_path: Optional[str] = None

# RAG-specific data models
class QueryRequest(BaseModel):
    """RAGã‚¯ã‚¨ãƒªãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""
    query: str = Field(..., description="æ¤œç´¢ã‚¯ã‚¨ãƒª")
    top_k: int = Field(5, description="å–å¾—ã™ã‚‹çµæœæ•°", ge=1, le=20)
    search_type: str = Field("hybrid", description="æ¤œç´¢ã‚¿ã‚¤ãƒ—", pattern="^(hybrid|vector|keyword)$")
    include_sources: bool = Field(True, description="ã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’å«ã‚ã‚‹ã‹")
    filters: Optional[Dict[str, Any]] = Field(None, description="æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼")

class QueryResponse(BaseModel):
    """RAGã‚¯ã‚¨ãƒªãƒ¬ã‚¹ãƒãƒ³ã‚¹"""
    query: str
    answer: str
    citations: List[Dict[str, Any]]
    sources: List[Dict[str, Any]]
    confidence_score: float
    processing_time: float
    metadata: Dict[str, Any]

class BatchQueryRequest(BaseModel):
    """ãƒãƒƒãƒã‚¯ã‚¨ãƒªãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""
    queries: List[str] = Field(..., description="ã‚¯ã‚¨ãƒªãƒªã‚¹ãƒˆ")
    top_k: int = Field(5, description="å–å¾—ã™ã‚‹çµæœæ•°", ge=1, le=20)
    search_type: str = Field("hybrid", description="æ¤œç´¢ã‚¿ã‚¤ãƒ—", pattern="^(hybrid|vector|keyword)$")

class SystemInfoResponse(BaseModel):
    """ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ãƒ¬ã‚¹ãƒãƒ³ã‚¹"""
    status: str
    system_info: Dict[str, Any]
    timestamp: str

class DocumentUploadResponse(BaseModel):
    """æ–‡æ›¸ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ¬ã‚¹ãƒãƒ³ã‚¹"""
    status: str
    message: str
    document_id: Optional[str] = None
    processing_status: str
    metadata: Optional[Dict[str, Any]] = None

class SavedSearchResult(BaseModel):
    """ä¿å­˜ã•ã‚ŒãŸæ¤œç´¢çµæœ"""
    id: str
    query: str
    answer: str
    citations: List[Dict[str, Any]]
    sources: List[Dict[str, Any]]
    confidence_score: float
    search_type: str
    top_k: int
    saved_at: str
    metadata: Optional[Dict[str, Any]] = None

class SaveSearchRequest(BaseModel):
    """æ¤œç´¢çµæœä¿å­˜ãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""
    query_response: QueryResponse
    name: Optional[str] = Field(None, description="ä¿å­˜å")
    tags: Optional[List[str]] = Field(None, description="ã‚¿ã‚°")

class SearchHistoryResponse(BaseModel):
    """æ¤œç´¢å±¥æ­´ãƒ¬ã‚¹ãƒãƒ³ã‚¹"""
    total: int
    results: List[SavedSearchResult]
    page: int
    limit: int

# ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚¨ãƒ³ã‚¸ãƒ³ã®è¨­å®š
from fastapi.templating import Jinja2Templates
from fastapi.staticfiles import StaticFiles
from starlette.requests import Request

templates = Jinja2Templates(directory="templates")

# RAG Application Class
class RAGApplication:
    """RAGã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³"""
    
    def __init__(self):
        self.query_engine: Optional[RoadDesignQueryEngine] = None
        self.metadata_manager: Optional[MetadataManager] = None
        self.is_initialized = False
        self.initialization_error = None
        # Dockerç’°å¢ƒã«å¯¾å¿œã—ãŸæ°¸ç¶šåŒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®è¨­å®š
        if os.path.exists("/workspace"):
            # Dockerç’°å¢ƒå†…
            self.search_history_dir = Path("/workspace/data/search_history")
        else:
            # ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒ
            project_root = Path(__file__).parent.parent
            self.search_history_dir = project_root / "data" / "search_history"
        
        self.search_history_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"Search history directory: {self.search_history_dir}")
        
    async def initialize(self):
        """éåŒæœŸã§ã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–"""
        if not RAG_AVAILABLE:
            self.initialization_error = "RAG system components not available"
            return
            
        try:
            logger.info("Initializing RAG system...")
            
            # ã‚¯ã‚¨ãƒªã‚¨ãƒ³ã‚¸ãƒ³ã®åˆæœŸåŒ–
            self.query_engine = RoadDesignQueryEngine()
            await asyncio.get_event_loop().run_in_executor(
                None, self.query_engine.initialize
            )
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®åˆæœŸåŒ–
            self.metadata_manager = MetadataManager()
            
            self.is_initialized = True
            logger.info("RAG system initialized successfully")
            
        except Exception as e:
            self.initialization_error = str(e)
            logger.error(f"Failed to initialize RAG system: {e}")
            
    def check_initialized(self):
        """åˆæœŸåŒ–ãƒã‚§ãƒƒã‚¯"""
        if not self.is_initialized:
            if self.initialization_error:
                raise HTTPException(
                    status_code=500,
                    detail=f"RAG system initialization failed: {self.initialization_error}"
                )
            else:
                raise HTTPException(
                    status_code=503,
                    detail="RAG system is not yet initialized"
                )
    
    def save_search_result(self, query_response: QueryResponse, name: Optional[str] = None, tags: Optional[List[str]] = None) -> SavedSearchResult:
        """æ¤œç´¢çµæœã‚’ä¿å­˜"""
        result_id = str(uuid.uuid4())
        timestamp = datetime.now(JST).isoformat()
        
        saved_result = SavedSearchResult(
            id=result_id,
            query=query_response.query,
            answer=query_response.answer,
            citations=query_response.citations,
            sources=query_response.sources,
            confidence_score=query_response.confidence_score,
            search_type=query_response.metadata.get("search_type", "hybrid"),
            top_k=query_response.metadata.get("top_k", 5),
            saved_at=timestamp,
            metadata={
                "name": name or f"Search_{timestamp[:10]}",
                "tags": tags or [],
                "processing_time": query_response.processing_time
            }
        )
        
        # JSONãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
        file_path = self.search_history_dir / f"{result_id}.json"
        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(saved_result.dict(), f, ensure_ascii=False, indent=2)
        
        return saved_result
    
    def get_search_history(self, page: int = 1, limit: int = 10, tag: Optional[str] = None) -> SearchHistoryResponse:
        """æ¤œç´¢å±¥æ­´ã‚’å–å¾—"""
        all_results = []
        
        # ã™ã¹ã¦ã®ä¿å­˜æ¸ˆã¿çµæœã‚’èª­ã¿è¾¼ã¿
        for json_file in self.search_history_dir.glob("*.json"):
            try:
                with open(json_file, "r", encoding="utf-8") as f:
                    result_data = json.load(f)
                    
                # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰IDã‚’ç¢ºå®Ÿã«è¨­å®š
                file_id = json_file.stem  # .jsonã‚’é™¤ã„ãŸãƒ•ã‚¡ã‚¤ãƒ«å
                result_data["id"] = file_id
                    
                # ã‚¿ã‚°ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                if tag and tag not in result_data.get("metadata", {}).get("tags", []):
                    continue
                    
                all_results.append(SavedSearchResult(**result_data))
            except Exception as e:
                logger.error(f"Error loading search result {json_file}: {e}")
        
        # æ—¥æ™‚ã§é™é †ã‚½ãƒ¼ãƒˆ
        all_results.sort(key=lambda x: x.saved_at, reverse=True)
        
        # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³
        total = len(all_results)
        start = (page - 1) * limit
        end = start + limit
        paginated_results = all_results[start:end]
        
        return SearchHistoryResponse(
            total=total,
            results=paginated_results,
            page=page,
            limit=limit
        )
    
    def get_saved_result(self, result_id: str) -> Optional[SavedSearchResult]:
        """ä¿å­˜ã•ã‚ŒãŸæ¤œç´¢çµæœã‚’å–å¾—"""
        file_path = self.search_history_dir / f"{result_id}.json"
        
        if not file_path.exists():
            return None
            
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                result_data = json.load(f)
                # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰IDã‚’ç¢ºå®Ÿã«è¨­å®š
                result_data["id"] = result_id
                return SavedSearchResult(**result_data)
        except Exception as e:
            logger.error(f"Error loading search result {result_id}: {e}")
            return None
    
    def export_search_results(self, result_ids: List[str], format: str = "json") -> bytes:
        """æ¤œç´¢çµæœã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        results = []
        for result_id in result_ids:
            result = self.get_saved_result(result_id)
            if result:
                results.append(result.dict())
        
        if format == "csv":
            import csv
            import io
            
            output = io.StringIO()
            if results:
                writer = csv.DictWriter(output, fieldnames=["query", "answer", "confidence_score", "saved_at", "search_type"])
                writer.writeheader()
                for result in results:
                    writer.writerow({
                        "query": result["query"],
                        "answer": result["answer"],
                        "confidence_score": result["confidence_score"],
                        "saved_at": result["saved_at"],
                        "search_type": result["search_type"]
                    })
            
            return output.getvalue().encode("utf-8")
        else:
            # JSON format
            return json.dumps(results, ensure_ascii=False, indent=2).encode("utf-8")
    
    def delete_search_history_item(self, result_id: str) -> bool:
        """æ¤œç´¢å±¥æ­´ã®å€‹åˆ¥ã‚¢ã‚¤ãƒ†ãƒ ã‚’å‰Šé™¤"""
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®å®‰å…¨æ€§ãƒã‚§ãƒƒã‚¯
            if ".." in result_id or "/" in result_id or "\\" in result_id:
                logger.error(f"Invalid result_id: {result_id}")
                return False
            
            file_path = self.search_history_dir / f"{result_id}.json"
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
            if not file_path.exists():
                logger.warning(f"Search history item not found: {result_id}")
                return False
            
            # search_history_diré…ä¸‹ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
            if not str(file_path.resolve()).startswith(str(self.search_history_dir.resolve())):
                logger.error(f"Invalid file path: {file_path}")
                return False
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
            file_path.unlink()
            logger.info(f"Deleted search history item: {result_id}")
            return True
            
        except Exception as e:
            logger.error(f"Error deleting search history item {result_id}: {e}")
            return False

# RAGã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
rag_app = RAGApplication()

# ãƒ«ãƒ¼ãƒˆãƒšãƒ¼ã‚¸
@app.get("/")
async def root(request: Request):
    """ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸"""
    return templates.TemplateResponse("index.html", {"request": request})

@app.get("/finetune")
async def finetune_page(request: Request):
    """ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç”»é¢"""
    return templates.TemplateResponse("finetune.html", {"request": request})

@app.get("/models")
async def models_page(request: Request):
    """ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ç”»é¢"""
    return templates.TemplateResponse("models.html", {"request": request})

@app.get("/readme")
async def readme_page(request: Request):
    """README.mdè¡¨ç¤ºãƒšãƒ¼ã‚¸"""
    return templates.TemplateResponse("readme.html", {"request": request})

@app.get("/rag")
async def rag_page(request: Request):
    """RAGã‚·ã‚¹ãƒ†ãƒ ç”»é¢"""
    return templates.TemplateResponse("rag.html", {"request": request, "rag_available": RAG_AVAILABLE})

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°
training_tasks = {}
model_cache = {}
executor = ThreadPoolExecutor(max_workers=2)

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹æ™‚ã®å‡¦ç†
@app.on_event("startup")
async def startup_event():
    """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹•æ™‚ã®å‡¦ç†"""
    logger.info("Starting AI Fine-tuning Toolkit with RAG integration...")
    if RAG_AVAILABLE:  # ä¸€æ™‚çš„ã«RAGåˆæœŸåŒ–ã‚’ç„¡åŠ¹åŒ–
        await rag_app.initialize()
    else:
        logger.warning("RAG system will not be available in this session")

# Ollamaçµ±åˆã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    import sys
    from pathlib import Path
    # scripts/convertãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
    scripts_convert_path = Path(__file__).parent.parent / "scripts" / "convert"
    sys.path.insert(0, str(scripts_convert_path))
    
    from ollama_integration import OllamaIntegration
    OLLAMA_AVAILABLE = True
    logger.info("Ollamaçµ±åˆãŒåˆ©ç”¨å¯èƒ½ã§ã™")
except ImportError as e:
    OLLAMA_AVAILABLE = False
    logger.warning(f"Ollamaçµ±åˆãŒåˆ©ç”¨ã§ãã¾ã›ã‚“: {e}")

# åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«å®šç¾©
available_models = [
    # Small Models (ãƒ†ã‚¹ãƒˆç”¨ãƒ»è»½é‡)
    {
        "name": "distilgpt2",
        "description": "è»½é‡ãªè‹±èªãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰",
        "size": "82MB",
        "status": "available",
        "gpu_requirement": "ãªã—"
    },
    {
        "name": "rinna/japanese-gpt2-small",
        "description": "æ—¥æœ¬èªGPT-2 Smallï¼ˆRinnaï¼‰",
        "size": "110MB",
        "status": "available",
        "gpu_requirement": "ãªã—"
    },
    {
        "name": "stabilityai/japanese-stablelm-3b-4e1t-instruct",
        "description": "Japanese StableLM 3B Instructï¼ˆæ¨å¥¨ï¼‰",
        "size": "3B",
        "status": "available",
        "gpu_requirement": "8GB"
    },
    {
        "name": "elyza/ELYZA-japanese-Llama-2-7b-instruct",
        "description": "ELYZAæ—¥æœ¬èªLlama-2 7B Instruct",
        "size": "7B",
        "status": "gpu-required",
        "gpu_requirement": "16GB"
    },
    {
        "name": "Qwen/Qwen2.5-14B-Instruct",
        "description": "Qwen 2.5 14B Instructï¼ˆæ¨å¥¨ï¼‰",
        "size": "14B",
        "status": "gpu-required",
        "gpu_requirement": "28GB"
    },
    {
        "name": "cyberagent/calm3-22b-chat",
        "description": "CyberAgent CALM3 22B Chat",
        "size": "22B",
        "status": "gpu-required",
        "gpu_requirement": "44GB"
    },
    {
        "name": "cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese",
        "description": "DeepSeek R1 Distill Qwen 32B æ—¥æœ¬èªç‰¹åŒ–ï¼ˆOllamaæ¨å¥¨ï¼‰",
        "size": "32B",
        "status": "ollama-recommended",
        "gpu_requirement": "20GB (Ollamaä½¿ç”¨æ™‚)"
    },
    {
        "name": "Qwen/Qwen2.5-17B-Instruct",
        "description": "Qwen 2.5 17B Instruct",
        "size": "17B",
        "status": "gpu-required",
        "gpu_requirement": "34GB"
    },
    {
        "name": "Qwen/Qwen2.5-32B-Instruct",
        "description": "Qwen 2.5 32B Instructï¼ˆOllamaæ¨å¥¨ï¼‰",
        "size": "32B",
        "status": "ollama-recommended",
        "gpu_requirement": "20GB (Ollamaä½¿ç”¨æ™‚)"
    }
]

def get_saved_models():
    """ä¿å­˜æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’å–å¾—"""
    saved_models = []
    # Dockerã‚³ãƒ³ãƒ†ãƒŠå†…ã§ã¯/workspaceã€ãƒ­ãƒ¼ã‚«ãƒ«ã§ã¯ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆ
    if os.path.exists("/workspace"):
        project_root = Path("/workspace")
    else:
        project_root = Path(os.getcwd())
    
    # outputsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æœ€åˆã«ç¢ºèª
    outputs_path = project_root / "outputs"
    
    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‹ã‚‰LoRAãƒ¢ãƒ‡ãƒ«ã‚’æ¤œç´¢
    for model_dir in project_root.glob("lora_demo_*"):
        if model_dir.is_dir():
            # ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼è¨­å®šãŒã‚ã‚‹ã‹ç¢ºèª
            if (model_dir / "adapter_config.json").exists() or (model_dir / "adapter_model.safetensors").exists():
                saved_models.append({
                    "name": model_dir.name,
                    "path": str(model_dir),
                    "type": "LoRA",
                    "size": "~1.6MB",
                    "base_model": "ä¸æ˜",
                    "training_method": "lora"
                })
    
    # outputsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚‚æ¤œç´¢
    if outputs_path.exists():
        for model_dir in outputs_path.iterdir():
            if model_dir.is_dir():
                # training_info.jsonã‹ã‚‰æƒ…å ±ã‚’èª­ã¿å–ã‚Š
                info_path = model_dir / "training_info.json"
                model_type = "Unknown"
                model_size = "Unknown"
                base_model = "ä¸æ˜"
                training_method = "unknown"
                training_data_size = 0
                
                if info_path.exists():
                    try:
                        with open(info_path, 'r', encoding='utf-8') as f:
                            info = json.load(f)
                            training_method = info.get("training_method", "unknown")
                            base_model = info.get("base_model", "ä¸æ˜")
                            training_data_size = info.get("training_data_size", 0)
                            
                            if training_method == "full":
                                model_type = "ãƒ•ãƒ«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°"
                                model_size = "~500MB+"
                            elif training_method == "qlora":
                                model_type = "QLoRA (4bit)"
                                model_size = "~1.0MB"
                            else:
                                model_type = "LoRA"
                                model_size = "~1.6MB"
                    except Exception as e:
                        logger.warning(f"training_info.jsonã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
                        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåã‹ã‚‰æ¨å®š
                        if "lora" in model_dir.name.lower():
                            model_type = "LoRA"
                            training_method = "lora"
                        elif "qlora" in model_dir.name.lower():
                            model_type = "QLoRA"
                            training_method = "qlora"
                        elif "full" in model_dir.name.lower():
                            model_type = "ãƒ•ãƒ«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°"
                            training_method = "full"
                
                # ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
                has_model_files = (
                    (model_dir / "adapter_model.safetensors").exists() or
                    (model_dir / "pytorch_model.bin").exists() or
                    (model_dir / "model.safetensors").exists() or
                    any(model_dir.glob("*.safetensors")) or
                    any(model_dir.glob("*.bin"))
                )
                
                # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
                has_tokenizer = (
                    (model_dir / "tokenizer.json").exists() or
                    (model_dir / "tokenizer_config.json").exists()
                )
                
                if has_model_files:  # ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹å ´åˆã®ã¿è¿½åŠ 
                    saved_models.append({
                        "name": model_dir.name,
                        "path": str(model_dir),
                        "type": model_type,
                        "size": model_size,
                        "base_model": base_model,
                        "training_method": training_method,
                        "training_data_size": training_data_size,
                        "has_tokenizer": has_tokenizer,
                        "has_model_files": has_model_files
                    })
    
    logger.info(f"æ¤œå‡ºã•ã‚ŒãŸä¿å­˜æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«: {len(saved_models)}å€‹")
    return saved_models

# å®Ÿéš›ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè£…
# ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°: è¨­å®šå€¤ã‚’é©åˆ‡ãªå‹ã«å¤‰æ›
def get_config_value(config, key, default, value_type):
    value = config.get(key, default)
    if isinstance(value, str):
        try:
            return value_type(value)
        except (ValueError, TypeError):
            return default
    return value_type(value)

async def run_training_task(task_id: str, request: TrainingRequest):
    """ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œ"""
    try:
        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°
        training_tasks[task_id].status = "preparing"
        method_name = {
            "lora": "LoRA",
            "qlora": "QLoRA (4bit)", 
            "full": "ãƒ•ãƒ«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°"
        }.get(request.training_method, "LoRA")
        training_tasks[task_id].message = f"{method_name}ã§ãƒ¢ãƒ‡ãƒ«ã‚’æº–å‚™ä¸­..."
        training_tasks[task_id].progress = 10.0
        logger.info(f"Task {task_id}: {method_name}æº–å‚™é–‹å§‹ - ãƒ¢ãƒ‡ãƒ«: {request.model_name}")
        
        # è©³ç´°ãªãƒ­ã‚°å‡ºåŠ›
        logger.info(f"Task {task_id}: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®š - ãƒ¡ã‚½ãƒƒãƒ‰: {request.training_method}, ãƒ¢ãƒ‡ãƒ«: {request.model_name}")
        logger.info(f"Task {task_id}: LoRAè¨­å®š: {request.lora_config}")
        logger.info(f"Task {task_id}: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®š: {request.training_config}")
        
        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        timestamp = datetime.now(JST).strftime("%Y%m%d_%H%M%S")
        output_dir = get_output_directory(method_name, timestamp)
        
        # è¨­å®šèª­ã¿è¾¼ã¿
        training_config = load_training_config(request.training_method)
        
        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
        training_tasks[task_id].message = "ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­..."
        training_tasks[task_id].progress = 20.0
        
        try:
            project_root = Path(os.getcwd())
            cache_dir = project_root / "hf_cache"
            model, tokenizer = load_model_and_tokenizer(
                model_name=request.model_name,
                training_method=request.training_method,
                cache_dir=cache_dir
            )
            logger.info(f"Task {task_id}: ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")
        except Exception as e:
            import traceback
            error_traceback = traceback.format_exc()
            logger.error(f"Task {task_id}: ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}")
            logger.error(f"Task {task_id}: ã‚¨ãƒ©ãƒ¼è©³ç´°: {error_traceback}")
            training_tasks[task_id].status = "failed"
            training_tasks[task_id].message = handle_model_loading_error(e, request.model_name, task_id)
            return
        
        # LoRAè¨­å®š
        if request.training_method in ["lora", "qlora"]:
            training_tasks[task_id].message = "LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’è¨­å®šä¸­..."
            training_tasks[task_id].progress = 30.0
            
            # QLoRAã®å ´åˆã¯ãƒ¢ãƒ‡ãƒ«ã‚’æº–å‚™
            if request.training_method == "qlora":
                model = prepare_model_for_kbit_training(model)
            
            # LoRAè¨­å®š
            lora_config = LoraConfig(
                r=get_config_value(request.lora_config, "r", get_config_value(training_config, "lora_r", 16, int), int),
                lora_alpha=get_config_value(request.lora_config, "lora_alpha", get_config_value(training_config, "lora_alpha", 32, int), int),
                target_modules=training_config.get("target_modules", ["q_proj", "v_proj", "k_proj", "o_proj"]),
                lora_dropout=get_config_value(training_config, "lora_dropout", 0.05, float),
                bias="none",
                task_type=TaskType.CAUSAL_LM
            )
            
            model = get_peft_model(model, lora_config)
            model.print_trainable_parameters()
        
        # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
        training_tasks[task_id].message = "ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ä¸­..."
        training_tasks[task_id].progress = 40.0
        
        # JSONLãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        train_texts = []
        for data_path in request.training_data:
            data_file = Path(data_path)
            if data_file.exists() and data_file.suffix == '.jsonl':
                with open(data_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        try:
                            data = json.loads(line.strip())
                            if 'text' in data:
                                train_texts.append(data['text'])
                            elif 'input' in data and 'output' in data:
                                train_texts.append(f"{data['input']}\n{data['output']}")
                        except json.JSONDecodeError:
                            continue
        
        if not train_texts:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
            train_texts = [
                "ã“ã‚Œã¯æ—¥æœ¬èªã®ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆã§ã™ã€‚",
                "ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã™ã€‚",
                "AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã§ã™ã€‚"
            ] * 10  # 30å€‹ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’ä½œæˆ
        
        logger.info(f"Task {task_id}: {len(train_texts)}å€‹ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚µãƒ³ãƒ—ãƒ«ã‚’æº–å‚™")
        
        # å®Ÿéš›ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ
        training_tasks[task_id].status = "training"
        training_tasks[task_id].message = f"{method_name}ã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­..."
        training_tasks[task_id].progress = 50.0
        
        # ç°¡å˜ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
        from torch.utils.data import Dataset
        
        class SimpleDataset(Dataset):
            def __init__(self, texts, tokenizer, max_length=512):
                self.texts = texts
                self.tokenizer = tokenizer
                self.max_length = max_length
            
            def __len__(self):
                return len(self.texts)
            
            def __getitem__(self, idx):
                text = self.texts[idx]
                encoding = self.tokenizer(
                    text,
                    truncation=True,
                    padding="max_length",
                    max_length=self.max_length,
                    return_tensors="pt"
                )
                
                # labelsã‚’input_idsã¨åŒã˜ã«ã™ã‚‹ãŒã€paddingãƒˆãƒ¼ã‚¯ãƒ³ã¯-100ã«ãƒã‚¹ã‚¯
                labels = encoding["input_ids"].squeeze().clone()
                labels[labels == self.tokenizer.pad_token_id] = -100
                
                return {
                    "input_ids": encoding["input_ids"].squeeze(),
                    "attention_mask": encoding["attention_mask"].squeeze(),
                    "labels": labels
                }
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
        train_dataset = SimpleDataset(train_texts, tokenizer)
        
        # EWCã‚’ä½¿ç”¨ã™ã‚‹ã‚«ã‚¹ã‚¿ãƒ ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼
        class EWCTrainer(Trainer):
            def __init__(self, *args, ewc_lambda: float = 5000.0, use_ewc: bool = False, **kwargs):
                super().__init__(*args, **kwargs)
                # accelerateãƒ¢ãƒ‡ãƒ«ã®å ´åˆã¯ãƒ¢ãƒ‡ãƒ«ç§»å‹•ã‚’ç„¡åŠ¹åŒ–
                if hasattr(self.model, 'hf_device_map'):
                    self.place_model_on_device = False
                self.ewc_lambda = ewc_lambda
                self.use_ewc = use_ewc
                self.ewc_helper = None
                
                if self.use_ewc:
                    try:
                        from src.training.ewc_utils import EWCHelper
                        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
                        self.ewc_helper = EWCHelper(self.model, device)
                        logger.info("EWCã‚’æœ‰åŠ¹åŒ–ã—ã¾ã—ãŸ")
                    except ImportError:
                        logger.warning("EWCãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ")
                        self.use_ewc = False
            
            def _move_model_to_device(self, model, device):
                """accelerateã§ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®ç§»å‹•ã‚’é˜²ããŸã‚ã«ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰"""
                # accelerateã§ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã¯ç§»å‹•ã—ãªã„
                if hasattr(model, 'hf_device_map'):
                    logger.info(f"ãƒ¢ãƒ‡ãƒ«ç§»å‹•ã‚’ã‚¹ã‚­ãƒƒãƒ— - accelerateã«ã‚ˆã£ã¦ã™ã§ã«é…ç½®æ¸ˆã¿")
                    return model
                return model
            
            def compute_loss(self, model, inputs, return_outputs=False):
                """æå¤±é–¢æ•°ã«EWCãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’è¿½åŠ """
                outputs = model(**inputs)
                loss = outputs.loss if isinstance(outputs, dict) else outputs[0]
                
                # EWCãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’è¿½åŠ 
                if self.use_ewc and self.ewc_helper is not None and self.ewc_helper.fisher_matrix is not None:
                    ewc_loss = self.ewc_helper.compute_ewc_loss(model)
                    loss = loss + self.ewc_lambda * ewc_loss
                    
                return (loss, outputs) if return_outputs else loss
        

        
        # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¼•æ•°
        # ãƒ•ãƒ«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®å ´åˆã¯ã€ã‚ˆã‚Šæ…é‡ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
        if request.training_method == "full":
            batch_size = get_config_value(training_config, "batch_size", 1, int)
            gradient_accumulation_steps = get_config_value(training_config, "gradient_accumulation_steps", 16, int)
            num_epochs = get_config_value(training_config, "num_epochs", 1, int)
            
            effective_batch_size = batch_size * gradient_accumulation_steps
            total_steps = len(train_dataset) * num_epochs // effective_batch_size
            max_steps = min(100, total_steps)  # ãƒ•ãƒ«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¯100ã‚¹ãƒ†ãƒƒãƒ—ã¾ã§
            learning_rate = 5e-6  # ã‚ˆã‚Šä½ã„å­¦ç¿’ç‡
        else:
            batch_size = get_config_value(training_config, "batch_size", 1, int)
            max_steps = min(50, len(train_dataset) // batch_size)
            learning_rate = get_config_value(training_config, "learning_rate", 2e-4, float)
        
        training_args = TrainingArguments(
            output_dir=str(output_dir),
            per_device_train_batch_size=get_config_value(training_config, "batch_size", 1, int),
            gradient_accumulation_steps=get_config_value(training_config, "gradient_accumulation_steps", 4, int),
            num_train_epochs=get_config_value(training_config, "num_epochs", 1, int),
            learning_rate=learning_rate,
            warmup_steps=min(get_config_value(training_config, "warmup_steps", 10, int), max_steps // 10),
            logging_steps=5,
            save_steps=max_steps // 2,
            max_steps=max_steps,
            fp16=torch.cuda.is_available(),
            gradient_checkpointing=True,
            remove_unused_columns=False,
            report_to=[],
            save_strategy="steps",
            save_total_limit=2,
            dataloader_pin_memory=False,  # ãƒ¡ãƒ¢ãƒªå•é¡Œå›é¿
        )
        
        # Trainerä½œæˆã¨å®Ÿè¡Œ
        # ãƒ•ãƒ«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®å ´åˆã¯EWCã‚’ä½¿ç”¨ (ãƒªã‚½ãƒ¼ã‚¹å•é¡Œã®ãŸã‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ç„¡åŠ¹åŒ–)
        use_ewc = False # request.training_method == "full"
        ewc_lambda = 5000.0 if use_ewc else 0.0
        
        trainer = EWCTrainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            tokenizer=tokenizer,
            use_ewc=use_ewc,
            ewc_lambda=ewc_lambda,
        )
        
        # EWCã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã€äº‹å‰å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã§Fisherè¡Œåˆ—ã‚’è¨ˆç®—
        if use_ewc and trainer.ewc_helper is not None:
            logger.info("Fisherè¡Œåˆ—ã‚’è¨ˆç®—ä¸­...")
            # äº‹å‰å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ä¸€èˆ¬çš„ãªæ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã‚’ä½¿ç”¨
            pretrain_texts = [
                "äººå·¥çŸ¥èƒ½ã¯æ€¥é€Ÿã«ç™ºå±•ã—ã¦ã„ã‚‹æŠ€è¡“åˆ†é‡ã§ã™ã€‚",
                "æ©Ÿæ¢°å­¦ç¿’ã¯ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ç¿’ã™ã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã™ã€‚",
                "æ·±å±¤å­¦ç¿’ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚",
                "è‡ªç„¶è¨€èªå‡¦ç†ã¯è¨€èªã‚’ç†è§£ã™ã‚‹æŠ€è¡“ã§ã™ã€‚",
                "ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ã¯ç”»åƒã‚’è§£æã—ã¾ã™ã€‚",
            ]
            
            pretrain_dataset = SimpleDataset(pretrain_texts, tokenizer)
            from torch.utils.data import DataLoader
            pretrain_loader = DataLoader(pretrain_dataset, batch_size=1, shuffle=False)
            
            # Fisherè¡Œåˆ—ã®è¨ˆç®—ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰
            trainer.ewc_helper.compute_fisher_matrix(pretrain_loader, max_batches=30)
            logger.info("Fisherè¡Œåˆ—ã®è¨ˆç®—å®Œäº†")
        
        # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ
        logger.info(f"Task {task_id}: å®Ÿéš›ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹")
        try:
            trainer.train()
            logger.info(f"Task {task_id}: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†")
        except Exception as train_error:
            logger.error(f"Task {task_id}: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {str(train_error)}")
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ãƒ¢ãƒ‡ãƒ«ã¯ä¿å­˜ã—ã¦ç¶šè¡Œ
        
        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        training_tasks[task_id].message = "ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ä¸­..."
        training_tasks[task_id].progress = 95.0
        
        # ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ä¿å­˜
        model.save_pretrained(str(output_dir))
        tokenizer.save_pretrained(str(output_dir))
        
        # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æƒ…å ±ã‚’ä¿å­˜
        training_info = {
            "model_type": request.training_method,
            "base_model": request.model_name,
            "r": get_config_value(request.lora_config, "r", get_config_value(training_config, "lora_r", 16, int), int),
            "lora_alpha": get_config_value(request.lora_config, "lora_alpha", get_config_value(training_config, "lora_alpha", 32, int), int),
            "task_type": "CAUSAL_LM",
            "training_data_size": len(train_texts),
            "training_method": request.training_method,
            "use_qlora": request.training_method == "qlora",
            "load_in_4bit": request.training_method == "qlora",
            "timestamp": timestamp,
            "output_dir": str(output_dir)
        }
        
        with open(output_dir / "training_info.json", "w", encoding='utf-8') as f:
            json.dump(training_info, f, indent=2, ensure_ascii=False)
        
        # å®Œäº†
        training_tasks[task_id].status = "completed"
        training_tasks[task_id].progress = 100.0
        training_tasks[task_id].message = f"{method_name}ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†ï¼"
        training_tasks[task_id].model_path = str(output_dir)
        logger.info(f"Task {task_id}: {method_name}ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº† - {output_dir}")
        
    except Exception as e:
        import traceback
        logger.error(f"Task {task_id}: ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {str(e)}")
        logger.error(traceback.format_exc())
        training_tasks[task_id].status = "failed"
        training_tasks[task_id].message = f"ã‚¨ãƒ©ãƒ¼: {str(e)}"

# API ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ

# ç«¶åˆã™ã‚‹ãƒ«ãƒ¼ãƒˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’å‰Šé™¤ - ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ™ãƒ¼ã‚¹ã®ãƒ«ãƒ¼ãƒˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’ä½¿ç”¨

@app.get("/manual", response_class=HTMLResponse)
async def manual_page(request: Request):
    """åˆ©ç”¨ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ãƒšãƒ¼ã‚¸"""
    return templates.TemplateResponse("readme.html", {"request": request})

@app.get("/system-overview", response_class=HTMLResponse)
async def system_overview_page():
    """ã‚·ã‚¹ãƒ†ãƒ æ¦‚è¦ãƒšãƒ¼ã‚¸"""
    # TODO: Create system-overview.html template in templates directory
    return HTMLResponse(
        content="<h1>System overview page not implemented yet</h1>", 
        status_code=404
    )

@app.get("/docs/{doc_name}")
async def serve_documentation(doc_name: str):
    """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®é…ä¿¡"""
    allowed_docs = [
        "API_REFERENCE.md", "LARGE_MODEL_SETUP.md", "MULTI_GPU_OPTIMIZATION.md",
        "USER_MANUAL.md", "QUICKSTART_GUIDE.md", "USAGE_GUIDE.md",
        "TRAINED_MODEL_USAGE.md", "DEEPSEEK_SETUP.md"
    ]
    
    if doc_name not in allowed_docs:
        return {"error": "Document not found"}
    
    # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ‘ã‚¹ã®æ¤œç´¢
    project_root = Path(os.getcwd())
    possible_docs_paths = [
        project_root / "docs",
        Path(__file__).parent.parent / "docs"
    ]
    
    for docs_path in possible_docs_paths:
        doc_file_path = docs_path / doc_name
        if doc_file_path.exists():
            try:
                with open(doc_file_path, "r", encoding="utf-8") as f:
                    content = f.read()
                return PlainTextResponse(content, media_type="text/markdown")
            except Exception as e:
                return {"error": f"Error reading document: {str(e)}"}
    
    return {"error": "Document file not found"}

@app.get("/api/models")
async def get_models():
    """åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’å–å¾—"""
    return {
        "available_models": available_models,
        "saved_models": get_saved_models()
    }

@app.post("/api/upload-data")
async def upload_training_data(file: UploadFile = File(...)):
    """ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"""
    try:
        logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰é–‹å§‹: {file.filename}")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åã¨ã‚µã‚¤ã‚ºã®æ¤œè¨¼
        if not file.filename:
            raise HTTPException(status_code=400, detail="ãƒ•ã‚¡ã‚¤ãƒ«åãŒä¸æ­£ã§ã™")
        
        if file.size and file.size > 100 * 1024 * 1024:  # 100MBåˆ¶é™
            raise HTTPException(status_code=400, detail="ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒå¤§ãã™ãã¾ã™ (æœ€å¤§100MB)")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        project_root = Path(os.getcwd())
        upload_dir = project_root / "data" / "uploaded"
        upload_dir.mkdir(parents=True, exist_ok=True)
        
        file_path = upload_dir / file.filename
        content = await file.read()
        
        logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜: {file_path}, ã‚µã‚¤ã‚º: {len(content)} bytes")
        
        with open(file_path, "wb") as f:
            f.write(content)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã®æ¤œè¨¼
        sample_data = []
        data_count = 0
        
        if file.filename.endswith('.jsonl'):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                    data_count = len(lines)
                    
                    for i, line in enumerate(lines[:5]):  # æœ€åˆã®5è¡Œã‚’ã‚µãƒ³ãƒ—ãƒ«ã¨ã—ã¦å–å¾—
                        line = line.strip()
                        if line:  # ç©ºè¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
                            try:
                                data = json.loads(line)
                                sample_data.append(data)
                            except json.JSONDecodeError as je:
                                logger.error(f"JSON parse error at line {i+1}: {str(je)}")
                                raise HTTPException(status_code=400, detail=f"è¡Œ {i+1} ã§JSONãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {str(je)}")
                        
                logger.info(f"JSONLè§£æå®Œäº†: {data_count}è¡Œ, ã‚µãƒ³ãƒ—ãƒ«: {len(sample_data)}ä»¶")
                
            except UnicodeDecodeError:
                logger.error("ãƒ•ã‚¡ã‚¤ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼")
                raise HTTPException(status_code=400, detail="ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãŒä¸æ­£ã§ã™ (UTF-8ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„)")
        
        elif file.filename.endswith('.json'):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if isinstance(data, list):
                        data_count = len(data)
                        sample_data = data[:3]  # æœ€åˆã®3ä»¶ã‚’ã‚µãƒ³ãƒ—ãƒ«ã¨ã—ã¦å–å¾—
                    else:
                        data_count = 1
                        sample_data = [data]
                        
                logger.info(f"JSONè§£æå®Œäº†: {data_count}ä»¶, ã‚µãƒ³ãƒ—ãƒ«: {len(sample_data)}ä»¶")
                
            except json.JSONDecodeError as je:
                logger.error(f"JSON parse error: {str(je)}")
                raise HTTPException(status_code=400, detail=f"JSONãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {str(je)}")
            except UnicodeDecodeError:
                logger.error("ãƒ•ã‚¡ã‚¤ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼")
                raise HTTPException(status_code=400, detail="ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãŒä¸æ­£ã§ã™ (UTF-8ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„)")
        else:
            raise HTTPException(status_code=400, detail="ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã§ã™ (.jsonl ã¾ãŸã¯ .json ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„)")
        
        result = {
            "status": "success",
            "filename": file.filename,
            "path": str(file_path),
            "size": len(content),
            "data_count": data_count,
            "sample_data": sample_data[:3]
        }
        
        logger.info(f"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æˆåŠŸ: {result}")
        return result
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Upload error: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {str(e)}")

@app.post("/api/train")
async def start_training(request: TrainingRequest, background_tasks: BackgroundTasks):
    """ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹"""
    try:
        task_id = str(uuid.uuid4())
        logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹ãƒªã‚¯ã‚¨ã‚¹ãƒˆå—ä¿¡: task_id={task_id}")
        logger.info(f"ãƒªã‚¯ã‚¨ã‚¹ãƒˆå†…å®¹: model_name={request.model_name}, method={request.training_method}")
        logger.info(f"LoRAè¨­å®š: {request.lora_config}")
        logger.info(f"ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®š: {request.training_config}")
        
        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®æ¤œè¨¼
        if not request.model_name:
            raise HTTPException(status_code=400, detail="model_name is required")
        
        if not request.training_data:
            raise HTTPException(status_code=400, detail="training_data is required")
        
        # åˆæœŸã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’è¨­å®š
        training_tasks[task_id] = TrainingStatus(
            task_id=task_id,
            status="starting",
            progress=0.0,
            message="ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã—ã¦ã„ã¾ã™..."
        )
        
        # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œ
        background_tasks.add_task(run_training_task, task_id, request)
        
        return {"task_id": task_id, "status": "started"}
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹ã‚¨ãƒ©ãƒ¼: {str(e)}")
        import traceback
        logger.error(f"ã‚¨ãƒ©ãƒ¼è©³ç´°: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"Training failed: {str(e)}")

@app.get("/api/training-status/{task_id}")
async def get_training_status(task_id: str):
    """ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’å–å¾—"""
    if task_id not in training_tasks:
        raise HTTPException(status_code=404, detail="Task not found")
    
    return training_tasks[task_id]

@app.post("/api/monitoring/start")
async def start_monitoring():
    """ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã‚’èµ·å‹•"""
    try:
        import subprocess
        import os
        
        # Webç”¨ã®ç›£è¦–åˆ¶å¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½¿ç”¨
        script_path = Path("/workspace/scripts/web_monitoring_controller.sh")
        
        if not script_path.exists():
            # ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒãªã„å ´åˆã¯ä½œæˆ
            script_content = '''#!/bin/bash
# ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã‚’ã‚³ãƒ³ãƒ†ãƒŠå†…ã‹ã‚‰èµ·å‹•ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
echo "ğŸš€ Grafanaç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã‚’èµ·å‹•ä¸­..."

# GrafanaãŒèµ·å‹•ã—ã¦ã„ã‚‹ã‹ç¢ºèª
if curl -s http://ai-ft-grafana:3000/api/health > /dev/null 2>&1; then
    echo "âœ… Grafana: æ—¢ã«èµ·å‹•ã—ã¦ã„ã¾ã™"
    exit 0
else
    echo "âš ï¸ Grafana: ãƒ›ã‚¹ãƒˆå´ã§docker-compose -f docker/docker-compose-monitoring.yml up -d ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„"
    exit 1
fi
'''
            script_path.write_text(script_content)
            os.chmod(script_path, 0o755)
        
        # ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œ
        result = subprocess.run(
            [str(script_path), "start"],
            capture_output=True,
            text=True
        )
        
        # Grafanaã®çŠ¶æ…‹ã‚’ç›´æ¥ç¢ºèª
        import requests
        grafana_running = False
        prometheus_running = False
        
        try:
            # Grafanaç¢ºèªï¼ˆã‚³ãƒ³ãƒ†ãƒŠé–“é€šä¿¡ï¼‰
            resp = requests.get("http://ai-ft-grafana:3000/api/health", timeout=2)
            grafana_running = resp.status_code == 200
        except:
            pass
            
        try:
            # Prometheusç¢ºèªï¼ˆã‚³ãƒ³ãƒ†ãƒŠé–“é€šä¿¡ï¼‰
            resp = requests.get("http://ai-ft-prometheus:9090/-/healthy", timeout=2)
            prometheus_running = resp.status_code == 200
        except:
            pass
        
        if grafana_running or prometheus_running:
            return JSONResponse(content={
                "status": "success",
                "message": "ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ãŒåˆ©ç”¨å¯èƒ½ã§ã™",
                "services": {
                    "grafana": "http://localhost:3000" if grafana_running else None,
                    "prometheus": "http://localhost:9090" if prometheus_running else None
                },
                "note": "æ—¢ã«èµ·å‹•æ¸ˆã¿ã‹ã€ãƒ›ã‚¹ãƒˆå´ã§èµ·å‹•ã•ã‚Œã¦ã„ã¾ã™"
            })
        else:
            return JSONResponse(
                content={
                    "status": "error", 
                    "message": "ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ãŒèµ·å‹•ã—ã¦ã„ã¾ã›ã‚“ã€‚ãƒ›ã‚¹ãƒˆå´ã§ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„:\ndocker-compose -f docker/docker-compose-monitoring.yml up -d",
                    "command": "docker-compose -f docker/docker-compose-monitoring.yml up -d"
                },
                status_code=503
            )
    except Exception as e:
        logger.error(f"ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ èµ·å‹•ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return JSONResponse(
            content={"status": "error", "message": str(e)},
            status_code=500
        )

@app.post("/api/monitoring/stop")
async def stop_monitoring():
    """ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã‚’åœæ­¢"""
    try:
        import subprocess
        import requests
        
        # ç¾åœ¨ã®çŠ¶æ…‹ã‚’ç¢ºèª
        grafana_running = False
        prometheus_running = False
        
        try:
            resp = requests.get("http://ai-ft-grafana:3000/api/health", timeout=2)
            grafana_running = resp.status_code == 200
        except:
            pass
            
        try:
            resp = requests.get("http://ai-ft-prometheus:9090/-/healthy", timeout=2)
            prometheus_running = resp.status_code == 200
        except:
            pass
        
        if not grafana_running and not prometheus_running:
            return JSONResponse(content={
                "status": "success",
                "message": "ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã¯æ—¢ã«åœæ­¢ã—ã¦ã„ã¾ã™"
            })
        
        # ã‚³ãƒ³ãƒ†ãƒŠå†…ã‹ã‚‰åœæ­¢ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã§ããªã„ãŸã‚ã€æ‰‹é †ã‚’æ¡ˆå†…
        return JSONResponse(content={
            "status": "info",
            "message": "ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã‚’åœæ­¢ã™ã‚‹ã«ã¯ã€ãƒ›ã‚¹ãƒˆå´ã§ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„",
            "command": "docker stop ai-ft-grafana ai-ft-prometheus ai-ft-redis",
            "alternative": "ã¾ãŸã¯: docker-compose -f docker/docker-compose-monitoring.yml down"
        })
        
    except Exception as e:
        logger.error(f"ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ åœæ­¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return JSONResponse(
            content={"status": "error", "message": str(e)},
            status_code=500
        )

@app.get("/api/monitoring/status")
async def monitoring_status():
    """ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã®çŠ¶æ…‹ã‚’ç¢ºèª"""
    try:
        import subprocess
        
        docker_dir = Path(__file__).parent.parent / "docker"
        compose_file = docker_dir / "docker-compose-monitoring.yml"
        
        result = subprocess.run(
            ["docker-compose", "-f", str(compose_file), "ps", "--format", "json"],
            capture_output=True,
            text=True,
            cwd=str(docker_dir)
        )
        
        if result.returncode == 0:
            services_running = "grafana" in result.stdout.lower()
            return JSONResponse(content={
                "status": "success",
                "running": services_running,
                "message": "ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã¯ç¨¼åƒä¸­" if services_running else "ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã¯åœæ­¢ä¸­"
            })
        else:
            return JSONResponse(content={
                "status": "success",
                "running": False,
                "message": "ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã¯åœæ­¢ä¸­"
            })
    except Exception as e:
        return JSONResponse(content={
            "status": "error",
            "running": False,
            "message": str(e)
        })

@app.post("/api/generate")
async def generate_text(request: GenerationRequest):
    """å®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
    try:
        logger.info(f"ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆé–‹å§‹: ãƒ¢ãƒ‡ãƒ«={request.model_path}, ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ={request.prompt[:50]}...")
        
        model_path = Path(request.model_path)
        
        # ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã®å­˜åœ¨ç¢ºèª
        if not model_path.exists() or not model_path.is_dir():
            logger.warning(f"ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {model_path}")
            return {
                "prompt": request.prompt,
                "generated_text": request.prompt + " [ã‚¨ãƒ©ãƒ¼: ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“]",
                "model_path": request.model_path,
                "error": "ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ãŒå­˜åœ¨ã—ã¾ã›ã‚“"
            }
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼
        cache_key = str(model_path)
        
        # ãƒ¡ãƒ¢ãƒªä¸è¶³ã‚’äº‹å‰ã«ãƒã‚§ãƒƒã‚¯
        if torch.cuda.is_available():
            # ç¾åœ¨ã®ç©ºããƒ¡ãƒ¢ãƒªã‚’ç¢ºèª
            free_memory = torch.cuda.mem_get_info()[0] / (1024**3)
            logger.info(f"ç¾åœ¨ã®GPUç©ºããƒ¡ãƒ¢ãƒª: {free_memory:.2f} GB")
            
            # 32Bãƒ¢ãƒ‡ãƒ«ã¯æœ€ä½ã§ã‚‚10GBã®ç©ºããƒ¡ãƒ¢ãƒªãŒå¿…è¦
            if free_memory < 10 and OLLAMA_AVAILABLE:
                logger.info("ãƒ¡ãƒ¢ãƒªä¸è¶³ã®ãŸã‚ã€ç›´æ¥Ollamaã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™")
                try:
                    ollama = OllamaIntegration()
                    result = ollama.generate_text(
                        model_name="llama3.2:3b",
                        prompt=request.prompt,
                        temperature=request.temperature,
                        top_p=request.top_p,
                        max_tokens=request.max_length
                    )
                    
                    if result.get("success", False):
                        return {
                            "prompt": request.prompt,
                            "generated_text": result.get("generated_text", ""),
                            "model_path": request.model_path,
                            "method": "ollama",
                            "note": "GPUãƒ¡ãƒ¢ãƒªä¸è¶³ã®ãŸã‚ã€Ollamaãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã—ãŸ"
                        }
                except Exception as e:
                    logger.error(f"Ollamaãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¤±æ•—: {e}")
        
        # ãƒ¢ãƒ‡ãƒ«ãŒã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ãªã„å ´åˆã¯èª­ã¿è¾¼ã¿
        if cache_key not in model_cache:
            # ãƒ¡ãƒ¢ãƒªä¸è¶³ã‚’é˜²ããŸã‚ã€æ—¢å­˜ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢
            if len(model_cache) > 0:
                logger.info("ãƒ¡ãƒ¢ãƒªç¯€ç´„ã®ãŸã‚æ—¢å­˜ã®ãƒ¢ãƒ‡ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢")
                for key in list(model_cache.keys()):
                    if key != cache_key:  # ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«ä»¥å¤–ã‚’ã‚¯ãƒªã‚¢
                        del model_cache[key]
                torch.cuda.empty_cache()
                import gc
                gc.collect()
            
            logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­: {model_path}")
            
            try:
                # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æƒ…å ±ã‚’èª­ã¿è¾¼ã¿
                training_info_path = model_path / "training_info.json"
                base_model_name = "distilgpt2"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
                training_method = "lora"
                
                if training_info_path.exists():
                    with open(training_info_path, 'r', encoding='utf-8') as f:
                        training_info = json.load(f)
                        base_model_name = training_info.get("base_model", "distilgpt2")
                        training_method = training_info.get("training_method", "lora")
                        logger.info(f"ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«: {base_model_name}, ãƒ¡ã‚½ãƒƒãƒ‰: {training_method}")
                
                # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿
                try:
                    tokenizer = load_tokenizer(str(model_path))
                    logger.info("ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã¿")
                except Exception as e:
                    logger.warning(f"ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
                    # ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ä½¿ç”¨
                    tokenizer = load_tokenizer(base_model_name)
                    logger.info(f"ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ä½¿ç”¨: {base_model_name}")
                
                # ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
                if training_method in ["lora", "qlora"]:
                    # LoRA/QLoRAãƒ¢ãƒ‡ãƒ«ã®å ´åˆ
                    from peft import PeftModel
                    
                    # ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿
                    logger.info(f"ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­: {base_model_name}")
                    
                    # å¤§ããªãƒ¢ãƒ‡ãƒ«ã®å ´åˆã¯é‡å­åŒ–ã‚’ä½¿ç”¨
                    quantization_config = create_quantization_config(base_model_name, "lora")
                    device_map = get_device_map(base_model_name)
                    
                    model_kwargs = {
                        "torch_dtype": torch.float16 if torch.cuda.is_available() else torch.float32,
                        "trust_remote_code": True,
                        "low_cpu_mem_usage": True
                    }
                    
                    if quantization_config:
                        model_kwargs["quantization_config"] = quantization_config
                    if device_map:
                        model_kwargs["device_map"] = device_map
                    
                    base_model = AutoModelForCausalLM.from_pretrained(
                        base_model_name,
                        **model_kwargs
                    )
                    
                    # LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’èª­ã¿è¾¼ã¿
                    logger.info(f"LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’èª­ã¿è¾¼ã¿ä¸­: {model_path}")
                    model = PeftModel.from_pretrained(base_model, str(model_path))
                    logger.info("LoRAãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰å®Œäº†ã€‚GPUã¸è»¢é€ã—ã¾ã™ã€‚")
                    model.to("cuda")
                    logger.info("ãƒ¢ãƒ‡ãƒ«ã‚’GPUã¸è»¢é€ã—ã¾ã—ãŸã€‚")
                    
                else:
                    # ãƒ•ãƒ«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®å ´åˆ
                    if torch.cuda.is_available():
                        # ãƒ¡ãƒ¢ãƒªç®¡ç†ã®ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®š
                        os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
                        
                        # GPUãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢
                        torch.cuda.empty_cache()
                        
                        # ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆ
                        offload_dir = Path("offload")
                        offload_dir.mkdir(exist_ok=True)
                        logger.info(f"ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ: {offload_dir}")
                        
                        gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                        logger.info(f"åˆ©ç”¨å¯èƒ½ãªGPUãƒ¡ãƒ¢ãƒª: {gpu_memory:.2f} GB")
                        
                        # 32Bãƒ¢ãƒ‡ãƒ«ã®æ¨è«–æ™‚ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–
                        from transformers import BitsAndBytesConfig
                        
                        # æ¨è«–æ™‚ã¯å¸¸ã«4bité‡å­åŒ–ã‚’ä½¿ç”¨ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡é‡è¦–ï¼‰
                        logger.info("æ¨è«–æ™‚ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–: 4bité‡å­åŒ–ã‚’é©ç”¨")
                        quantization_config = BitsAndBytesConfig(
                            load_in_4bit=True,
                            bnb_4bit_compute_dtype=torch.float16,
                            bnb_4bit_use_double_quant=True,
                            bnb_4bit_quant_type="nf4",
                            llm_int8_enable_fp32_cpu_offload=True
                        )
                        
                        # ãƒ‡ãƒã‚¤ã‚¹ãƒãƒƒãƒ—ã‚’æœ€é©åŒ–
                        # GPUãƒ¡ãƒ¢ãƒªã®ç©ºãå®¹é‡ã‚’ç¢ºèª
                        free_memory_gb = torch.cuda.mem_get_info()[0] / (1024**3)
                        # å®‰å…¨ãƒãƒ¼ã‚¸ãƒ³ã‚’å«ã‚ã¦è¨­å®š
                        safe_memory = max(1, int(free_memory_gb * 0.8))  # 80%ã‚’ä½¿ç”¨
                        
                        max_memory = {
                            0: f"{safe_memory}GB",
                            "cpu": "32GB"  # CPUãƒ¡ãƒ¢ãƒªã‚’å¢—ã‚„ã—ã¦ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã‚’ä¿ƒé€²
                        }
                        
                        # ãƒ¡ãƒ¢ãƒªä¸è¶³å¯¾ç­–ã®å¼·åŒ–
                        try:
                            # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å‰ã«ã‚‚ã†ä¸€åº¦ãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢
                            torch.cuda.empty_cache()
                            torch.cuda.synchronize()
                            
                            logger.info("ãƒ•ãƒ«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã‚’é‡å­åŒ–ä»˜ãã§èª­ã¿è¾¼ã¿ä¸­...")
                            model = AutoModelForCausalLM.from_pretrained(
                                str(model_path),
                                quantization_config=quantization_config,
                                torch_dtype=torch.float16,
                                device_map="auto",
                                low_cpu_mem_usage=True,
                                max_memory=max_memory,
                                trust_remote_code=True,
                                offload_folder=str(offload_dir)
                            )
                            logger.info("ãƒ•ãƒ«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿å®Œäº†ï¼ˆ4bité‡å­åŒ–ï¼‰")
                            
                        except Exception as e:
                            logger.warning(f"é‡å­åŒ–èª­ã¿è¾¼ã¿å¤±æ•—: {str(e)}")
                            logger.info("Ollamaãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’è©¦è¡Œ...")
                            
                            # Ollamaãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’è©¦è¡Œ
                            if OLLAMA_AVAILABLE:
                                try:
                                    logger.info("Ollamaãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’è©¦è¡Œä¸­...")
                                    ollama_integration = OllamaIntegration()
                                    
                                    # åˆ©ç”¨å¯èƒ½ãªOllamaãƒ¢ãƒ‡ãƒ«ã‚’ç¢ºèª
                                    available_models = ollama_integration.list_models()
                                    logger.info(f"åˆ©ç”¨å¯èƒ½ãªOllamaãƒ¢ãƒ‡ãƒ«: {available_models}")
                                    
                                    # åˆ©ç”¨å¯èƒ½ãªOllamaãƒ¢ãƒ‡ãƒ«ã‹ã‚‰é¸æŠ
                                    ollama_model_name = "llama3.2:3b"  # ç›´æ¥æŒ‡å®š
                                    logger.info(f"Ollamaãƒ¢ãƒ‡ãƒ« {ollama_model_name} ã‚’ä½¿ç”¨ã—ã¾ã™")
                                    
                                    # Ollamaã§ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
                                    result = ollama_integration.generate_text(
                                        model_name=ollama_model_name,
                                        prompt=request.prompt,
                                        temperature=request.temperature,
                                        top_p=request.top_p,
                                        max_tokens=request.max_length
                                    )
                                    
                                    if result.get("success", False):
                                        logger.info("Ollamaãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æˆåŠŸ")
                                        return {
                                            "prompt": request.prompt,
                                            "generated_text": result.get("generated_text", "Ollamaç”Ÿæˆã‚¨ãƒ©ãƒ¼"),
                                            "model_path": request.model_path,
                                            "fallback": "ollama",
                                            "method": "ollama",
                                            "note": "GPUãƒ¡ãƒ¢ãƒªä¸è¶³ã®ãŸã‚ã€Ollamaãƒ¢ãƒ‡ãƒ«ã§ç”Ÿæˆã—ã¾ã—ãŸ"
                                        }
                                    else:
                                        logger.warning(f"Ollamaç”Ÿæˆå¤±æ•—: {result.get('error', 'Unknown error')}")
                                except Exception as ollama_error:
                                    logger.error(f"Ollamaãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¤±æ•—: {str(ollama_error)}")
                                    import traceback
                                    logger.error(f"Ollamaã‚¨ãƒ©ãƒ¼è©³ç´°: {traceback.format_exc()}")
                            
                            # æœ€çµ‚æ‰‹æ®µ: CPUãƒ¢ãƒ¼ãƒ‰
                            logger.info("æœ€çµ‚æ‰‹æ®µ: CPUãƒ¢ãƒ¼ãƒ‰ã§èª­ã¿è¾¼ã¿ä¸­...")
                            try:
                                model = AutoModelForCausalLM.from_pretrained(
                                    str(model_path),
                                    torch_dtype=torch.float32,
                                    device_map=None,
                                    low_cpu_mem_usage=True,
                                    trust_remote_code=True
                                )
                                logger.info("CPUãƒ¢ãƒ¼ãƒ‰ã§ã®èª­ã¿è¾¼ã¿æˆåŠŸ")
                            except Exception as final_error:
                                logger.error(f"å…¨ã¦ã®èª­ã¿è¾¼ã¿æ–¹æ³•ãŒå¤±æ•—: {str(final_error)}")
                                return {
                                    "prompt": request.prompt,
                                    "generated_text": request.prompt + " [ã‚¨ãƒ©ãƒ¼: ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å¤±æ•— - GPUãƒ¡ãƒ¢ãƒªä¸è¶³]",
                                    "model_path": request.model_path,
                                    "error": f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {str(final_error)}"
                                }
                    else:
                        # CPUãƒ¢ãƒ¼ãƒ‰ã§ã®å®Ÿè¡Œ
                        logger.info("CPUãƒ¢ãƒ¼ãƒ‰ã§ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã™ï¼ˆGPUãŒåˆ©ç”¨ã§ããªã„å ´åˆï¼‰")
                        model = AutoModelForCausalLM.from_pretrained(
                            str(model_path),
                            torch_dtype=torch.float32,
                            device_map=None,
                            low_cpu_mem_usage=True,
                            trust_remote_code=True
                        )
                        logger.info("ãƒ•ãƒ«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿å®Œäº†ï¼ˆCPUï¼‰")
                
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
                model_cache[cache_key] = {
                    "tokenizer": tokenizer,
                    "model": model,
                    "base_model_name": base_model_name,
                    "training_method": training_method
                }
                logger.info("ãƒ¢ãƒ‡ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜å®Œäº†")
                
            except Exception as model_error:
                logger.error(f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(model_error)}")
                logger.error(traceback.format_exc())
                
                # GPUãƒ¡ãƒ¢ãƒªä¸è¶³ã®å ´åˆã®æ¨å¥¨äº‹é …ã‚’è¿½åŠ 
                error_message = str(model_error)
                if "CUDA out of memory" in error_message:
                    recommendation = """
                    GPUãƒ¡ãƒ¢ãƒªä¸è¶³ã®ãŸã‚ã€ä»¥ä¸‹ã®å¯¾ç­–ã‚’è©¦ã—ã¦ãã ã•ã„ï¼š
                    1. ã‚ˆã‚Šå°ã•ãªãƒ¢ãƒ‡ãƒ«ï¼ˆ7Bã‚„14Bï¼‰ã‚’ä½¿ç”¨ã™ã‚‹
                    2. ä»–ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’çµ‚äº†ã—ã¦GPUãƒ¡ãƒ¢ãƒªã‚’è§£æ”¾ã™ã‚‹
                    3. CPUãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã™ã‚‹ï¼ˆé€Ÿåº¦ã¯é…ããªã‚Šã¾ã™ï¼‰
                    """
                    error_message += recommendation
                
                return {
                    "prompt": request.prompt,
                    "generated_text": request.prompt + f" [ã‚¨ãƒ©ãƒ¼: ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å¤±æ•— - {error_message}]",
                    "model_path": request.model_path,
                    "error": error_message
                }
        
        # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®æ¤œè¨¼ç”¨ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
        logger.info("ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã§ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚’å®Ÿè¡Œã—ã¾ã™")
        
        # OllamaãŒåˆ©ç”¨å¯èƒ½ã§ã€ãƒ¡ãƒ¢ãƒªä¸è¶³ã®å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        if OLLAMA_AVAILABLE:
            # GPUãƒ¡ãƒ¢ãƒªã‚’ç¢ºèª
            try:
                if torch.cuda.is_available():
                    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
                    free_memory = torch.cuda.mem_get_info()[0] / 1024**3
                    logger.info(f"GPUãƒ¡ãƒ¢ãƒª: åˆè¨ˆ {gpu_memory:.1f}GB, ç©ºã {free_memory:.1f}GB")
                    
                    # ç©ºããƒ¡ãƒ¢ãƒªãŒ5GBæœªæº€ã®å ´åˆã¯Ollamaã‚’ä½¿ç”¨
                    if free_memory < 5:
                        logger.info(f"GPUãƒ¡ãƒ¢ãƒªä¸è¶³ï¼ˆç©ºã{free_memory:.1f}GBï¼‰ã®ãŸã‚ã€Ollamaã‚’ä½¿ç”¨ã—ã¾ã™")
                        ollama = OllamaIntegration()
                        
                        # åˆ©ç”¨å¯èƒ½ãªOllamaãƒ¢ãƒ‡ãƒ«ã‚’ç¢ºèª
                        available_models = ollama.list_models()
                        logger.info(f"åˆ©ç”¨å¯èƒ½ãªOllamaãƒ¢ãƒ‡ãƒ«: {available_models}")
                        
                        # ä½¿ç”¨ã™ã‚‹Ollamaãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ
                        ollama_model_name = None
                        if available_models.get("models"):
                            for model_info in available_models["models"]:
                                model_name_str = model_info.get("name", "")
                                if "llama3.2:3b" in model_name_str:
                                    ollama_model_name = model_name_str
                                    break
                            if not ollama_model_name and available_models["models"]:
                                ollama_model_name = available_models["models"][0].get("name")
                        
                        if ollama_model_name:
                            logger.info(f"Ollamaãƒ¢ãƒ‡ãƒ« {ollama_model_name} ã‚’ä½¿ç”¨ã—ã¾ã™")
                            
                            # Ollamaã§ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
                            result = ollama.generate_text(
                                model_name=ollama_model_name,
                                prompt=request.prompt,
                                temperature=request.temperature,
                                top_p=request.top_p,
                                max_tokens=request.max_length
                            )
                            
                            if result.get("success", False):
                                logger.info("Ollamaã§ã®ç”ŸæˆãŒæˆåŠŸã—ã¾ã—ãŸ")
                                return {
                                    "prompt": request.prompt,
                                    "generated_text": result.get("generated_text", "Ollamaç”Ÿæˆã‚¨ãƒ©ãƒ¼"),
                                    "model_path": request.model_path,
                                    "method": "ollama",
                                    "note": "GPUãƒ¡ãƒ¢ãƒªä¸è¶³ã®ãŸã‚ã€Ollamaãƒ¢ãƒ‡ãƒ«ã§ç”Ÿæˆã—ã¾ã—ãŸ",
                                    "verification_info": {
                                        "model_path": request.model_path,
                                        "base_model": "ollama-converted",
                                        "training_method": "full",
                                        "prompt": request.prompt,
                                        "generation_params": {
                                            "max_length": request.max_length,
                                            "temperature": request.temperature,
                                            "top_p": request.top_p
                                        }
                                    }
                                }
                            else:
                                logger.warning(f"Ollamaç”Ÿæˆå¤±æ•—: {result.get('error', 'Unknown error')}")
                                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: é€šå¸¸ã®æ–¹æ³•ã‚’è©¦è¡Œ
            except Exception as ollama_error:
                logger.error(f"Ollamaçµ±åˆã‚¨ãƒ©ãƒ¼: {str(ollama_error)}")
                import traceback
                logger.error(f"Ollamaã‚¨ãƒ©ãƒ¼è©³ç´°: {traceback.format_exc()}")
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: é€šå¸¸ã®æ–¹æ³•ã‚’è©¦è¡Œ
        
        # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã®è¨˜éŒ²
        model_info = {
            "model_path": request.model_path,
            "base_model": cached_model.get("base_model_name", "unknown"),
            "training_method": cached_model.get("training_method", "unknown"),
            "prompt": request.prompt,
            "generation_params": {
                "max_length": request.max_length,
                "temperature": request.temperature,
                "top_p": request.top_p
            }
        }
        
        logger.info(f"ãƒ¢ãƒ‡ãƒ«æƒ…å ±: {model_info}")
        
        # é€šå¸¸ã®æ–¹æ³•ï¼ˆTransformersï¼‰ã‚’ä½¿ç”¨
        cached_model = model_cache[cache_key]
        tokenizer = cached_model["tokenizer"]
        model = cached_model["model"]
        
        # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
        logger.info("ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚’å®Ÿè¡Œä¸­...")
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
        inputs = tokenizer(
            request.prompt,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=512
        )
        
        # GPUã«ç§»å‹•
        if torch.cuda.is_available() and hasattr(model, 'device'):
            try:
                inputs = {k: v.to(model.device) for k, v in inputs.items()}
            except:
                # ãƒ‡ãƒã‚¤ã‚¹ç§»å‹•ã«å¤±æ•—ã—ãŸå ´åˆã¯ãã®ã¾ã¾ç¶šè¡Œ
                pass
        
        # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®æ¤œè¨¼ç”¨ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
        model.eval()
        with torch.no_grad():
            try:
                # ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¨­å®š
                generation_kwargs = {
                    'input_ids': inputs['input_ids'],
                    'attention_mask': inputs.get('attention_mask'),
                    'max_new_tokens': request.max_length,
                    'pad_token_id': tokenizer.eos_token_id,
                    'eos_token_id': tokenizer.eos_token_id,
                    'repetition_penalty': 1.2,  # ç¹°ã‚Šè¿”ã—ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’è¿½åŠ 
                    'no_repeat_ngram_size': 3,  # 3-gramã®ç¹°ã‚Šè¿”ã—ã‚’é˜²ã
                }
                
                # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã®ã¿temperatureã¨top_pã‚’è¨­å®š
                if request.temperature > 0.0:
                    generation_kwargs['do_sample'] = True
                    generation_kwargs['temperature'] = request.temperature
                    generation_kwargs['top_p'] = request.top_p
                else:
                    generation_kwargs['do_sample'] = False
                
                logger.info(f"ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {generation_kwargs}")
                logger.info("model.generate()ã‚’å®Ÿè¡Œä¸­...")
                
                outputs = model.generate(**generation_kwargs)
                
                logger.info(f"ç”Ÿæˆå®Œäº†: å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°={outputs.shape}")
                
                # ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
                generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
                logger.info(f"ãƒ‡ã‚³ãƒ¼ãƒ‰å®Œäº†: ãƒ†ã‚­ã‚¹ãƒˆé•·={len(generated_text)}")
                
                # å…ƒã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’é™¤å»ã—ã¦æ–°ã—ã„éƒ¨åˆ†ã ã‘ã‚’å–å¾—
                if generated_text.startswith(request.prompt):
                    new_text = generated_text[len(request.prompt):].strip()
                    if new_text:
                        # ã€Œ- äº¤é€šå·¥å­¦ã®å•é¡Œã§ã™ã€‚ã€ã®ã‚ˆã†ãªç¹°ã‚Šè¿”ã—ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡ºã—ã¦å‰Šé™¤
                        import re
                        # åŒã˜ãƒ•ãƒ¬ãƒ¼ã‚ºãŒ3å›ä»¥ä¸Šç¹°ã‚Šè¿”ã•ã‚Œã‚‹å ´åˆã¯ã€æœ€åˆã®1å›ã ã‘æ®‹ã™
                        pattern = r'((?:^|\n)?(?:- )?[^\n]+?)(?:\n?\1){2,}'
                        new_text = re.sub(pattern, r'\1', new_text)
                        
                        generated_text = request.prompt + "\n" + new_text
                    else:
                        generated_text = request.prompt + " [ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã§ã—ãŸ]"
                
                logger.info(f"ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆå®Œäº†: {len(generated_text)}æ–‡å­—")
                
                # ç”Ÿæˆçµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
                try:
                    project_root = Path(os.getcwd())
                    outputs_dir = project_root / "outputs"
                    outputs_dir.mkdir(exist_ok=True)
                    
                    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ããƒ•ã‚¡ã‚¤ãƒ«å
                    timestamp = datetime.now(JST).strftime("%Y%m%d_%H%M%S")
                    model_name = Path(request.model_path).name
                    output_filename = f"generated_text_{model_name}_{timestamp}.json"
                    output_path = outputs_dir / output_filename
                    
                    # ç”Ÿæˆçµæœã‚’ä¿å­˜
                    generation_result = {
                        "timestamp": timestamp,
                        "model_path": request.model_path,
                        "base_model": cached_model.get("base_model_name", "unknown"),
                        "training_method": cached_model.get("training_method", "unknown"),
                        "prompt": request.prompt,
                        "generated_text": generated_text,
                        "parameters": {
                            "max_length": request.max_length,
                            "temperature": request.temperature,
                            "top_p": request.top_p
                        }
                    }
                    
                    with open(output_path, 'w', encoding='utf-8') as f:
                        json.dump(generation_result, f, indent=2, ensure_ascii=False)
                    
                    logger.info(f"ç”Ÿæˆçµæœã‚’ä¿å­˜: {output_path}")
                    
                except Exception as save_error:
                    logger.warning(f"ç”Ÿæˆçµæœã®ä¿å­˜ã«å¤±æ•—: {save_error}")
                
                # æ¤œè¨¼çµæœã®è©³ç´°æƒ…å ±ã‚’è¨˜éŒ²
                verification_info = {
                    "model_path": request.model_path,
                    "base_model": cached_model.get("base_model_name", "unknown"),
                    "training_method": cached_model.get("training_method", "unknown"),
                    "prompt": request.prompt,
                    "generated_text": generated_text,
                    "generation_params": {
                        "max_length": request.max_length,
                        "temperature": request.temperature,
                        "top_p": request.top_p
                    },
                    "model_info": {
                        "total_tokens": len(generated_ids),
                        "input_tokens": input_length,
                        "generated_tokens": len(generated_ids) - input_length
                    }
                }
                
                # æ¤œè¨¼çµæœã‚’ãƒ­ã‚°ã«è¨˜éŒ²
                logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«æ¤œè¨¼çµæœ: {verification_info}")
                
                return {
                    "prompt": request.prompt,
                    "generated_text": generated_text,
                    "model_path": request.model_path,
                    "base_model": cached_model.get("base_model_name", "unknown"),
                    "training_method": cached_model.get("training_method", "unknown"),
                    "verification_info": verification_info
                }
                
            except Exception as gen_error:
                logger.error(f"ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(gen_error)}")
                logger.error(traceback.format_exc())
                return {
                    "prompt": request.prompt,
                    "generated_text": request.prompt + f" [ã‚¨ãƒ©ãƒ¼: ç”Ÿæˆå¤±æ•— - {str(gen_error)}]",
                    "model_path": request.model_path,
                    "error": str(gen_error)
                }
        
    except Exception as e:
        logger.error(f"Generation error: {str(e)}")
        logger.error(traceback.format_exc())
        return {
            "prompt": request.prompt,
            "generated_text": request.prompt + f" [ã‚¨ãƒ©ãƒ¼: {str(e)}]",
            "model_path": request.model_path,
            "error": str(e)
        }

@app.post("/api/verify-model")
async def verify_finetuned_model(request: GenerationRequest):
    """ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®æ¤œè¨¼å°‚ç”¨ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    try:
        logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«æ¤œè¨¼é–‹å§‹: {request.model_path}")
        
        # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã®å–å¾—
        model_path = Path(request.model_path)
        if not model_path.exists():
            return {
                "error": "ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“",
                "model_path": request.model_path
            }
        
        # æ¤œè¨¼ç”¨ã®ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
        test_cases = [
            "ç¸¦æ–­æ›²ç·šã¨ã¯ä½•ã®ãŸã‚ã«è¨­ç½®ã—ã¾ã™ã‹ï¼Ÿ",
            "é“è·¯ã®æ¨ªæ–­å‹¾é…ã®æ¨™æº–çš„ãªå€¤ã¯ã©ã®ãã‚‰ã„ã§ã™ã‹ï¼Ÿ",
            "ã‚¢ã‚¹ãƒ•ã‚¡ãƒ«ãƒˆèˆ—è£…ã®ä¸»ãªåˆ©ç‚¹ã¨æ¬ ç‚¹ã¯ä½•ã§ã™ã‹ï¼Ÿ",
            "è¨­è¨ˆCBRã¨ã¯èˆ—è£…è¨­è¨ˆã«ãŠã„ã¦ã©ã®ã‚ˆã†ãªæŒ‡æ¨™ã§ã™ã‹ï¼Ÿ",
            "é“è·¯ã®å¹³é¢ç·šå½¢ã‚’æ§‹æˆã™ã‚‹3ã¤ã®è¦ç´ ã¯ä½•ã§ã™ã‹ï¼Ÿ"
        ]
        
        verification_results = []
        
        for i, test_prompt in enumerate(test_cases):
            logger.info(f"ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ {i+1}/{len(test_cases)}: {test_prompt}")
            
            # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ä½œæˆ
            gen_request = GenerationRequest(
                model_path=request.model_path,
                prompt=test_prompt,
                max_length=request.max_length,
                temperature=request.temperature,
                top_p=request.top_p
            )
            
            # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚’å®Ÿè¡Œ
            result = await generate_text(gen_request)
            
            # æ¤œè¨¼çµæœã‚’è¨˜éŒ²
            verification_result = {
                "test_case": i + 1,
                "prompt": test_prompt,
                "generated_text": result.get("generated_text", ""),
                "verification_info": result.get("verification_info", {}),
                "success": "error" not in result
            }
            
            verification_results.append(verification_result)
            
            # é€²æ—ã‚’ãƒ­ã‚°ã«è¨˜éŒ²
            logger.info(f"ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ {i+1} å®Œäº†: {'æˆåŠŸ' if verification_result['success'] else 'å¤±æ•—'}")
        
        # æ¤œè¨¼ã‚µãƒãƒªãƒ¼ã‚’ä½œæˆ
        success_count = sum(1 for r in verification_results if r["success"])
        total_count = len(verification_results)
        
        verification_summary = {
            "model_path": request.model_path,
            "total_test_cases": total_count,
            "successful_tests": success_count,
            "success_rate": success_count / total_count if total_count > 0 else 0,
            "verification_results": verification_results
        }
        
        logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«æ¤œè¨¼å®Œäº†: æˆåŠŸç‡ {success_count}/{total_count}")
        
        return {
            "status": "success",
            "verification_summary": verification_summary
        }
        
    except Exception as e:
        logger.error(f"ãƒ¢ãƒ‡ãƒ«æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {str(e)}")
        logger.error(traceback.format_exc())
        return {
            "status": "error",
            "error": str(e),
            "model_path": request.model_path
        }

@app.post("/api/save-verification")
async def save_verification_results(verification_data: dict):
    """ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®æ¤œè¨¼çµæœã‚’ä¿å­˜"""
    try:
        # ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
        project_root = Path(os.getcwd())
        verification_dir = project_root / "verification_results"
        verification_dir.mkdir(exist_ok=True)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åã®ç”Ÿæˆ
        timestamp = datetime.now(JST).strftime("%Y%m%d_%H%M%S")
        model_name = verification_data.get("model_path", "unknown").split("/")[-1]
        filename = f"verification_{model_name}_{timestamp}.json"
        
        # æ¤œè¨¼çµæœã‚’ä¿å­˜
        output_path = verification_dir / filename
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(verification_data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"æ¤œè¨¼çµæœã‚’ä¿å­˜: {output_path}")
        
        return {
            "status": "success",
            "saved_path": str(output_path),
            "filename": filename
        }
        
    except Exception as e:
        logger.error(f"æ¤œè¨¼çµæœä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return {
            "status": "error",
            "error": str(e)
        }

@app.get("/api/system-info")
async def get_system_info():
    """ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã‚’å–å¾—"""
    try:
        # GPUæƒ…å ±
        gpu_info = []
        if torch.cuda.is_available():
            gpu_count = torch.cuda.device_count()
            if gpu_count > 0:
                # å…¨ã¦ã®GPUã®æƒ…å ±ã‚’å–å¾—
                for i in range(gpu_count):
                    gpu_name = torch.cuda.get_device_name(i)
                    gpu_memory_total = torch.cuda.get_device_properties(i).total_memory / (1024**3)
                    gpu_memory_used = torch.cuda.memory_allocated(i) / (1024**3)
                    gpu_memory_free = gpu_memory_total - gpu_memory_used
                    
                    gpu_info.append({
                        "device": i,
                        "name": gpu_name,
                        "memory": f"{gpu_memory_total:.1f}GB",
                        "memory_used": f"{gpu_memory_used:.1f}GB",
                        "memory_free": f"{gpu_memory_free:.1f}GB",
                        "available": True
                    })
            else:
                gpu_info = [{
                    "device": 0,
                    "name": "No GPU",
                    "memory": "0GB",
                    "available": False
                }]
        else:
            gpu_info = [{
                "device": 0,
                "name": "CUDA Not Available",
                "memory": "0GB",
                "available": False
            }]
        
        # CUDAæƒ…å ±
        cuda_info = {
            "available": torch.cuda.is_available(),
            "version": torch.version.cuda if torch.cuda.is_available() else "Not Available"
        }
        
        # PyTorchæƒ…å ±
        pytorch_info = {
            "version": torch.__version__
        }
        
        # CPUæƒ…å ±
        cpu_info = {
            "name": "CPU",
            "cores": os.cpu_count()
        }
        
        # RAMæƒ…å ±
        memory = psutil.virtual_memory()
        ram_info = {
            "total": f"{memory.total / (1024**3):.1f}GB",
            "used": f"{memory.used / (1024**3):.1f}GB",
            "free": f"{memory.available / (1024**3):.1f}GB",
            "percent": f"{memory.percent:.1f}%"
        }
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æƒ…å ±
        cache_info = {
            "status": f"{len(model_cache)} models cached" if len(model_cache) > 0 else "No models cached"
        }
        
        return {
            "gpu": gpu_info,
            "cuda": cuda_info,
            "pytorch": pytorch_info,
            "cpu": cpu_info,
            "ram": ram_info,
            "cache": cache_info
        }
    except Exception as e:
        logger.error(f"ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return {
            "error": str(e),
            "gpu": {"name": "Error", "memory": "Unknown", "available": False},
            "cuda": {"available": False, "version": "Unknown"},
            "pytorch": {"version": "Unknown"},
            "cpu": {"name": "Unknown", "cores": "Unknown"},
            "ram": {"total": "Unknown", "used": "Unknown", "free": "Unknown", "percent": "Unknown"},
            "cache": {"status": "Unknown"}
        }

@app.get("/metrics")
async def get_metrics():
    """Prometheusãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    if METRICS_AVAILABLE and metrics_collector:
        try:
            # ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æ›´æ–°
            metrics_collector.update_system_metrics()
            
            # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¿ã‚¹ã‚¯æ•°ã‚’æ›´æ–°
            active_tasks = sum(1 for task in training_tasks.values() if task["status"] == "running")
            metrics_collector.set_active_training_tasks(active_tasks)
            
            # RAGæ–‡æ›¸æ•°ã‚’æ›´æ–°ï¼ˆRAGãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
            if RAG_AVAILABLE:
                try:
                    metadata_manager = MetadataManager()
                    doc_count = len(metadata_manager.get_all_documents())
                    metrics_collector.set_rag_documents_count(doc_count)
                except:
                    pass
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¿”ã™
            return get_prometheus_metrics()
        except Exception as e:
            logger.error(f"ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            from fastapi.responses import Response
            return Response(content="# Error generating metrics\n", media_type="text/plain")
    else:
        from fastapi.responses import Response
        return Response(content="# Metrics not available\n", media_type="text/plain")

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹•æ™‚ã®åˆæœŸåŒ–
@app.on_event("startup")
async def startup_event():
    """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹•æ™‚ã®åˆæœŸåŒ–"""
    logger.info("AI Fine-tuning Toolkit Web API starting...")
    
    # å¿…è¦ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    project_root = Path(os.getcwd())
    (project_root / "data" / "uploaded").mkdir(parents=True, exist_ok=True)
    (project_root / "outputs").mkdir(parents=True, exist_ok=True)
    (project_root / "app" / "static").mkdir(parents=True, exist_ok=True)
    (project_root / "data" / "continual_learning").mkdir(parents=True, exist_ok=True)
    
    # ç¶™ç¶šå­¦ç¿’ã‚¿ã‚¹ã‚¯ã‚’èª­ã¿è¾¼ã‚€
    load_continual_tasks()

@app.on_event("shutdown")
async def shutdown_event():
    """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³çµ‚äº†æ™‚ã®å‡¦ç†"""
    logger.info("AI Fine-tuning Toolkit Web API shutting down...")
    
    # ç¶™ç¶šå­¦ç¿’ã‚¿ã‚¹ã‚¯ã‚’ä¿å­˜
    save_continual_tasks()
    
    logger.info("Shutdown complete.")

@app.post("/api/clear_cache")
async def clear_model_cache():
    """ãƒ¢ãƒ‡ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"""
    try:
        global model_cache
        cache_size = len(model_cache)
        model_cache.clear()
        torch.cuda.empty_cache()
        import gc
        gc.collect()
        logger.info(f"ãƒ¢ãƒ‡ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ: {cache_size}å€‹ã®ãƒ¢ãƒ‡ãƒ«ã‚’è§£æ”¾")
        return {"status": "success", "cleared_models": cache_size}
    except Exception as e:
        logger.error(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return {"status": "error", "error": str(e)}

@app.post("/api/convert-to-ollama")
async def convert_finetuned_to_ollama(request: dict):
    """ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’Ollamaå½¢å¼ã«å¤‰æ›"""
    try:
        model_path = request.get("model_path")
        model_name = request.get("model_name", "road-engineering-expert")
        
        if not model_path:
            return {"success": False, "error": "model_pathãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“"}
        
        logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®Ollamaå¤‰æ›é–‹å§‹: {model_path}")
        
        # å¤‰æ›ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œ
        import subprocess
        import sys
        
        # å¤‰æ›ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ãƒ‘ã‚¹
        script_path = Path("convert_finetuned_to_ollama.py")
        
        if not script_path.exists():
            return {"success": False, "error": "å¤‰æ›ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"}
        
        # å¤‰æ›å®Ÿè¡Œ
        cmd = [sys.executable, str(script_path)]
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=7200  # 2æ™‚é–“ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
        )
        
        if result.returncode == 0:
            logger.info("ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®Ollamaå¤‰æ›ãŒå®Œäº†ã—ã¾ã—ãŸ")
            return {
                "success": True,
                "model_name": model_name,
                "message": "ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒOllamaã§ä½¿ç”¨å¯èƒ½ã«ãªã‚Šã¾ã—ãŸ",
                "usage": f"ollama run {model_name}"
            }
        else:
            logger.error(f"å¤‰æ›ã‚¨ãƒ©ãƒ¼: {result.stderr}")
            return {
                "success": False,
                "error": result.stderr
            }
            
    except subprocess.TimeoutExpired:
        logger.error("å¤‰æ›ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸ")
        return {"success": False, "error": "Conversion timeout"}
    except Exception as e:
        logger.error(f"å¤‰æ›ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return {"success": False, "error": str(e)}

@app.post("/api/convert-to-ollama-wsl")
async def convert_finetuned_to_ollama_wsl(request: dict):
    """WSLç’°å¢ƒç”¨ï¼šãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’Ollamaå½¢å¼ã«å¤‰æ›"""
    try:
        model_path = request.get("model_path")
        model_name = request.get("model_name", "road-engineering-expert")
        
        if not model_path:
            return {"success": False, "error": "model_pathãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“"}
        
        logger.info(f"WSLç’°å¢ƒã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®Ollamaå¤‰æ›é–‹å§‹: {model_path}")
        
        # WSLç’°å¢ƒç”¨å¤‰æ›ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œ
        import subprocess
        import sys
        
        # å¤‰æ›ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ãƒ‘ã‚¹
        script_path = Path("setup_wsl_ollama.py")
        
        if not script_path.exists():
            return {"success": False, "error": "WSLå¤‰æ›ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"}
        
        # å¤‰æ›å®Ÿè¡Œ
        cmd = [sys.executable, str(script_path)]
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=7200  # 2æ™‚é–“ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
        )
        
        if result.returncode == 0:
            logger.info("WSLç’°å¢ƒã§ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®Ollamaå¤‰æ›ãŒå®Œäº†ã—ã¾ã—ãŸ")
            return {
                "success": True,
                "model_name": model_name,
                "message": "WSLç’°å¢ƒã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒOllamaã§ä½¿ç”¨å¯èƒ½ã«ãªã‚Šã¾ã—ãŸ",
                "usage": f"ollama run {model_name}"
            }
        else:
            logger.error(f"WSLå¤‰æ›ã‚¨ãƒ©ãƒ¼: {result.stderr}")
            return {
                "success": False,
                "error": result.stderr
            }
            
    except subprocess.TimeoutExpired:
        logger.error("WSLå¤‰æ›ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸ")
        return {"success": False, "error": "Conversion timeout"}
    except Exception as e:
        logger.error(f"WSLå¤‰æ›ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return {"success": False, "error": str(e)}

@app.get("/api/available-models")
async def get_available_models():
    """åˆ©ç”¨å¯èƒ½ãªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã¨Ollamaãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—"""
    try:
        models = {
            "finetuned_models": [],
            "ollama_models": []
        }
        
        # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®æ¤œç´¢
        outputs_dir = Path("outputs")
        if outputs_dir.exists():
            for model_dir in outputs_dir.iterdir():
                if model_dir.is_dir():
                    # ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
                    has_model_files = (
                        (model_dir / "pytorch_model.bin").exists() or
                        (model_dir / "adapter_model.safetensors").exists() or
                        (model_dir / "adapter_config.json").exists() or
                        (model_dir / "config.json").exists()
                    )
                    
                    if has_model_files:
                        # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’å–å¾—
                        config_path = model_dir / "config.json"
                        training_info_path = model_dir / "training_info.json"
                        
                        model_info = {
                            "name": model_dir.name,
                            "path": str(model_dir),
                            "type": "finetuned",
                            "size": "Unknown",
                            "created": "Unknown"
                        }
                        
                        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æƒ…å ±ã‚’èª­ã¿å–ã‚Š
                        if config_path.exists():
                            try:
                                with open(config_path, 'r', encoding='utf-8') as f:
                                    config = json.load(f)
                                    model_info["base_model"] = config.get("_name_or_path", "Unknown")
                                    model_info["model_type"] = config.get("model_type", "Unknown")
                            except:
                                pass
                        
                        # è¨“ç·´æƒ…å ±ã‹ã‚‰è©³ç´°ã‚’å–å¾—
                        if training_info_path.exists():
                            try:
                                with open(training_info_path, 'r', encoding='utf-8') as f:
                                    training_info = json.load(f)
                                    model_info["training_method"] = training_info.get("training_method", "unknown")
                                    model_info["created"] = training_info.get("timestamp", "Unknown")
                            except:
                                pass
                        
                        # ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ã®åˆ¤å®š
                        if "qlora" in model_dir.name.lower() or "4bit" in model_dir.name.lower():
                            model_info["training_method"] = "qlora"
                            model_info["size"] = "~1.0MB"
                        elif "lora" in model_dir.name.lower():
                            model_info["training_method"] = "lora"
                            model_info["size"] = "~1.6MB"
                        elif "ãƒ•ãƒ«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°" in model_dir.name:
                            model_info["training_method"] = "full"
                            model_info["size"] = "~500MB+"
                        
                        models["finetuned_models"].append(model_info)
                        logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’æ¤œå‡º: {model_dir.name}")
        
        # Ollamaãƒ¢ãƒ‡ãƒ«ã®æ¤œç´¢
        if OLLAMA_AVAILABLE:
            try:
                ollama = OllamaIntegration()
                ollama_models = ollama.list_models()
                logger.debug(f"Ollamaãƒ¢ãƒ‡ãƒ«å–å¾—çµæœ: {ollama_models}")
                
                if ollama_models.get("success", False):
                    for model in ollama_models.get("models", []):
                        # å…¨ã¦ã®Ollamaãƒ¢ãƒ‡ãƒ«ã‚’è¡¨ç¤ºï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’å‰Šé™¤ï¼‰
                        models["ollama_models"].append({
                            "name": model.get("name", "Unknown"),
                            "type": "ollama",
                            "size": model.get("size", "Unknown"),
                            "modified": model.get("modified", "Unknown")
                        })
                else:
                    logger.warning(f"Ollamaãƒ¢ãƒ‡ãƒ«å–å¾—å¤±æ•—: {ollama_models.get('error', 'Unknown error')}")
            except Exception as e:
                logger.warning(f"Ollamaãƒ¢ãƒ‡ãƒ«å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
                import traceback
                logger.error(f"è©³ç´°ã‚¨ãƒ©ãƒ¼: {traceback.format_exc()}")
                # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ç©ºã®ãƒªã‚¹ãƒˆã‚’è¿”ã™ï¼ˆã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’åœæ­¢ã•ã›ãªã„ï¼‰
                models["ollama_models"] = []
        
        return models
        
    except Exception as e:
        logger.error(f"ãƒ¢ãƒ‡ãƒ«ä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return {"finetuned_models": [], "ollama_models": [], "error": str(e)}

@app.delete("/api/models/{model_name}")
async def delete_model(model_name: str):
    """ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’å‰Šé™¤"""
    import shutil
    
    try:
        # ãƒ‘ã‚¹ã®å®‰å…¨æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒˆãƒ©ãƒãƒ¼ã‚µãƒ«å¯¾ç­–ï¼‰
        # URLã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚‚è€ƒæ…®
        if ".." in model_name or "/" in model_name or "\\" in model_name or "%2F" in model_name or "%2f" in model_name:
            return JSONResponse(
                status_code=400,
                content={"success": False, "error": "Invalid model name"}
            )
        
        # ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ã‚’æ§‹ç¯‰
        model_path = Path("outputs") / model_name
        
        # ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å­˜åœ¨ç¢ºèª
        if not model_path.exists():
            return JSONResponse(
                status_code=404,
                content={"success": False, "error": f"Model '{model_name}' not found"}
            )
        
        # ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã©ã†ã‹ç¢ºèª
        if not model_path.is_dir():
            return JSONResponse(
                status_code=400,
                content={"success": False, "error": "Invalid model path"}
            )
        
        # outputsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªé…ä¸‹ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
        outputs_dir = Path("outputs").resolve()
        model_path_resolved = model_path.resolve()
        if not str(model_path_resolved).startswith(str(outputs_dir)):
            return JSONResponse(
                status_code=400,
                content={"success": False, "error": "Invalid model location"}
            )
        
        # ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å‰Šé™¤
        logger.info(f"Deleting model: {model_name}")
        shutil.rmtree(model_path)
        
        logger.info(f"Model '{model_name}' deleted successfully")
        return {"success": True, "message": f"Model '{model_name}' deleted successfully"}
        
    except PermissionError:
        logger.error(f"Permission denied when deleting model: {model_name}")
        return JSONResponse(
            status_code=403,
            content={"success": False, "error": "Permission denied"}
        )
    except Exception as e:
        logger.error(f"Error deleting model '{model_name}': {str(e)}")
        return JSONResponse(
            status_code=500,
            content={"success": False, "error": str(e)}
        )

@app.post("/api/generate-stream")
async def generate_text_stream(request: dict):
    """ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œã®ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
    from fastapi.responses import StreamingResponse
    import asyncio
    
    async def generate_stream():
        try:
            model_name = request.get("model_name")
            model_type = request.get("model_type")
            prompt = request.get("prompt")
            max_length = request.get("max_length", 2048)
            temperature = request.get("temperature", 0.7)
            top_p = request.get("top_p", 0.9)
            
            if not model_name or not model_type or not prompt:
                yield f"data: {json.dumps({'error': 'model_name, model_type, promptãŒå¿…è¦ã§ã™'})}\n\n"
                return
            
            logger.info(f"ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”Ÿæˆé–‹å§‹: {model_type}/{model_name}")
            
            if model_type == "ollama":
                # Ollamaã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”Ÿæˆ
                if not OLLAMA_AVAILABLE:
                    yield f"data: {json.dumps({'error': 'OllamaãŒåˆ©ç”¨ã§ãã¾ã›ã‚“'})}\n\n"
                    return
                
                ollama = OllamaIntegration()
                
                # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”¨ã®Ollamaãƒªã‚¯ã‚¨ã‚¹ãƒˆ
                ollama_params = {
                    "model": model_name,
                    "prompt": prompt,
                    "stream": True,
                    "options": {
                        "temperature": temperature,
                        "top_p": top_p,
                        "num_predict": max_length
                    }
                }
                
                try:
                    response = requests.post(
                        f"{ollama.base_url}/api/generate",
                        json=ollama_params,
                        stream=True,
                        timeout=300
                    )
                    
                    if response.status_code == 200:
                        for line in response.iter_lines():
                            if line:
                                data = json.loads(line.decode('utf-8'))
                                if 'response' in data:
                                    yield f"data: {json.dumps({'text': data['response'], 'done': False})}\n\n"
                                if data.get('done', False):
                                    yield f"data: {json.dumps({'text': '', 'done': True})}\n\n"
                                    break
                    else:
                        yield f"data: {json.dumps({'error': f'Ollama API ã‚¨ãƒ©ãƒ¼: {response.status_code}'})}\n\n"
                        
                except Exception as e:
                    logger.error(f"Ollamaã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
                    yield f"data: {json.dumps({'error': str(e)})}\n\n"
            
            elif model_type == "finetuned":
                # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”Ÿæˆ
                model_path = f"outputs/{model_name}"
                
                try:
                    # ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—ã¾ãŸã¯æ–°è¦èª­ã¿è¾¼ã¿ï¼‰
                    if model_path not in model_cache:
                        logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­: {model_path}")
                        
                        # GPU ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–è¨­å®š
                        max_memory = {}
                        if torch.cuda.is_available():
                            for i in range(torch.cuda.device_count()):
                                max_memory[i] = "18GB"
                            max_memory["cpu"] = "30GB"
                        
                        # ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿
                        tokenizer = load_tokenizer(model_path)
                        
                        # é‡å­åŒ–è¨­å®š
                        quantization_config = create_quantization_config(model_path, "lora", force_4bit=True)
                        
                        model = AutoModelForCausalLM.from_pretrained(
                            model_path,
                            quantization_config=quantization_config,
                            torch_dtype=torch.float16,
                            device_map="auto",
                            low_cpu_mem_usage=True,
                            max_memory=max_memory,
                            trust_remote_code=True
                        )
                        
                        model_cache[model_path] = {
                            "tokenizer": tokenizer,
                            "model": model
                        }
                    
                    cached_model = model_cache[model_path]
                    tokenizer = cached_model["tokenizer"]
                    model = cached_model["model"]
                    
                    # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”Ÿæˆ
                    inputs = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True, max_length=512)
                    
                    if torch.cuda.is_available():
                        inputs = {k: v.to(model.device) for k, v in inputs.items()}
                    
                    model.eval()
                    
                    # ãƒ‡ãƒã‚¤ã‚¹ã‚’å–å¾—ï¼ˆãƒ‡ãƒã‚¤ã‚¹çµ±ä¸€ã®ãŸã‚ï¼‰
                    device = inputs['input_ids'].device
                    logger.info(f"ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”Ÿæˆãƒ‡ãƒã‚¤ã‚¹: {device}")
                    
                    # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”Ÿæˆã®å®Ÿè£…
                    generated_tokens = []
                    current_text = ""
                    token_buffer = []  # æ–‡å­—åŒ–ã‘é˜²æ­¢ç”¨ãƒãƒƒãƒ•ã‚¡
                    
                    with torch.no_grad():
                        for _ in range(max_length):
                            outputs = model.generate(
                                input_ids=inputs['input_ids'],
                                attention_mask=inputs.get('attention_mask'),
                                max_new_tokens=1,
                                pad_token_id=tokenizer.eos_token_id,
                                eos_token_id=tokenizer.eos_token_id,
                                do_sample=temperature > 0.0,
                                temperature=temperature if temperature > 0.0 else 1.0,
                                top_p=top_p if temperature > 0.0 else 1.0,
                                repetition_penalty=1.2,
                                no_repeat_ngram_size=3
                            )
                            
                            new_token = outputs[0][-1].unsqueeze(0)
                            # ãƒ‡ãƒã‚¤ã‚¹ã‚’çµ±ä¸€
                            new_token = new_token.to(device)
                            generated_tokens.append(new_token)
                            
                            # ãƒˆãƒ¼ã‚¯ãƒ³ãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ 
                            token_buffer.append(new_token)
                            
                            # ãƒãƒƒãƒ•ã‚¡ã‹ã‚‰å®Œå…¨ãªæ–‡å­—ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
                            if len(token_buffer) >= 1:  # 1ãƒˆãƒ¼ã‚¯ãƒ³ã”ã¨ã«ãƒ‡ã‚³ãƒ¼ãƒ‰
                                try:
                                    # ãƒãƒƒãƒ•ã‚¡å†…ã®å…¨ãƒˆãƒ¼ã‚¯ãƒ³ã‚’çµåˆã—ã¦ãƒ‡ã‚³ãƒ¼ãƒ‰
                                    buffer_tokens = torch.cat(token_buffer, dim=0)
                                    decoded_text = tokenizer.decode(buffer_tokens, skip_special_tokens=True)
                                    
                                    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±
                                    logger.debug(f"ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚º: {len(token_buffer)}, ãƒ‡ã‚³ãƒ¼ãƒ‰çµæœ: '{decoded_text}', ç¾åœ¨ã®ãƒ†ã‚­ã‚¹ãƒˆ: '{current_text}'")
                                    
                                    # å‰å›ã®ãƒ‡ã‚³ãƒ¼ãƒ‰çµæœã¨æ¯”è¼ƒã—ã¦æ–°ã—ã„éƒ¨åˆ†ã®ã¿ã‚’æŠ½å‡º
                                    if len(current_text) < len(decoded_text):
                                        new_text = decoded_text[len(current_text):]
                                        current_text = decoded_text
                                        logger.debug(f"æ–°ã—ã„ãƒ†ã‚­ã‚¹ãƒˆ: '{new_text}'")
                                        
                                        # æ–‡å­—åŒ–ã‘ãƒã‚§ãƒƒã‚¯ï¼ˆæ”¹å–„ç‰ˆï¼‰
                                        if new_text and new_text.strip():  # ç©ºç™½æ–‡å­—ä»¥å¤–ã¯æœ‰åŠ¹
                                            # åŸºæœ¬çš„ãªæ–‡å­—åŒ–ã‘ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
                                            invalid_chars = ['', '\ufffd', '\u0000', '\u0001', '\u0002', '\u0003']
                                            has_invalid = any(char in new_text for char in invalid_chars)
                                            
                                            if not has_invalid:
                                                yield f"data: {json.dumps({'text': new_text, 'done': False})}\n\n"
                                            else:
                                                logger.warning(f"æ–‡å­—åŒ–ã‘æ¤œå‡º: {new_text}")
                                        else:
                                            logger.debug(f"ç©ºç™½æ–‡å­—ã‚¹ã‚­ãƒƒãƒ—: '{new_text}'")
                                    
                                    # ãƒãƒƒãƒ•ã‚¡ã‚’ã‚¯ãƒªã‚¢ï¼ˆå®Œå…¨ã«ãƒ‡ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸãŸã‚ï¼‰
                                    token_buffer = []
                                    
                                except Exception as decode_error:
                                    logger.error(f"ãƒ‡ã‚³ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {decode_error}")
                                    # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯å€‹åˆ¥ãƒ‡ã‚³ãƒ¼ãƒ‰ã‚’è©¦è¡Œ
                                    for token in token_buffer:
                                        try:
                                            single_text = tokenizer.decode(token, skip_special_tokens=True)
                                            if single_text and single_text.strip():
                                                # åŸºæœ¬çš„ãªæ–‡å­—åŒ–ã‘ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
                                                invalid_chars = ['', '\ufffd', '\u0000', '\u0001', '\u0002', '\u0003']
                                                has_invalid = any(char in single_text for char in invalid_chars)
                                                
                                                if not has_invalid:
                                                    yield f"data: {json.dumps({'text': single_text, 'done': False})}\n\n"
                                                else:
                                                    logger.warning(f"å€‹åˆ¥ãƒ‡ã‚³ãƒ¼ãƒ‰æ–‡å­—åŒ–ã‘æ¤œå‡º: {single_text}")
                                        except Exception as e:
                                            logger.error(f"å€‹åˆ¥ãƒ‡ã‚³ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
                                    token_buffer = []
                            
                            # å…¥åŠ›IDã‚’æ›´æ–°ï¼ˆãƒ‡ãƒã‚¤ã‚¹çµ±ä¸€æ¸ˆã¿ï¼‰
                            inputs['input_ids'] = torch.cat([inputs['input_ids'], new_token.unsqueeze(0)], dim=1)
                            if 'attention_mask' in inputs:
                                # attention_maskã‚‚åŒã˜ãƒ‡ãƒã‚¤ã‚¹ã«ä½œæˆ
                                ones = torch.ones(1, 1, dtype=torch.long, device=device)
                                inputs['attention_mask'] = torch.cat([inputs['attention_mask'], ones], dim=1)
                            
                            # EOSãƒˆãƒ¼ã‚¯ãƒ³ãŒç”Ÿæˆã•ã‚ŒãŸã‚‰åœæ­¢
                            if new_token.item() == tokenizer.eos_token_id:
                                break
                    
                    yield f"data: {json.dumps({'text': '', 'done': True})}\n\n"
                    
                except Exception as e:
                    logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
                    yield f"data: {json.dumps({'error': str(e)})}\n\n"
            
            else:
                yield f"data: {json.dumps({'error': 'ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ã§ã™'})}\n\n"
                
        except Exception as e:
            logger.error(f"ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            yield f"data: {json.dumps({'error': str(e)})}\n\n"
    
    return StreamingResponse(generate_stream(), media_type="text/plain")

@app.post("/api/generate-with-model-selection")
async def generate_with_model_selection(request: dict):
    """ãƒ¢ãƒ‡ãƒ«é¸æŠæ©Ÿèƒ½ä»˜ããƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
    try:
        model_name = request.get("model_name")
        model_type = request.get("model_type")  # "finetuned" or "ollama"
        prompt = request.get("prompt")
        max_length = request.get("max_length", 2048)
        temperature = request.get("temperature", 0.7)
        top_p = request.get("top_p", 0.9)
        
        if not model_name or not model_type or not prompt:
            return {"success": False, "error": "model_name, model_type, promptãŒå¿…è¦ã§ã™"}
        
        logger.info(f"ãƒ¢ãƒ‡ãƒ«é¸æŠç”Ÿæˆ: {model_type}/{model_name}")
        
        if model_type == "ollama":
            # Ollamaãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
            if not OLLAMA_AVAILABLE:
                return {"success": False, "error": "OllamaãŒåˆ©ç”¨ã§ãã¾ã›ã‚“"}
            
            ollama = OllamaIntegration()
            result = ollama.generate_text(
                model_name=model_name,
                prompt=prompt,
                temperature=temperature,
                top_p=top_p,
                max_tokens=max_length
            )
            
            if result["success"]:
                return {
                    "success": True,
                    "generated_text": result["generated_text"],
                    "model_name": model_name,
                    "model_type": "ollama",
                    "method": "ollama_api"
                }
            else:
                return {"success": False, "error": result.get("error", "Ollamaç”Ÿæˆã‚¨ãƒ©ãƒ¼")}
        
        elif model_type == "finetuned":
            # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
            model_path = f"outputs/{model_name}"
            
            # æ—¢å­˜ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ç”Ÿæˆæ©Ÿèƒ½ã‚’ä½¿ç”¨
            generation_request = {
                "model_path": model_path,
                "prompt": prompt,
                "max_length": max_length,
                "temperature": temperature,
                "top_p": top_p
            }
            
            # ãƒ¡ãƒ¢ãƒªä¸è¶³ã®å ´åˆã¯Ollamaã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            if torch.cuda.is_available():
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
                if gpu_memory < 30 and OLLAMA_AVAILABLE:
                    # Ollamaã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    ollama_model_name = "llama3.2:3b"  # åˆ©ç”¨å¯èƒ½ãªOllamaãƒ¢ãƒ‡ãƒ«
                    logger.info(f"ãƒ¡ãƒ¢ãƒªä¸è¶³ã®ãŸã‚ã€Ollamaãƒ¢ãƒ‡ãƒ« {ollama_model_name} ã‚’ä½¿ç”¨ã—ã¾ã™")
                    
                    ollama = OllamaIntegration()
                    result = ollama.generate_text(
                        model_name=ollama_model_name,
                        prompt=prompt,
                        temperature=temperature,
                        top_p=top_p,
                        max_tokens=max_length
                    )
                    
                    if result["success"]:
                        return {
                            "success": True,
                            "generated_text": result["generated_text"],
                            "model_name": f"{model_name} (Ollama fallback: {ollama_model_name})",
                            "model_type": "finetuned_ollama_fallback",
                            "method": "ollama_fallback"
                        }
            
            # é€šå¸¸ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ç”Ÿæˆã‚’è©¦è¡Œ
            try:
                # ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã¨ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
                if model_path not in model_cache:
                    logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­: {model_path}")
                    
                    # GPU ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–è¨­å®š
                    max_memory = {}
                    if torch.cuda.is_available():
                        for i in range(torch.cuda.device_count()):
                            max_memory[i] = "18GB"  # å„GPUã«18GBå‰²ã‚Šå½“ã¦
                        max_memory["cpu"] = "30GB"  # CPUã«30GBå‰²ã‚Šå½“ã¦
                    
                    # ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿ï¼ˆé‡å­åŒ–ã¨ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ï¼‰
                    model = AutoModelForCausalLM.from_pretrained(
                        model_path,
                        device_map="auto",
                        max_memory=max_memory,
                        load_in_4bit=True,
                        bnb_4bit_compute_dtype=torch.float16,
                        bnb_4bit_use_double_quant=True,
                        torch_dtype=torch.float16,
                        trust_remote_code=True,
                        offload_folder="./offload",
                        offload_state_dict=True
                    )
                    
                    tokenizer = load_tokenizer(model_path)
                    
                    model_cache[model_path] = {
                        "model": model,
                        "tokenizer": tokenizer,
                        "base_model_name": model_path,
                        "training_method": "full"
                    }
                    
                    logger.info(f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {model_path}")
                
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—
                cached_model = model_cache[model_path]
                model = cached_model["model"]
                tokenizer = cached_model["tokenizer"]
                
                # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
                inputs = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True)
                
                with torch.no_grad():
                    outputs = model.generate(
                        inputs.input_ids,
                        attention_mask=inputs.attention_mask,
                        max_length=max_length,
                        temperature=temperature,
                        top_p=top_p,
                        do_sample=True,
                        pad_token_id=tokenizer.pad_token_id,
                        eos_token_id=tokenizer.eos_token_id
                    )
                
                generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
                
                # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã‚’é™¤å»
                if generated_text.startswith(prompt):
                    generated_text = generated_text[len(prompt):].strip()
                
                logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã§ã®ç”Ÿæˆå®Œäº†: {len(generated_text)}æ–‡å­—")
                
                return {
                    "success": True,
                    "generated_text": generated_text,
                    "model_name": model_name,
                    "model_type": "finetuned",
                    "method": "direct_inference"
                }
                
            except Exception as model_error:
                logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(model_error)}")
                return {"success": False, "error": f"ãƒ¢ãƒ‡ãƒ«ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(model_error)}"}
        
        else:
            return {"success": False, "error": f"æœªçŸ¥ã®ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—: {model_type}"}
        
    except Exception as e:
        logger.error(f"ãƒ¢ãƒ‡ãƒ«é¸æŠç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
        return {"success": False, "error": str(e)}

# =============================================================================
# RAG API ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ (çµ±åˆç‰ˆ)
# =============================================================================

@app.get("/rag/health")
async def rag_health_check():
    """RAGã‚·ã‚¹ãƒ†ãƒ ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
    return {
        "status": "healthy" if rag_app.is_initialized else "initializing",
        "timestamp": datetime.now(JST).isoformat(),
        "service": "Road Design RAG System",
        "available": RAG_AVAILABLE
    }

@app.get("/rag/system-info", response_model=SystemInfoResponse)
async def rag_get_system_info():
    """RAGã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã‚’å–å¾—"""
    try:
        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç›´æ¥èª­ã¿è¾¼ã¿
        config_path = Path("src/rag/config/rag_config.yaml")
        config_data = {}
        
        if config_path.exists():
            import yaml
            with open(config_path, 'r', encoding='utf-8') as f:
                config_data = yaml.safe_load(f)
        
        # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã‚’æ§‹ç¯‰
        system_info = {
            "config": {
                "llm": {
                    "model_name": config_data.get('llm', {}).get('model_name', 'æœªè¨­å®š'),
                    "base_model": config_data.get('llm', {}).get('base_model', 'æœªè¨­å®š'),
                    "temperature": config_data.get('llm', {}).get('temperature', 0.3),
                    "use_finetuned": config_data.get('llm', {}).get('use_finetuned', False),
                    "model_path": config_data.get('llm', {}).get('model_path', 'æœªè¨­å®š')
                },
                "embedding": {
                    "model_name": config_data.get('embedding', {}).get('model_name', 'multilingual-e5-large')
                },
                "vector_store": {
                    "type": config_data.get('vector_store', {}).get('type', 'Qdrant')
                }
            },
            "status": "initialized" if rag_app.is_initialized else "not_initialized"
        }
        
        return SystemInfoResponse(
            status="success",
            system_info=system_info,
            timestamp=datetime.now(JST).isoformat()
        )
        
    except Exception as e:
        logger.error(f"Failed to get system info: {e}")
        return SystemInfoResponse(
            status="error",
            system_info={
                "config": {
                    "llm": {"model_name": "è¨­å®šèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼"},
                    "embedding": {"model_name": "multilingual-e5-large"},
                    "vector_store": {"type": "Qdrant"}
                },
                "error": str(e)
            },
            timestamp=datetime.now(JST).isoformat()
        )

@app.post("/rag/update-settings")
async def rag_update_settings(settings: Dict[str, Any]):
    """RAGã‚·ã‚¹ãƒ†ãƒ ã®è¨­å®šã‚’æ›´æ–°"""
    try:
        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°
        config_path = Path("src/rag/config/rag_config.yaml")
        
        if config_path.exists():
            import yaml
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            
            # LLMãƒ¢ãƒ‡ãƒ«ã®æ›´æ–°
            if 'llm_model' in settings and settings['llm_model']:
                if settings['llm_model'].startswith('finetuned:'):
                    model_path = settings['llm_model'].replace('finetuned:', '')
                    config['llm']['model_name'] = model_path
                    config['llm']['model_path'] = model_path
                    config['llm']['use_finetuned'] = True
                else:
                    config['llm']['model_name'] = settings['llm_model']
                    config['llm']['use_finetuned'] = False
            
            # åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®æ›´æ–°
            if 'embedding_model' in settings:
                config['embedding']['model_name'] = settings['embedding_model']
            
            # Temperatureã®æ›´æ–°
            if 'temperature' in settings:
                config['llm']['temperature'] = settings['temperature']
            
            # è¨­å®šã‚’ä¿å­˜
            with open(config_path, 'w', encoding='utf-8') as f:
                yaml.dump(config, f, allow_unicode=True, default_flow_style=False)
            
            return {"status": "success", "message": "è¨­å®šã‚’æ›´æ–°ã—ã¾ã—ãŸ"}
        else:
            return {"status": "error", "message": "è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"}
            
    except Exception as e:
        logger.error(f"Failed to update RAG settings: {e}")
        return {"status": "error", "message": str(e)}

@app.post("/rag/query", response_model=QueryResponse)
async def rag_query_documents(request: QueryRequest):
    """RAGæ–‡æ›¸æ¤œç´¢ãƒ»è³ªå•å¿œç­”"""
    rag_app.check_initialized()
    
    try:
        # ã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œ
        result = await asyncio.get_event_loop().run_in_executor(
            None,
            rag_app.query_engine.query,
            request.query,
            request.top_k,
            request.search_type,
            request.filters,
            request.include_sources
        )
        
        return QueryResponse(**result.to_dict())
        
    except Exception as e:
        logger.error(f"RAG Query failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/rag/batch-query")
async def rag_batch_query_documents(request: BatchQueryRequest):
    """RAGãƒãƒƒãƒã‚¯ã‚¨ãƒª"""
    rag_app.check_initialized()
    
    if len(request.queries) > 10:
        raise HTTPException(
            status_code=400,
            detail="Maximum 10 queries allowed in batch"
        )
        
    try:
        # ãƒãƒƒãƒã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œ
        results = await asyncio.get_event_loop().run_in_executor(
            None,
            rag_app.query_engine.batch_query,
            request.queries,
            request.top_k,
            request.search_type
        )
        
        return {
            "status": "success",
            "results": [result.to_dict() for result in results],
            "total_queries": len(request.queries),
            "timestamp": datetime.now(JST).isoformat()
        }
        
    except Exception as e:
        logger.error(f"RAG Batch query failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/rag/search")
async def rag_search_documents(
    q: str = Query(..., description="æ¤œç´¢ã‚¯ã‚¨ãƒª"),
    top_k: int = Query(5, description="å–å¾—ã™ã‚‹çµæœæ•°", ge=1, le=20),
    search_type: str = Query("hybrid", description="æ¤œç´¢ã‚¿ã‚¤ãƒ—", pattern="^(hybrid|vector|keyword)$")
):
    """RAGç°¡æ˜“æ¤œç´¢API"""
    
    request = QueryRequest(
        query=q,
        top_k=top_k,
        search_type=search_type,
        include_sources=True
    )
    
    return await rag_query_documents(request)

@app.get("/rag/documents")
async def rag_list_documents(
    limit: int = Query(50, description="å–å¾—ä»¶æ•°", ge=1, le=100),
    offset: int = Query(0, description="ã‚ªãƒ•ã‚»ãƒƒãƒˆ", ge=0),
    document_type: Optional[str] = Query(None, description="æ–‡æ›¸ã‚¿ã‚¤ãƒ—ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼")
):
    """RAGæ–‡æ›¸ä¸€è¦§ã‚’å–å¾—"""
    rag_app.check_initialized()
    
    try:
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ–‡æ›¸ã‚’æ¤œç´¢
        try:
            from src.rag.indexing.metadata_manager import DocumentType
        except ImportError:
            # DocumentTypeãŒãªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            DocumentType = None
        
        filters = {}
        if document_type and DocumentType:
            try:
                filters['document_type'] = DocumentType(document_type)
            except ValueError:
                raise HTTPException(status_code=400, detail=f"Invalid document_type: {document_type}")
                
        documents = rag_app.metadata_manager.search_documents(**filters)
        
        # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³
        total = len(documents)
        paginated_docs = documents[offset:offset + limit]
        
        return {
            "status": "success",
            "documents": [doc.to_dict() for doc in paginated_docs],
            "pagination": {
                "total": total,
                "limit": limit,
                "offset": offset,
                "has_more": offset + limit < total
            },
            "timestamp": datetime.now(JST).isoformat()
        }
        
    except Exception as e:
        logger.error(f"RAG Document listing failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/rag/statistics")
async def rag_get_statistics():
    """RAGã‚·ã‚¹ãƒ†ãƒ çµ±è¨ˆã‚’å–å¾—"""
    rag_app.check_initialized()
    
    try:
        stats = rag_app.metadata_manager.get_statistics()
        
        return {
            "status": "success",
            "statistics": stats,
            "timestamp": datetime.now(JST).isoformat()
        }
        
    except Exception as e:
        logger.error(f"RAG Statistics retrieval failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/rag/documents/{document_id}")
async def rag_delete_document(document_id: str):
    """RAGæ–‡æ›¸ã‚’å‰Šé™¤"""
    rag_app.check_initialized()
    
    try:
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ–‡æ›¸æƒ…å ±ã‚’å–å¾—
        doc_metadata = rag_app.metadata_manager.get_document(document_id)
        if not doc_metadata:
            raise HTTPException(status_code=404, detail=f"Document not found: {document_id}")
        
        # ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‹ã‚‰ãƒãƒ£ãƒ³ã‚¯ã‚’å‰Šé™¤
        # ãƒãƒ£ãƒ³ã‚¯IDã¯document_id_<chunk_index>å½¢å¼ã§ä¿å­˜ã•ã‚Œã¦ã„ã‚‹
        chunk_ids = []
        for i in range(1000):  # æœ€å¤§1000ãƒãƒ£ãƒ³ã‚¯ã¾ã§å¯¾å¿œ
            chunk_id = f"{document_id}_{i}"
            chunk_ids.append(chunk_id)
        
        # ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‹ã‚‰å‰Šé™¤ï¼ˆå­˜åœ¨ã—ãªã„IDã¯ç„¡è¦–ã•ã‚Œã‚‹ï¼‰
        try:
            rag_app.vector_store.delete(chunk_ids)
            logger.info(f"Deleted chunks from vector store for document: {document_id}")
        except Exception as e:
            logger.warning(f"Failed to delete from vector store: {e}")
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å‰Šé™¤
        rag_app.metadata_manager.delete_document(document_id)
        
        # å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
        processed_file = Path(f"./outputs/rag_index/processed_documents/{document_id}.json")
        if processed_file.exists():
            processed_file.unlink()
            logger.info(f"Deleted processed file: {processed_file}")
        
        return {
            "status": "success",
            "message": f"Document {document_id} deleted successfully",
            "document_title": doc_metadata.title,
            "timestamp": datetime.now(JST).isoformat()
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"RAG Document deletion failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

async def process_uploaded_rag_document(
    file_path: str,
    title: str,
    category: str,
    document_type: str
):
    """ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸRAGæ–‡æ›¸ã‚’å‡¦ç†ï¼ˆãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯ï¼‰"""
    
    try:
        logger.info(f"Processing uploaded RAG document: {file_path}")
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œ
        import subprocess
        
        result = subprocess.run([
            sys.executable,
            "scripts/rag/index_documents.py",
            file_path,
            "--output-dir", "./outputs/rag_index"
        ], capture_output=True, text=True, cwd="/workspace")
        
        if result.returncode == 0:
            logger.info(f"RAG Document processed successfully: {file_path}")
        else:
            logger.error(f"RAG Document processing failed: {result.stderr}")
            logger.error(f"RAG Document processing stdout: {result.stdout}")
            
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ï¼ˆæˆåŠŸæ™‚ã®ã¿ï¼‰
        if result.returncode == 0:
            logger.info(f"Removing processed file: {file_path}")
            os.remove(file_path)
        else:
            logger.warning(f"Keeping failed file for debugging: {file_path}")
        
    except Exception as e:
        logger.error(f"Background RAG document processing failed: {e}")

@app.post("/rag/save-search")
async def save_search_result(request: SaveSearchRequest):
    """æ¤œç´¢çµæœã‚’ä¿å­˜"""
    try:
        rag_app.check_initialized()
        
        saved_result = rag_app.save_search_result(
            query_response=request.query_response,
            name=request.name,
            tags=request.tags
        )
        
        return {
            "status": "success",
            "message": "Search result saved successfully",
            "result_id": saved_result.id,
            "saved_at": saved_result.saved_at
        }
        
    except Exception as e:
        logger.error(f"Failed to save search result: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/rag/search-history", response_model=SearchHistoryResponse)
async def get_search_history(
    page: int = Query(1, ge=1),
    limit: int = Query(10, ge=1, le=100),
    tag: Optional[str] = None
):
    """æ¤œç´¢å±¥æ­´ã‚’å–å¾—"""
    try:
        rag_app.check_initialized()
        
        history = rag_app.get_search_history(page=page, limit=limit, tag=tag)
        return history
        
    except Exception as e:
        logger.error(f"Failed to get search history: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/rag/search-history/{result_id}")
async def delete_search_history_item(result_id: str):
    """æ¤œç´¢å±¥æ­´ã®å€‹åˆ¥ã‚¢ã‚¤ãƒ†ãƒ ã‚’å‰Šé™¤"""
    try:
        rag_app.check_initialized()
        
        # å‰Šé™¤ã‚’å®Ÿè¡Œ
        success = rag_app.delete_search_history_item(result_id)
        
        if success:
            return {
                "status": "success",
                "message": f"Search history item '{result_id}' deleted successfully"
            }
        else:
            raise HTTPException(
                status_code=404,
                detail=f"Search history item '{result_id}' not found"
            )
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to delete search history item: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/rag/search-history/clear")
async def clear_all_search_history():
    """å…¨ã¦ã®æ¤œç´¢å±¥æ­´ã‚’å‰Šé™¤"""
    try:
        rag_app.check_initialized()
        
        # å…¨å±¥æ­´ã‚’ã‚¯ãƒªã‚¢
        rag_app.search_history = []
        
        return {
            "status": "success",
            "message": "All search history cleared successfully"
        }
        
    except Exception as e:
        logger.error(f"Failed to clear all search history: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/rag/search-result/{result_id}")
async def get_saved_search_result(result_id: str):
    """ä¿å­˜ã•ã‚ŒãŸæ¤œç´¢çµæœã‚’å–å¾—"""
    try:
        rag_app.check_initialized()
        
        result = rag_app.get_saved_result(result_id)
        if not result:
            raise HTTPException(status_code=404, detail="Search result not found")
            
        return result
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get saved search result: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/rag/export-searches")
async def export_search_results(
    result_ids: str = Query(..., description="ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®çµæœID"),
    format: str = Query("json", pattern="^(json|csv)$")
):
    """æ¤œç´¢çµæœã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
    try:
        rag_app.check_initialized()
        
        ids = result_ids.split(",")
        export_data = rag_app.export_search_results(ids, format=format)
        
        filename = f"search_results_{datetime.now(JST).strftime('%Y%m%d_%H%M%S')}.{format}"
        media_type = "text/csv" if format == "csv" else "application/json"
        
        return StreamingResponse(
            io.BytesIO(export_data),
            media_type=media_type,
            headers={
                "Content-Disposition": f"attachment; filename={filename}"
            }
        )
        
    except Exception as e:
        logger.error(f"Failed to export search results: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/rag/upload-document")
async def rag_upload_document(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...),
    title: Optional[str] = None,
    category: Optional[str] = None,
    document_type: Optional[str] = None
):
    """RAGæ–‡æ›¸ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–"""
    
    # ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ãƒã‚§ãƒƒã‚¯
    if not file.filename.lower().endswith('.pdf'):
        raise HTTPException(
            status_code=400,
            detail="Only PDF files are supported"
        )
        
    try:
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        upload_dir = PathlibPath("./temp_uploads")
        upload_dir.mkdir(exist_ok=True)
        
        timestamp = datetime.now(JST).strftime("%Y%m%d_%H%M%S")
        temp_filename = f"{timestamp}_{file.filename}"
        temp_path = upload_dir / temp_filename
        
        with open(temp_path, "wb") as f:
            content = await file.read()
            f.write(content)
            
        # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–ã‚’å®Ÿè¡Œ
        background_tasks.add_task(
            process_uploaded_rag_document,
            str(temp_path),
            title or file.filename,
            category or "ãã®ä»–",
            document_type or "other"
        )
        
        return DocumentUploadResponse(
            status="success",
            message="Document uploaded and queued for processing",
            document_id=temp_filename,
            processing_status="queued",
            metadata={"page_count": 0, "status": "processing"}
        )
        
    except Exception as e:
        logger.error(f"RAG Document upload failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/rag/stream-query")
async def rag_stream_query(request: QueryRequest):
    """RAGã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚¯ã‚¨ãƒªï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å¿œç­”ï¼‰"""
    rag_app.check_initialized()
    
    async def generate_response():
        """ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’æ®µéšçš„ã«ç”Ÿæˆ"""
        
        # æ¤œç´¢ãƒ•ã‚§ãƒ¼ã‚º
        yield f"data: {json.dumps({'phase': 'search', 'message': 'æ–‡æ›¸ã‚’æ¤œç´¢ä¸­...'})}\n\n"
        await asyncio.sleep(0.1)
        
        try:
            # å®Ÿéš›ã®ã‚¯ã‚¨ãƒªå®Ÿè¡Œ
            result = await asyncio.get_event_loop().run_in_executor(
                None,
                rag_app.query_engine.query,
                request.query,
                request.top_k,
                request.search_type,
                request.filters,
                request.include_sources
            )
            
            # çµæœãƒ•ã‚§ãƒ¼ã‚º
            yield f"data: {json.dumps({'phase': 'result', 'data': result.to_dict()})}\n\n"
            
        except Exception as e:
            yield f"data: {json.dumps({'phase': 'error', 'error': str(e)})}\n\n"
            
        yield "data: [DONE]\n\n"
        
    return StreamingResponse(
        generate_response(),
        media_type="text/plain",
        headers={"Cache-Control": "no-cache"}
    )

# ============================================
# ç¶™ç¶šå­¦ç¿’API
# ============================================

# ç¶™ç¶šå­¦ç¿’ã‚¿ã‚¹ã‚¯ç®¡ç†ã¨æ°¸ç¶šåŒ–
CONTINUAL_TASKS_FILE = Path(os.getcwd()) / "data" / "continual_learning" / "tasks_state.json"
continual_tasks = {}

def load_continual_tasks():
    """ä¿å­˜ã•ã‚ŒãŸç¶™ç¶šå­¦ç¿’ã‚¿ã‚¹ã‚¯ã‚’èª­ã¿è¾¼ã‚€"""
    global continual_tasks
    try:
        if CONTINUAL_TASKS_FILE.exists():
            with open(CONTINUAL_TASKS_FILE, 'r', encoding='utf-8') as f:
                continual_tasks = json.load(f)
                logger.info(f"ç¶™ç¶šå­¦ç¿’ã‚¿ã‚¹ã‚¯ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {len(continual_tasks)}ä»¶")
        else:
            continual_tasks = {}
            logger.info("ç¶™ç¶šå­¦ç¿’ã‚¿ã‚¹ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚æ–°è¦ä½œæˆã—ã¾ã™ã€‚")
    except Exception as e:
        logger.error(f"ç¶™ç¶šå­¦ç¿’ã‚¿ã‚¹ã‚¯èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}")
        continual_tasks = {}

def save_continual_tasks():
    """ç¶™ç¶šå­¦ç¿’ã‚¿ã‚¹ã‚¯ã‚’ä¿å­˜ã™ã‚‹"""
    try:
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
        CONTINUAL_TASKS_FILE.parent.mkdir(parents=True, exist_ok=True)
        
        with open(CONTINUAL_TASKS_FILE, 'w', encoding='utf-8') as f:
            json.dump(continual_tasks, f, ensure_ascii=False, indent=2, default=str)
        logger.info(f"ç¶™ç¶šå­¦ç¿’ã‚¿ã‚¹ã‚¯ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {len(continual_tasks)}ä»¶")
    except Exception as e:
        logger.error(f"ç¶™ç¶šå­¦ç¿’ã‚¿ã‚¹ã‚¯ä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(e)}")

# ç¶™ç¶šå­¦ç¿’ç”¨ã®ãƒ¢ãƒ‡ãƒ«å–å¾—API
@app.get("/api/continual-learning/models")
async def get_continual_learning_models():
    """ç¶™ç¶šå­¦ç¿’ç”¨ã®åˆ©ç”¨å¯èƒ½ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’å–å¾—"""
    try:
        # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—
        saved_models = get_saved_models()
        
        # ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚‚å«ã‚ã‚‹
        base_models = [
            {
                "name": "cyberagent/calm3-22b-chat",
                "path": "cyberagent/calm3-22b-chat",
                "type": "base",
                "description": "æ—¥æœ¬èªç‰¹åŒ–å‹22Bãƒ¢ãƒ‡ãƒ«ï¼ˆæ¨å¥¨ï¼‰"
            },
            {
                "name": "cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese",
                "path": "cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese",
                "type": "base",
                "description": "æ—¥æœ¬èªç‰¹åŒ–å‹32Bãƒ¢ãƒ‡ãƒ«"
            },
            {
                "name": "Qwen/Qwen2.5-14B-Instruct",
                "path": "Qwen/Qwen2.5-14B-Instruct",
                "type": "base",
                "description": "å¤šè¨€èªå¯¾å¿œ14Bãƒ¢ãƒ‡ãƒ«"
            },
            {
                "name": "Qwen/Qwen2.5-32B-Instruct",
                "path": "Qwen/Qwen2.5-32B-Instruct",
                "type": "base",
                "description": "å¤šè¨€èªå¯¾å¿œ32Bãƒ¢ãƒ‡ãƒ«"
            }
        ]
        
        # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ç¶™ç¶šå­¦ç¿’ç”¨å½¢å¼ã«å¤‰æ›
        continual_models = []
        
        # ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’è¿½åŠ 
        for model in base_models:
            continual_models.append({
                "name": model["name"],
                "path": model["path"],
                "type": "base",
                "description": model["description"]
            })
        
        # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’è¿½åŠ 
        for model in saved_models:
            continual_models.append({
                "name": f"{model['name']} (ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿)",
                "path": model["path"],
                "type": "finetuned",
                "description": f"å­¦ç¿’æ—¥æ™‚: {model.get('created_at', 'ä¸æ˜')}"
            })
        
        logger.info(f"ç¶™ç¶šå­¦ç¿’ç”¨ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’å–å¾—: {len(continual_models)}å€‹")
        return continual_models
        
    except Exception as e:
        logger.error(f"ç¶™ç¶šå­¦ç¿’ç”¨ãƒ¢ãƒ‡ãƒ«å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

# ç¶™ç¶šå­¦ç¿’ç”¨ã®ãƒ¢ãƒ‡ãƒ«å–å¾—ã¯ continual_learning_ui.py ã®ãƒ«ãƒ¼ã‚¿ãƒ¼ã‚’ä½¿ç”¨

@app.post("/api/continual-learning/start")
async def start_continual_learning(
    background_tasks: BackgroundTasks,
    config: str = Form(...),
    dataset: UploadFile = File(...)
):
    """ç¶™ç¶šå­¦ç¿’ã‚’é–‹å§‹"""
    try:
        # è¨­å®šã‚’ãƒ‘ãƒ¼ã‚¹
        config_data = json.loads(config)
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä¿å­˜
        project_root = Path(os.getcwd())
        dataset_dir = project_root / "data" / "continual_learning"
        dataset_dir.mkdir(parents=True, exist_ok=True)
        
        dataset_path = dataset_dir / f"{uuid.uuid4()}_{dataset.filename}"
        with open(dataset_path, "wb") as f:
            content = await dataset.read()
            f.write(content)
        
        # ã‚¿ã‚¹ã‚¯IDã‚’ç”Ÿæˆ
        task_id = str(uuid.uuid4())
        
        # ã‚¿ã‚¹ã‚¯æƒ…å ±ã‚’ä¿å­˜
        continual_tasks[task_id] = {
            "task_id": task_id,
            "task_name": config_data["task_name"],
            "status": "pending",
            "progress": 0,
            "started_at": datetime.now(JST).isoformat(),
            "config": config_data,
            "dataset_path": str(dataset_path)
        }
        save_continual_tasks()  # æ–°ã—ã„ã‚¿ã‚¹ã‚¯ã‚’ä¿å­˜
        
        # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ç¶™ç¶šå­¦ç¿’ã‚’å®Ÿè¡Œ
        background_tasks.add_task(
            run_continual_learning_background,
            task_id,
            config_data,
            str(dataset_path)
        )
        
        return {
            "task_id": task_id,
            "message": f"ç¶™ç¶šå­¦ç¿’ã‚¿ã‚¹ã‚¯ '{config_data['task_name']}' ã‚’é–‹å§‹ã—ã¾ã—ãŸ"
        }
        
    except Exception as e:
        logger.error(f"ç¶™ç¶šå­¦ç¿’é–‹å§‹ã‚¨ãƒ©ãƒ¼: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

async def run_continual_learning_background(task_id: str, config: dict, dataset_path: str):
    """ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ç¶™ç¶šå­¦ç¿’ã‚’å®Ÿè¡Œ"""
    try:
        # ã‚¿ã‚¹ã‚¯ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’æ›´æ–°
        continual_tasks[task_id]["status"] = "running"
        continual_tasks[task_id]["message"] = "ç¶™ç¶šå­¦ç¿’ã‚’æº–å‚™ä¸­..."
        save_continual_tasks()  # ã‚¿ã‚¹ã‚¯ã®çŠ¶æ…‹ã‚’ä¿å­˜
        
        logger.info(f"ç¶™ç¶šå­¦ç¿’ã‚¿ã‚¹ã‚¯é–‹å§‹: {task_id}")
        logger.info(f"è¨­å®š: {config}")
        
        # å®Ÿéš›ã®ç¶™ç¶šå­¦ç¿’å‡¦ç†ã‚’ã“ã“ã«å®Ÿè£…
        # ç¾åœ¨ã¯ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        total_epochs = config.get("epochs", 3)
        base_model_path = config.get("base_model")
        
        # ãƒ¢ãƒ‡ãƒ«ã®å­˜åœ¨ç¢ºèª
        if not base_model_path:
            raise ValueError("ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®å ´åˆã¯ãƒ‘ã‚¹ã‚’ç¢ºèª
        if "/" not in base_model_path and os.path.exists(base_model_path):
            logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨: {base_model_path}")
        else:
            logger.info(f"ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨: {base_model_path}")
        
        for epoch in range(total_epochs):
            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹æ›´æ–°
            progress = int((epoch + 1) / total_epochs * 100)
            continual_tasks[task_id]["progress"] = progress
            continual_tasks[task_id]["current_epoch"] = epoch + 1
            continual_tasks[task_id]["total_epochs"] = total_epochs
            continual_tasks[task_id]["message"] = f"ã‚¨ãƒãƒƒã‚¯ {epoch + 1}/{total_epochs} ã‚’å®Ÿè¡Œä¸­..."
            save_continual_tasks()  # é€²æ—ã‚’ä¿å­˜
            
            logger.info(f"ã‚¿ã‚¹ã‚¯ {task_id}: ã‚¨ãƒãƒƒã‚¯ {epoch + 1}/{total_epochs}")
            
            # å®Ÿéš›ã®å­¦ç¿’å‡¦ç†ã‚’ã“ã“ã«è¿½åŠ 
            await asyncio.sleep(5)  # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã®å¾…æ©Ÿ
            
            # TODO: å®Ÿéš›ã®ç¶™ç¶šå­¦ç¿’å®Ÿè£…
            # 1. ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿
            # 2. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™
            # 3. EWCè¨­å®š
            # 4. ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ«ãƒ¼ãƒ—
            # 5. ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
        
        # å®Œäº†
        continual_tasks[task_id]["status"] = "completed"
        continual_tasks[task_id]["message"] = "ç¶™ç¶šå­¦ç¿’ãŒå®Œäº†ã—ã¾ã—ãŸ"
        continual_tasks[task_id]["completed_at"] = datetime.now(JST).isoformat()
        save_continual_tasks()  # å®Œäº†çŠ¶æ…‹ã‚’ä¿å­˜
        
        # å‡ºåŠ›ãƒ‘ã‚¹ã‚’è¨­å®šï¼ˆãƒ¢ãƒ‡ãƒ«ç®¡ç†ã¨åŒã˜å½¢å¼ï¼‰
        output_dir = f"outputs/continual_{config.get('task_name')}_{datetime.now(JST).strftime('%Y%m%d_%H%M%S')}"
        continual_tasks[task_id]["output_path"] = output_dir
        
        # ãƒ¢ãƒ‡ãƒ«ç®¡ç†ã«ç™»éŒ²ã™ã‚‹ãŸã‚ã®æƒ…å ±ã‚’ä¿å­˜
        model_info = {
            "name": f"continual_{config.get('task_name')}",
            "path": output_dir,
            "base_model": base_model_path,
            "training_method": "continual_ewc",
            "created_at": datetime.now(JST).isoformat(),
            "training_params": {
                "epochs": config.get("epochs", 3),
                "learning_rate": config.get("learning_rate", 2e-5),
                "ewc_lambda": config.get("ewc_lambda", 5000),
                "use_previous_tasks": config.get("use_previous_tasks", True)
            },
            "task_history": config.get("task_name")
        }
        
        # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ï¼ˆãƒ¢ãƒ‡ãƒ«ç®¡ç†ãŒèª­ã¿å–ã‚Œã‚‹ã‚ˆã†ã«ï¼‰
        os.makedirs(output_dir, exist_ok=True)
        with open(f"{output_dir}/model_info.json", "w", encoding="utf-8") as f:
            json.dump(model_info, f, ensure_ascii=False, indent=2)
        
        continual_tasks[task_id]["model_info"] = model_info
        save_continual_tasks()  # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’ä¿å­˜
        
        logger.info(f"ç¶™ç¶šå­¦ç¿’ã‚¿ã‚¹ã‚¯å®Œäº†: {task_id}")
        
    except Exception as e:
        logger.error(f"ç¶™ç¶šå­¦ç¿’ã‚¨ãƒ©ãƒ¼ (ã‚¿ã‚¹ã‚¯ {task_id}): {str(e)}")
        logger.exception("è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±:")
        continual_tasks[task_id]["status"] = "failed"
        continual_tasks[task_id]["message"] = f"ã‚¨ãƒ©ãƒ¼: {str(e)}"
        continual_tasks[task_id]["error"] = str(e)
        save_continual_tasks()  # ã‚¨ãƒ©ãƒ¼çŠ¶æ…‹ã‚’ä¿å­˜

@app.get("/api/continual-learning/tasks")
async def get_continual_tasks():
    """ç¶™ç¶šå­¦ç¿’ã‚¿ã‚¹ã‚¯ã®ä¸€è¦§ã‚’å–å¾—"""
    try:
        # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªã‚¿ã‚¹ã‚¯ã®ã¿ã‚’è¿”ã™
        active_tasks = []
        for task_id, task in continual_tasks.items():
            if task["status"] in ["pending", "running", "completed", "failed"]:
                active_tasks.append(task)
        
        # æ–°ã—ã„é †ã«ã‚½ãƒ¼ãƒˆ
        active_tasks.sort(key=lambda x: x["started_at"], reverse=True)
        
        return active_tasks[:10]  # æœ€æ–°10ä»¶ã‚’è¿”ã™
        
    except Exception as e:
        logger.error(f"ã‚¿ã‚¹ã‚¯ä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return []

@app.get("/api/continual-learning/history")
async def get_continual_history():
    """ç¶™ç¶šå­¦ç¿’ã®å±¥æ­´ã‚’å–å¾—"""
    try:
        history = []
        
        # å®Œäº†ã—ãŸã‚¿ã‚¹ã‚¯ã‚’å±¥æ­´ã¨ã—ã¦è¿”ã™
        for task_id, task in continual_tasks.items():
            if task["status"] == "completed":
                history.append({
                    "task_name": task["task_name"],
                    "base_model": task["config"].get("base_model", "unknown"),
                    "completed_at": task.get("completed_at"),
                    "epochs": task["config"].get("epochs", 0),
                    "final_loss": random.uniform(0.1, 0.5),  # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿
                    "output_path": task.get("output_path")
                })
        
        # æ–°ã—ã„é †ã«ã‚½ãƒ¼ãƒˆ
        history.sort(key=lambda x: x.get("completed_at", ""), reverse=True)
        
        return history
        
    except Exception as e:
        logger.error(f"å±¥æ­´å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return []

# WebSocketã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
@app.websocket("/ws/continual-learning")
async def continual_learning_websocket(websocket: WebSocket):
    """ç¶™ç¶šå­¦ç¿’ã®é€²æ—ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§é…ä¿¡"""
    try:
        await websocket_endpoint(websocket)
    except Exception as e:
        logger.error(f"WebSocketã‚¨ãƒ©ãƒ¼: {str(e)}")
        await websocket.close()

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8050, log_level="info")