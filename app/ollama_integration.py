"""
Ollamaçµ±åˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« - ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªæ¨è«–ã®ãŸã‚ã®è‡ªå‹•ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
"""

import os
import json
import torch
import psutil
import requests
from typing import Optional, Dict, Any, Union
from loguru import logger
from transformers import AutoModelForCausalLM, AutoTokenizer

class HybridModelManager:
    """
    HuggingFaceãƒ¢ãƒ‡ãƒ«ã¨Ollamaãƒ¢ãƒ‡ãƒ«ã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ç®¡ç†
    ãƒ¡ãƒ¢ãƒªä¸è¶³æ™‚ã«è‡ªå‹•çš„ã«Ollamaã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    """
    
    def __init__(self, config_path: str = "/workspace/config/ollama_config.json"):
        self.config = self._load_config(config_path)
        self.ollama_base_url = self.config["ollama_integration"]["base_url"]
        self.memory_threshold_gb = self.config["ollama_integration"]["memory_threshold_gb"]
        self.current_model = None
        self.current_tokenizer = None
        self.use_ollama = False
        
    def _load_config(self, config_path: str) -> Dict:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        if os.path.exists(config_path):
            with open(config_path, "r") as f:
                return json.load(f)
        return {
            "ollama_integration": {
                "enabled": True,
                "base_url": "http://localhost:11434",
                "fallback_on_oom": True,
                "models": {
                    "primary": "llama3.2:3b",
                    "fallback": "llama3.2:3b"
                },
                "memory_threshold_gb": 8,
                "auto_select": True
            }
        }
    
    def check_memory_availability(self) -> tuple[float, float]:
        """åˆ©ç”¨å¯èƒ½ãªGPUãƒ¡ãƒ¢ãƒªã‚’ç¢ºèª"""
        if torch.cuda.is_available():
            try:
                # GPU ãƒ¡ãƒ¢ãƒªæƒ…å ±
                gpu_memory = torch.cuda.get_device_properties(0).total_memory
                gpu_memory_allocated = torch.cuda.memory_allocated(0)
                gpu_memory_free = gpu_memory - gpu_memory_allocated
                gpu_memory_free_gb = gpu_memory_free / (1024**3)
                
                # ã‚·ã‚¹ãƒ†ãƒ RAMæƒ…å ±
                ram = psutil.virtual_memory()
                ram_free_gb = ram.available / (1024**3)
                
                logger.info(f"ğŸ“Š ãƒ¡ãƒ¢ãƒªçŠ¶æ³: GPU={gpu_memory_free_gb:.1f}GB, RAM={ram_free_gb:.1f}GB")
                return gpu_memory_free_gb, ram_free_gb
            except Exception as e:
                logger.warning(f"âš ï¸ GPUãƒ¡ãƒ¢ãƒªç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")
        
        # CPUã®ã¿ã®å ´åˆ
        ram = psutil.virtual_memory()
        ram_free_gb = ram.available / (1024**3)
        return 0, ram_free_gb
    
    def should_use_ollama(self, model_name: str) -> bool:
        """Ollamaã‚’ä½¿ç”¨ã™ã¹ãã‹åˆ¤å®š"""
        gpu_free, ram_free = self.check_memory_availability()
        
        # ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã®æ¨å®šï¼ˆæ¦‚ç®—ï¼‰
        model_size_map = {
            "70b": 140, "32b": 64, "22b": 44, 
            "14b": 28, "13b": 26, "8b": 16, "7b": 14, "3b": 6
        }
        
        estimated_size = 14  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        for size_key, size_gb in model_size_map.items():
            if size_key in model_name.lower():
                estimated_size = size_gb
                break
        
        # é‡å­åŒ–è€ƒæ…®ï¼ˆ4bitæƒ³å®šï¼‰
        estimated_size_quantized = estimated_size / 4
        
        # ãƒ¡ãƒ¢ãƒªä¸è¶³åˆ¤å®š
        if gpu_free < estimated_size_quantized or gpu_free < self.memory_threshold_gb:
            logger.warning(f"âš ï¸ ãƒ¡ãƒ¢ãƒªä¸è¶³: å¿…è¦={estimated_size_quantized:.1f}GB, åˆ©ç”¨å¯èƒ½={gpu_free:.1f}GB")
            return True
        
        return False
    
    def load_model(self, model_name: str, force_ollama: bool = False) -> bool:
        """ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆè‡ªå‹•é¸æŠï¼‰"""
        try:
            # Ollamaã‚’å¼·åˆ¶ä½¿ç”¨ã¾ãŸã¯ãƒ¡ãƒ¢ãƒªä¸è¶³ã®å ´åˆ
            if force_ollama or self.should_use_ollama(model_name):
                logger.info(f"ğŸ”„ Ollamaãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨: {self.config['ollama_integration']['models']['primary']}")
                self.use_ollama = True
                self.current_model = self.config['ollama_integration']['models']['primary']
                return self._test_ollama_connection()
            
            # HuggingFaceãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
            logger.info(f"ğŸ“¦ HuggingFaceãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰: {model_name}")
            from app.memory_optimized_loader import load_model_with_optimization
            
            self.current_model, self.current_tokenizer = load_model_with_optimization(
                model_name,
                device_map="auto",
                load_in_4bit=True  # ãƒ¡ãƒ¢ãƒªç¯€ç´„ã®ãŸã‚4bité‡å­åŒ–
            )
            self.use_ollama = False
            return True
            
        except torch.cuda.OutOfMemoryError:
            logger.error("âŒ GPU OOM - Ollamaã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
            if self.config["ollama_integration"]["fallback_on_oom"]:
                self.use_ollama = True
                self.current_model = self.config['ollama_integration']['models']['fallback']
                return self._test_ollama_connection()
            return False
            
        except Exception as e:
            logger.error(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def _test_ollama_connection(self) -> bool:
        """Ollamaæ¥ç¶šãƒ†ã‚¹ãƒˆ"""
        try:
            response = requests.get(f"{self.ollama_base_url}/api/tags", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def generate(self, 
                prompt: str, 
                max_tokens: int = 512,
                temperature: float = 0.7,
                **kwargs) -> Optional[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆè‡ªå‹•é¸æŠï¼‰"""
        
        if self.use_ollama:
            return self._generate_with_ollama(prompt, max_tokens, temperature)
        else:
            return self._generate_with_hf(prompt, max_tokens, temperature, **kwargs)
    
    def _generate_with_ollama(self, 
                             prompt: str, 
                             max_tokens: int,
                             temperature: float) -> Optional[str]:
        """Ollamaã§ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
        try:
            response = requests.post(
                f"{self.ollama_base_url}/api/generate",
                json={
                    "model": self.current_model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "num_predict": max_tokens,
                        "temperature": temperature,
                        "top_p": 0.9,
                    }
                },
                timeout=120
            )
            
            if response.status_code == 200:
                result = response.json()
                generated_text = result.get("response", "")
                logger.info(f"âœ… Ollamaç”Ÿæˆå®Œäº†: {len(generated_text)}æ–‡å­—")
                return generated_text
            else:
                logger.error(f"Ollamaç”Ÿæˆã‚¨ãƒ©ãƒ¼: {response.status_code}")
                return None
                
        except Exception as e:
            logger.error(f"Ollamaç”Ÿæˆä¾‹å¤–: {e}")
            return None
    
    def _generate_with_hf(self, 
                         prompt: str, 
                         max_tokens: int,
                         temperature: float,
                         **kwargs) -> Optional[str]:
        """HuggingFaceãƒ¢ãƒ‡ãƒ«ã§ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
        try:
            if not self.current_model or not self.current_tokenizer:
                logger.error("HuggingFaceãƒ¢ãƒ‡ãƒ«ãŒæœªãƒ­ãƒ¼ãƒ‰")
                return None
            
            inputs = self.current_tokenizer(prompt, return_tensors="pt")
            if torch.cuda.is_available():
                inputs = inputs.to("cuda")
            
            with torch.no_grad():
                outputs = self.current_model.generate(
                    **inputs,
                    max_new_tokens=max_tokens,
                    temperature=temperature,
                    do_sample=True,
                    top_p=0.9,
                    **kwargs
                )
            
            generated_text = self.current_tokenizer.decode(
                outputs[0][inputs['input_ids'].shape[1]:], 
                skip_special_tokens=True
            )
            
            logger.info(f"âœ… HFç”Ÿæˆå®Œäº†: {len(generated_text)}æ–‡å­—")
            return generated_text
            
        except Exception as e:
            logger.error(f"HFç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def cleanup(self):
        """ãƒªã‚½ãƒ¼ã‚¹ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        if not self.use_ollama and self.current_model:
            del self.current_model
            del self.current_tokenizer
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            logger.info("ğŸ§¹ ãƒ¢ãƒ‡ãƒ«ãƒ¡ãƒ¢ãƒªã‚’è§£æ”¾")

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
_hybrid_manager = None

def get_hybrid_manager() -> HybridModelManager:
    """ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’å–å¾—"""
    global _hybrid_manager
    if _hybrid_manager is None:
        _hybrid_manager = HybridModelManager()
    return _hybrid_manager

def generate_with_best_model(
    model_name: str,
    prompt: str,
    max_tokens: int = 512,
    temperature: float = 0.7,
    **kwargs
) -> Optional[str]:
    """
    æœ€é©ãªãƒ¢ãƒ‡ãƒ«ã§ç”Ÿæˆï¼ˆãƒ¡ãƒ¢ãƒªçŠ¶æ³ã«å¿œã˜ã¦è‡ªå‹•é¸æŠï¼‰
    
    Args:
        model_name: å¸Œæœ›ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å
        prompt: å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        max_tokens: æœ€å¤§ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°
        temperature: æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        
    Returns:
        ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
    """
    manager = get_hybrid_manager()
    
    # ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
    if not manager.load_model(model_name):
        logger.error("ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å¤±æ•—")
        return None
    
    # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
    result = manager.generate(prompt, max_tokens, temperature, **kwargs)
    
    # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
    # manager.cleanup()
    
    return result

if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    test_prompt = "æ—¥æœ¬ã®å››å­£ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚"
    
    # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªç”Ÿæˆ
    result = generate_with_best_model(
        "cyberagent/calm3-22b-chat",  # å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®š
        test_prompt,
        max_tokens=200
    )
    
    if result:
        print(f"ç”Ÿæˆçµæœ:\n{result}")
    else:
        print("ç”Ÿæˆå¤±æ•—")