#!/usr/bin/env python3
"""
ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’Ollamaã§ä½¿ç”¨ã§ãã‚‹ã‚ˆã†ã«å¤‰æ›ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import os
import json
import logging
import subprocess
from pathlib import Path
from typing import Dict, Any
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

logger = logging.getLogger(__name__)

class FinetunedModelConverter:
    """ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’Ollamaå½¢å¼ã«å¤‰æ›"""
    
    def __init__(self, model_path: str):
        self.model_path = Path(model_path)
        self.output_dir = Path("ollama_models")
        self.output_dir.mkdir(exist_ok=True)
        
    def convert_to_gguf(self, model_name: str) -> Dict[str, Any]:
        """ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’GGUFå½¢å¼ã«å¤‰æ›"""
        try:
            logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«å¤‰æ›é–‹å§‹: {self.model_path}")
            
            # 1. ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã®ç¢ºèª
            if not self.model_path.exists():
                return {"success": False, "error": "ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ãŒå­˜åœ¨ã—ã¾ã›ã‚“"}
            
            # 2. è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
            config_path = self.model_path / "config.json"
            if config_path.exists():
                with open(config_path, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                logger.info(f"ãƒ¢ãƒ‡ãƒ«è¨­å®š: {config.get('model_type', 'unknown')}")
            
            # 3. GGUFå¤‰æ›ã®å®Ÿè¡Œ
            return self._run_gguf_conversion(model_name)
            
        except Exception as e:
            logger.error(f"å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            return {"success": False, "error": str(e)}
    
    def _run_gguf_conversion(self, model_name: str) -> Dict[str, Any]:
        """GGUFå¤‰æ›ã‚’å®Ÿè¡Œ"""
        try:
            # llama.cppã‚’ä½¿ç”¨ã—ãŸGGUFå¤‰æ›
            output_file = self.output_dir / f"{model_name}.gguf"
            
            # å¤‰æ›ã‚³ãƒãƒ³ãƒ‰
            cmd = [
                "python3", "-m", "llama_cpp.convert",
                str(self.model_path),
                "--outfile", str(output_file),
                "--outtype", "q4_k_m"  # 4bité‡å­åŒ–ã§ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–
            ]
            
            logger.info(f"GGUFå¤‰æ›ã‚³ãƒãƒ³ãƒ‰: {' '.join(cmd)}")
            
            # å¤‰æ›å®Ÿè¡Œ
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=7200  # 2æ™‚é–“ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
            )
            
            if result.returncode == 0:
                logger.info(f"GGUFå¤‰æ›å®Œäº†: {output_file}")
                return {
                    "success": True,
                    "gguf_path": str(output_file),
                    "model_name": model_name
                }
            else:
                logger.error(f"GGUFå¤‰æ›ã‚¨ãƒ©ãƒ¼: {result.stderr}")
                return {
                    "success": False,
                    "error": result.stderr
                }
                
        except subprocess.TimeoutExpired:
            logger.error("GGUFå¤‰æ›ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸ")
            return {"success": False, "error": "Conversion timeout"}
        except Exception as e:
            logger.error(f"GGUFå¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            return {"success": False, "error": str(e)}
    
    def create_ollama_modelfile(self, gguf_path: str, model_name: str) -> str:
        """Ollamaç”¨ã®Modelfileã‚’ä½œæˆ"""
        return f"""
FROM {gguf_path}
PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER top_k 40
PARAMETER repeat_penalty 1.1
PARAMETER stop "Human:"
PARAMETER stop "Assistant:"
PARAMETER stop "è³ªå•:"
PARAMETER stop "å›ç­”:"

# ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿é“è·¯å·¥å­¦å°‚é–€å®¶ãƒ¢ãƒ‡ãƒ«
TEMPLATE """
{{ if .System }}<|im_start|>system
{{ .System }}<|im_end|>
{{ end }}{{ if .Prompt }}<|im_start|>user
{{ .Prompt }}<|im_end|>
{{ end }}<|im_start|>assistant
{{ .Response }}<|im_end|>
"""

SYSTEM """ã‚ãªãŸã¯é“è·¯å·¥å­¦ã®å°‚é–€å®¶ã§ã™ã€‚è³ªå•ã«å¯¾ã—ã¦æ­£ç¢ºã§åˆ†ã‹ã‚Šã‚„ã™ã„å›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚"""
"""
    
    def setup_ollama_model(self, gguf_path: str, model_name: str) -> Dict[str, Any]:
        """Ollamaãƒ¢ãƒ‡ãƒ«ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        try:
            # 1. Modelfileã‚’ä½œæˆ
            modelfile_content = self.create_ollama_modelfile(gguf_path, model_name)
            
            # 2. Modelfileã‚’ä¿å­˜
            modelfile_path = self.output_dir / f"{model_name}.Modelfile"
            with open(modelfile_path, "w", encoding="utf-8") as f:
                f.write(modelfile_content)
            
            # 3. Ollamaãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
            cmd = ["ollama", "create", model_name, str(modelfile_path)]
            result = subprocess.run(cmd, capture_output=True, text=True)
            
            if result.returncode == 0:
                logger.info(f"Ollamaãƒ¢ãƒ‡ãƒ«ä½œæˆæˆåŠŸ: {model_name}")
                return {
                    "success": True,
                    "model_name": model_name,
                    "message": "Ollamaãƒ¢ãƒ‡ãƒ«ãŒæ­£å¸¸ã«ä½œæˆã•ã‚Œã¾ã—ãŸ"
                }
            else:
                return {
                    "success": False,
                    "error": f"Ollamaãƒ¢ãƒ‡ãƒ«ä½œæˆå¤±æ•—: {result.stderr}"
                }
                
        except Exception as e:
            logger.error(f"Ollamaãƒ¢ãƒ‡ãƒ«ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
            return {"success": False, "error": str(e)}
    
    def convert_and_setup(self, model_name: str) -> Dict[str, Any]:
        """ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’å¤‰æ›ã—ã¦Ollamaã§ä½¿ç”¨å¯èƒ½ã«ã™ã‚‹"""
        try:
            logger.info("ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®å¤‰æ›ã‚’é–‹å§‹ã—ã¾ã™")
            
            # 1. GGUFå¤‰æ›
            conversion_result = self.convert_to_gguf(model_name)
            
            if not conversion_result["success"]:
                return conversion_result
            
            # 2. Ollamaãƒ¢ãƒ‡ãƒ«ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
            gguf_path = conversion_result["gguf_path"]
            ollama_result = self.setup_ollama_model(gguf_path, model_name)
            
            if ollama_result["success"]:
                logger.info("âœ… å¤‰æ›ã¨ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãŒå®Œäº†ã—ã¾ã—ãŸ")
                return {
                    "success": True,
                    "model_name": model_name,
                    "gguf_path": gguf_path,
                    "message": "ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒOllamaã§ä½¿ç”¨å¯èƒ½ã«ãªã‚Šã¾ã—ãŸ"
                }
            else:
                return ollama_result
                
        except Exception as e:
            logger.error(f"å¤‰æ›ãƒ»ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
            return {"success": False, "error": str(e)}

def install_requirements():
    """å¿…è¦ãªä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«"""
    try:
        logger.info("å¿…è¦ãªä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­...")
        
        # llama-cpp-pythonã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
        subprocess.run([
            "pip", "install", "llama-cpp-python"
        ], check=True)
        
        # Ollamaã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç¢ºèª
        result = subprocess.run(["ollama", "--version"], capture_output=True, text=True)
        if result.returncode != 0:
            logger.warning("OllamaãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚æ‰‹å‹•ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„:")
            logger.warning("curl -fsSL https://ollama.ai/install.sh | sh")
        
        logger.info("ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå®Œäº†ã—ã¾ã—ãŸ")
        return True
        
    except Exception as e:
        logger.error(f"ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã«å¤±æ•—: {e}")
        return False

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹
    finetuned_model_path = "/workspace/outputs/ãƒ•ãƒ«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°_20250723_041920"
    
    # Ollamaãƒ¢ãƒ‡ãƒ«å
    ollama_model_name = "road-engineering-expert"
    
    logger.info("ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®Ollamaå¤‰æ›ã‚’é–‹å§‹ã—ã¾ã™")
    
    # 1. ä¾å­˜é–¢ä¿‚ã®ç¢ºèªãƒ»ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
    if not install_requirements():
        logger.error("ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return
    
    # 2. å¤‰æ›ãƒ»ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
    converter = FinetunedModelConverter(finetuned_model_path)
    result = converter.convert_and_setup(ollama_model_name)
    
    if result["success"]:
        logger.info("âœ… å¤‰æ›ãŒå®Œäº†ã—ã¾ã—ãŸ")
        logger.info(f"ä½¿ç”¨æ–¹æ³•: ollama run {ollama_model_name}")
        logger.info(f"ä¾‹: ollama run {ollama_model_name} 'ç¸¦æ–­æ›²ç·šã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ'")
        
        # ä½¿ç”¨ä¾‹ã‚’è¡¨ç¤º
        print("\n" + "="*50)
        print("ğŸ‰ ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®å¤‰æ›ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        print("="*50)
        print(f"ãƒ¢ãƒ‡ãƒ«å: {ollama_model_name}")
        print(f"ä½¿ç”¨æ–¹æ³•: ollama run {ollama_model_name}")
        print("ä¾‹:")
        print(f"  ollama run {ollama_model_name} 'ç¸¦æ–­æ›²ç·šã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ'")
        print(f"  ollama run {ollama_model_name} 'é“è·¯ã®æ¨ªæ–­å‹¾é…ã®æ¨™æº–å€¤ã¯ï¼Ÿ'")
        print("="*50)
        
    else:
        logger.error(f"âŒ å¤‰æ›ã«å¤±æ•—ã—ã¾ã—ãŸ: {result.get('error', 'Unknown error')}")

if __name__ == "__main__":
    main() 