#!/usr/bin/env python3
"""
WSLç’°å¢ƒç”¨ï¼šãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’Ollamaã§ä½¿ç”¨ã§ãã‚‹ã‚ˆã†ã«å¤‰æ›ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import os
import json
import logging
import subprocess
import sys
from pathlib import Path
from typing import Dict, Any
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

logger = logging.getLogger(__name__)

class WSLFinetunedModelConverter:
    """WSLç’°å¢ƒç”¨ï¼šãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’Ollamaå½¢å¼ã«å¤‰æ›"""
    
    def __init__(self, model_path: str):
        self.model_path = Path(model_path)
        self.output_dir = Path("ollama_models")
        self.output_dir.mkdir(exist_ok=True)
        
    def check_wsl_environment(self) -> bool:
        """WSLç’°å¢ƒã®ç¢ºèª"""
        try:
            # WSLç’°å¢ƒã®ç¢ºèª
            with open('/proc/version', 'r') as f:
                version_info = f.read()
                if 'microsoft' in version_info.lower():
                    logger.info("WSLç’°å¢ƒã‚’æ¤œå‡ºã—ã¾ã—ãŸ")
                    return True
                else:
                    logger.warning("WSLç’°å¢ƒã§ã¯ã‚ã‚Šã¾ã›ã‚“")
                    return False
        except Exception as e:
            logger.error(f"ç’°å¢ƒç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def install_ollama_wsl(self) -> bool:
        """WSLç’°å¢ƒã§Ollamaã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«"""
        try:
            logger.info("WSLç’°å¢ƒã§Ollamaã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­...")
            
            # ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å®Ÿè¡Œ
            install_script = Path("install_ollama_wsl.sh")
            if install_script.exists():
                subprocess.run(["bash", str(install_script)], check=True)
            else:
                # æ‰‹å‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
                subprocess.run([
                    "curl", "-fsSL", "https://ollama.ai/install.sh", "|", "sh"
                ], shell=True, check=True)
            
            # ç’°å¢ƒå¤‰æ•°ã®è¨­å®š
            os.environ['PATH'] = f"{os.environ.get('HOME')}/.local/bin:{os.environ.get('PATH')}"
            
            # Ollamaã®ç¢ºèª
            result = subprocess.run(["ollama", "--version"], capture_output=True, text=True)
            if result.returncode == 0:
                logger.info(f"Ollamaã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æˆåŠŸ: {result.stdout.strip()}")
                return True
            else:
                logger.error("Ollamaã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã«å¤±æ•—ã—ã¾ã—ãŸ")
                return False
                
        except Exception as e:
            logger.error(f"Ollamaã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def convert_to_gguf(self, model_name: str) -> Dict[str, Any]:
        """ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’GGUFå½¢å¼ã«å¤‰æ›ï¼ˆWSLå¯¾å¿œï¼‰"""
        try:
            logger.info(f"WSLç’°å¢ƒã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«å¤‰æ›é–‹å§‹: {self.model_path}")
            
            # 1. ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã®ç¢ºèª
            if not self.model_path.exists():
                return {"success": False, "error": "ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ãŒå­˜åœ¨ã—ã¾ã›ã‚“"}
            
            # 2. llama-cpp-pythonã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç¢ºèª
            try:
                import llama_cpp
                logger.info("llama-cpp-pythonãŒåˆ©ç”¨å¯èƒ½ã§ã™")
            except ImportError:
                logger.info("llama-cpp-pythonã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­...")
                subprocess.run([
                    sys.executable, "-m", "pip", "install", "llama-cpp-python"
                ], check=True)
            
            # 3. GGUFå¤‰æ›ã®å®Ÿè¡Œ
            return self._run_gguf_conversion_wsl(model_name)
            
        except Exception as e:
            logger.error(f"å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            return {"success": False, "error": str(e)}
    
    def _run_gguf_conversion_wsl(self, model_name: str) -> Dict[str, Any]:
        """WSLç’°å¢ƒã§GGUFå¤‰æ›ã‚’å®Ÿè¡Œ"""
        try:
            # llama.cppã‚’ä½¿ç”¨ã—ãŸGGUFå¤‰æ›
            output_file = self.output_dir / f"{model_name}.gguf"
            
            # WSLç’°å¢ƒç”¨ã®å¤‰æ›ã‚³ãƒãƒ³ãƒ‰
            cmd = [
                sys.executable, "-m", "llama_cpp.convert",
                str(self.model_path),
                "--outfile", str(output_file),
                "--outtype", "q4_k_m"  # 4bité‡å­åŒ–ã§ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–
            ]
            
            logger.info(f"WSL GGUFå¤‰æ›ã‚³ãƒãƒ³ãƒ‰: {' '.join(cmd)}")
            
            # å¤‰æ›å®Ÿè¡Œï¼ˆWSLç’°å¢ƒã§ã®æœ€é©åŒ–ï¼‰
            env = os.environ.copy()
            env['OMP_NUM_THREADS'] = '4'  # ã‚¹ãƒ¬ãƒƒãƒ‰æ•°ã‚’åˆ¶é™
            env['MKL_NUM_THREADS'] = '4'
            
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=7200,  # 2æ™‚é–“ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                env=env
            )
            
            if result.returncode == 0:
                logger.info(f"WSL GGUFå¤‰æ›å®Œäº†: {output_file}")
                return {
                    "success": True,
                    "gguf_path": str(output_file),
                    "model_name": model_name
                }
            else:
                logger.error(f"WSL GGUFå¤‰æ›ã‚¨ãƒ©ãƒ¼: {result.stderr}")
                return {
                    "success": False,
                    "error": result.stderr
                }
                
        except subprocess.TimeoutExpired:
            logger.error("WSL GGUFå¤‰æ›ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸ")
            return {"success": False, "error": "Conversion timeout"}
        except Exception as e:
            logger.error(f"WSL GGUFå¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            return {"success": False, "error": str(e)}
    
    def create_ollama_modelfile_wsl(self, gguf_path: str, model_name: str) -> str:
        """WSLç’°å¢ƒç”¨ã®Ollama Modelfileã‚’ä½œæˆ"""
        template_content = f"""FROM {gguf_path}
PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER top_k 40
PARAMETER repeat_penalty 1.1
PARAMETER stop "Human:"
PARAMETER stop "Assistant:"
PARAMETER stop "è³ªå•:"
PARAMETER stop "å›ç­”:"

# WSLç’°å¢ƒç”¨ï¼šãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿é“è·¯å·¥å­¦å°‚é–€å®¶ãƒ¢ãƒ‡ãƒ«
TEMPLATE """
{{ if .System }}<|im_start|>system
{{ .System }}<|im_end|>
{{ end }}{{ if .Prompt }}<|im_start|>user
{{ .Prompt }}<|im_end|>
{{ end }}<|im_start|>assistant
{{ .Response }}<|im_end|>
"""

SYSTEM """ã‚ãªãŸã¯é“è·¯å·¥å­¦ã®å°‚é–€å®¶ã§ã™ã€‚è³ªå•ã«å¯¾ã—ã¦æ­£ç¢ºã§åˆ†ã‹ã‚Šã‚„ã™ã„å›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚"""
"""
        return template_content
    
    def setup_ollama_model_wsl(self, gguf_path: str, model_name: str) -> Dict[str, Any]:
        """WSLç’°å¢ƒã§Ollamaãƒ¢ãƒ‡ãƒ«ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        try:
            # 1. Modelfileã‚’ä½œæˆ
            modelfile_content = self.create_ollama_modelfile_wsl(gguf_path, model_name)
            
            # 2. Modelfileã‚’ä¿å­˜
            modelfile_path = self.output_dir / f"{model_name}.Modelfile"
            with open(modelfile_path, "w", encoding="utf-8") as f:
                f.write(modelfile_content)
            
            # 3. WSLç’°å¢ƒã§Ollamaãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
            cmd = ["ollama", "create", model_name, str(modelfile_path)]
            result = subprocess.run(cmd, capture_output=True, text=True)
            
            if result.returncode == 0:
                logger.info(f"WSL Ollamaãƒ¢ãƒ‡ãƒ«ä½œæˆæˆåŠŸ: {model_name}")
                return {
                    "success": True,
                    "model_name": model_name,
                    "message": "WSLç’°å¢ƒã§Ollamaãƒ¢ãƒ‡ãƒ«ãŒæ­£å¸¸ã«ä½œæˆã•ã‚Œã¾ã—ãŸ"
                }
            else:
                return {
                    "success": False,
                    "error": f"WSL Ollamaãƒ¢ãƒ‡ãƒ«ä½œæˆå¤±æ•—: {result.stderr}"
                }
                
        except Exception as e:
            logger.error(f"WSL Ollamaãƒ¢ãƒ‡ãƒ«ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
            return {"success": False, "error": str(e)}
    
    def convert_and_setup_wsl(self, model_name: str) -> Dict[str, Any]:
        """WSLç’°å¢ƒã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’å¤‰æ›ã—ã¦Ollamaã§ä½¿ç”¨å¯èƒ½ã«ã™ã‚‹"""
        try:
            logger.info("WSLç’°å¢ƒã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®å¤‰æ›ã‚’é–‹å§‹ã—ã¾ã™")
            
            # 1. WSLç’°å¢ƒã®ç¢ºèª
            if not self.check_wsl_environment():
                logger.warning("WSLç’°å¢ƒã§ã¯ã‚ã‚Šã¾ã›ã‚“ãŒã€å‡¦ç†ã‚’ç¶šè¡Œã—ã¾ã™")
            
            # 2. Ollamaã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç¢ºèª
            try:
                subprocess.run(["ollama", "--version"], capture_output=True, check=True)
                logger.info("OllamaãŒåˆ©ç”¨å¯èƒ½ã§ã™")
            except (subprocess.CalledProcessError, FileNotFoundError):
                logger.info("Ollamaã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™")
                if not self.install_ollama_wsl():
                    return {"success": False, "error": "Ollamaã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã«å¤±æ•—ã—ã¾ã—ãŸ"}
            
            # 3. GGUFå¤‰æ›
            conversion_result = self.convert_to_gguf(model_name)
            
            if not conversion_result["success"]:
                return conversion_result
            
            # 4. Ollamaãƒ¢ãƒ‡ãƒ«ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
            gguf_path = conversion_result["gguf_path"]
            ollama_result = self.setup_ollama_model_wsl(gguf_path, model_name)
            
            if ollama_result["success"]:
                logger.info("âœ… WSLç’°å¢ƒã§ã®å¤‰æ›ã¨ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãŒå®Œäº†ã—ã¾ã—ãŸ")
                return {
                    "success": True,
                    "model_name": model_name,
                    "gguf_path": gguf_path,
                    "message": "WSLç’°å¢ƒã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒOllamaã§ä½¿ç”¨å¯èƒ½ã«ãªã‚Šã¾ã—ãŸ"
                }
            else:
                return ollama_result
                
        except Exception as e:
            logger.error(f"WSLå¤‰æ›ãƒ»ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
            return {"success": False, "error": str(e)}

def main():
    """WSLç’°å¢ƒç”¨ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹
    finetuned_model_path = "/workspace/outputs/ãƒ•ãƒ«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°_20250723_041920"
    
    # Ollamaãƒ¢ãƒ‡ãƒ«å
    ollama_model_name = "road-engineering-expert"
    
    logger.info("WSLç’°å¢ƒã§ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®Ollamaå¤‰æ›ã‚’é–‹å§‹ã—ã¾ã™")
    
    # 1. WSLç’°å¢ƒã§ã®å¤‰æ›ãƒ»ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
    converter = WSLFinetunedModelConverter(finetuned_model_path)
    result = converter.convert_and_setup_wsl(ollama_model_name)
    
    if result["success"]:
        logger.info("âœ… WSLç’°å¢ƒã§ã®å¤‰æ›ãŒå®Œäº†ã—ã¾ã—ãŸ")
        logger.info(f"ä½¿ç”¨æ–¹æ³•: ollama run {ollama_model_name}")
        logger.info(f"ä¾‹: ollama run {ollama_model_name} 'ç¸¦æ–­æ›²ç·šã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ'")
        
        # ä½¿ç”¨ä¾‹ã‚’è¡¨ç¤º
        print("\n" + "="*50)
        print("ğŸ‰ WSLç’°å¢ƒã§ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®å¤‰æ›ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        print("="*50)
        print(f"ãƒ¢ãƒ‡ãƒ«å: {ollama_model_name}")
        print(f"ä½¿ç”¨æ–¹æ³•: ollama run {ollama_model_name}")
        print("ä¾‹:")
        print(f"  ollama run {ollama_model_name} 'ç¸¦æ–­æ›²ç·šã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ'")
        print(f"  ollama run {ollama_model_name} 'é“è·¯ã®æ¨ªæ–­å‹¾é…ã®æ¨™æº–å€¤ã¯ï¼Ÿ'")
        print("="*50)
        
    else:
        logger.error(f"âŒ WSLç’°å¢ƒã§ã®å¤‰æ›ã«å¤±æ•—ã—ã¾ã—ãŸ: {result.get('error', 'Unknown error')}")

if __name__ == "__main__":
    main() 