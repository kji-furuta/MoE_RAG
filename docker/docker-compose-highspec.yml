version: '3.8'

services:
  ai-ft:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    image: ai-ft-rag:latest
    container_name: ai-ft-container
    hostname: ai-ft
    
    # GPU access with proper configuration
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
        limits:
          # 明示的なリソース制限（十分な値）
          memory: 80G  # 80GB RAM制限
          cpus: '16'   # 16 CPU cores
    
    # Environment variables
    environment:
      - CUDA_VISIBLE_DEVICES=0,1
      - PYTHONPATH=/workspace
      - WANDB_API_KEY=${WANDB_API_KEY:-}
      - HF_TOKEN=${HF_TOKEN:-}
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      # タイムアウト設定を追加
      - UVICORN_TIMEOUT_KEEP_ALIVE=120
      - UVICORN_TIMEOUT_NOTIFY=120
      - TRANSFORMERS_TIMEOUT=300
      - HF_HUB_DOWNLOAD_TIMEOUT=300
      # バッファサイズ増加
      - PYTHONUNBUFFERED=1
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True,max_split_size_mb:512
    
    # Port mappings
    ports:
      - "8888:8888"  # Jupyter Lab
      - "6006:6006"  # TensorBoard
      - "8050:8050"  # Web Interface
      - "8051:8051"  # RAG API
      - "11434:11434"  # Ollama API
    
    # Volume mounts
    volumes:
      - ../src:/workspace/src
      - ../config:/workspace/config
      - ../scripts:/workspace/scripts
      - ../notebooks:/workspace/notebooks
      - ../data:/workspace/data
      - ../models:/workspace/models
      - ../tests:/workspace/tests
      - ../app:/workspace/app
      - ../templates:/workspace/templates
      - ../outputs:/workspace/outputs
      - ./logs:/workspace/logs
      - ~/.cache/huggingface:/home/ai-user/.cache/huggingface
      - ~/.wandb:/home/ai-user/.wandb
      # RAG専用
      - ../temp_uploads:/workspace/temp_uploads
      - ../qdrant_data:/workspace/qdrant_data
      - ../docs:/workspace/docs
      - ../examples:/workspace/examples
      # データ永続化
      - ai_ft_rag_metadata:/workspace/metadata
      - ai_ft_rag_processed:/workspace/outputs/rag_index
      - ollama_models:/usr/share/ollama/.ollama/models
    
    # Working directory
    working_dir: /workspace
    
    # Keep container running
    tty: true
    stdin_open: true
    
    # Restart policy
    restart: unless-stopped
    
    # Shared memory size - 重要！深層学習には大きな値が必要
    shm_size: '32gb'
    
    # Ulimits for better performance
    ulimits:
      memlock:
        soft: -1
        hard: -1
      stack:
        soft: 67108864
        hard: 67108864
      nofile:
        soft: 65536
        hard: 65536

  # Qdrant Vector Database
  qdrant:
    image: qdrant/qdrant:latest
    container_name: ai-ft-qdrant
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant_storage:/qdrant/storage
    environment:
      - QDRANT__TELEMETRY_DISABLED=true
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '2'

# Networks
networks:
  default:
    name: ai-ft-network

# Volumes
volumes:
  models_data:
    driver: local
  logs_data:
    driver: local
  qdrant_storage:
    driver: local
  ai_ft_rag_metadata:
    driver: local
  ai_ft_rag_processed:
    driver: local
  ollama_models:
    driver: local
