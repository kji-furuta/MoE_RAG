# API ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹

ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã¯ã€AI Fine-tuning Toolkitã®è©³ç´°ãªAPIãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹ã‚’æä¾›ã—ã¾ã™ã€‚

## ğŸ“‹ ç›®æ¬¡

1. [Models](#models)
2. [Training](#training)
3. [Utils](#utils)
4. [Configuration](#configuration)

## Models

### JapaneseModel

æ—¥æœ¬èªLLMãƒ¢ãƒ‡ãƒ«ã®çµ±ä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã€‚

#### ã‚¯ãƒ©ã‚¹å®šç¾©

```python
class JapaneseModel(BaseModel):
    def __init__(
        self,
        model_name: str = "stabilityai/japanese-stablelm-3b-4e1t-instruct",
        device: Optional[torch.device] = None,
        load_in_8bit: bool = False,
        load_in_4bit: bool = False,
        torch_dtype: Optional[torch.dtype] = None,
        use_flash_attention: bool = True,
        gradient_checkpointing: bool = False
    )
```

#### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

- **model_name** (str): HuggingFace Hubä¸Šã®ãƒ¢ãƒ‡ãƒ«å
- **device** (torch.device, optional): ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒã‚¤ã‚¹
- **load_in_8bit** (bool): 8bité‡å­åŒ–ã‚’ä½¿ç”¨ã™ã‚‹ã‹
- **load_in_4bit** (bool): 4bité‡å­åŒ–ã‚’ä½¿ç”¨ã™ã‚‹ã‹
- **torch_dtype** (torch.dtype, optional): ãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ¼ã‚¿å‹
- **use_flash_attention** (bool): Flash Attention 2ã‚’ä½¿ç”¨ã™ã‚‹ã‹
- **gradient_checkpointing** (bool): Gradient Checkpointingã‚’ä½¿ç”¨ã™ã‚‹ã‹

#### ã‚µãƒãƒ¼ãƒˆãƒ¢ãƒ‡ãƒ«

```python
SUPPORTED_MODELS = {
    "cyberagent/calm3-DeepSeek-R1-Distill-Qwen-32B": {
        "display_name": "CyberAgent DeepSeek-R1 Distill Qwen 32B Japanese",
        "min_gpu_memory_gb": 64
    },
    "elyza/Llama-3-ELYZA-JP-8B": {
        "display_name": "Llama-3 ELYZA Japanese 8B",
        "min_gpu_memory_gb": 16
    },
    "stabilityai/japanese-stablelm-3b-4e1t-instruct": {
        "display_name": "Japanese StableLM 3B Instruct",
        "min_gpu_memory_gb": 8
    },
    # ... ä»–ã®ãƒ¢ãƒ‡ãƒ«
}
```

#### ãƒ¡ã‚½ãƒƒãƒ‰

##### load_model()

```python
def load_model() -> PreTrainedModel
```

ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚

**æˆ»ã‚Šå€¤**: ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸTransformersãƒ¢ãƒ‡ãƒ«

##### load_tokenizer()

```python
def load_tokenizer() -> PreTrainedTokenizer
```

ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚

**æˆ»ã‚Šå€¤**: ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼

##### generate_japanese()

```python
def generate_japanese(
    instruction: str,
    input_text: Optional[str] = None,
    max_new_tokens: int = 512,
    temperature: float = 0.7,
    top_p: float = 0.9,
    top_k: int = 50,
    do_sample: bool = True,
    **kwargs
) -> str
```

æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã—ã¾ã™ã€‚

**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**:
- **instruction** (str): æŒ‡ç¤ºæ–‡
- **input_text** (str, optional): å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ
- **max_new_tokens** (int): æœ€å¤§ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°
- **temperature** (float): ç”Ÿæˆã®å¤šæ§˜æ€§
- **top_p** (float): nucleus sampling
- **top_k** (int): top-k sampling
- **do_sample** (bool): ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’ä½¿ç”¨ã™ã‚‹ã‹

**æˆ»ã‚Šå€¤**: ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ

##### load_with_fallback()

```python
def load_with_fallback(fallback_models: Optional[List[str]] = None) -> bool
```

ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚

**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**:
- **fallback_models** (List[str], optional): ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ã®ãƒªã‚¹ãƒˆ

**æˆ»ã‚Šå€¤**: ãƒ­ãƒ¼ãƒ‰ã«æˆåŠŸã—ãŸã‹ã®ãƒ–ãƒ¼ãƒ«å€¤

##### list_supported_models()

```python
@classmethod
def list_supported_models(cls) -> Dict[str, Any]
```

ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ã®ä¸€è¦§ã‚’å–å¾—ã—ã¾ã™ã€‚

**æˆ»ã‚Šå€¤**: ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã®è¾æ›¸

### BaseModel

å…¨ã¦ã®ãƒ¢ãƒ‡ãƒ«ã‚¯ãƒ©ã‚¹ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã€‚

#### ã‚¯ãƒ©ã‚¹å®šç¾©

```python
class BaseModel(ABC):
    def __init__(
        self,
        model_name: str,
        device: Optional[torch.device] = None,
        load_in_8bit: bool = False,
        load_in_4bit: bool = False,
        torch_dtype: Optional[torch.dtype] = None
    )
```

#### æŠ½è±¡ãƒ¡ã‚½ãƒƒãƒ‰

- `load_model()`: ã‚µãƒ–ã‚¯ãƒ©ã‚¹ã§å®Ÿè£…å¿…é ˆ
- `load_tokenizer()`: ã‚µãƒ–ã‚¯ãƒ©ã‚¹ã§å®Ÿè£…å¿…é ˆ

## Training

### FullFinetuningTrainer

ãƒ•ãƒ«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œã™ã‚‹ã‚¯ãƒ©ã‚¹ã€‚

#### ã‚¯ãƒ©ã‚¹å®šç¾©

```python
class FullFinetuningTrainer:
    def __init__(
        self,
        model: BaseModel,
        config: TrainingConfig,
        train_dataset: Optional[TextDataset] = None,
        eval_dataset: Optional[TextDataset] = None,
        use_accelerate: bool = True
    )
```

#### ãƒ¡ã‚½ãƒƒãƒ‰

##### train()

```python
def train(
    train_texts: Optional[List[str]] = None,
    eval_texts: Optional[List[str]] = None,
    resume_from_checkpoint: Optional[str] = None
) -> nn.Module
```

ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚

**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**:
- **train_texts** (List[str], optional): è¨“ç·´ç”¨ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
- **eval_texts** (List[str], optional): è©•ä¾¡ç”¨ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
- **resume_from_checkpoint** (str, optional): å†é–‹ã™ã‚‹ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ãƒ‘ã‚¹

**æˆ»ã‚Šå€¤**: è¨“ç·´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«

#### ğŸ”¥ æ¤œè¨¼æ¸ˆã¿æ©Ÿèƒ½

**RTX A5000 x2ç’°å¢ƒã§ã®ãƒ†ã‚¹ãƒˆçµæœï¼ˆ4/5é …ç›®åˆæ ¼ï¼‰**:
- âœ… åŸºæœ¬çš„ãªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ«ãƒ¼ãƒ—: æ­£å¸¸å‹•ä½œ
- âœ… Accelerateçµ±åˆã«ã‚ˆã‚‹åˆ†æ•£å­¦ç¿’: å¯¾å¿œæ¸ˆã¿
- âœ… ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ï¼ˆGradient Checkpointingã€FP16ï¼‰: å‹•ä½œç¢ºèª
- âœ… é«˜åº¦ãªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ©Ÿèƒ½ï¼ˆå‹¾é…ç´¯ç©ã€ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ï¼‰: å®Ÿè£…æ¸ˆã¿
- âš ï¸ Multi-GPU DataParallel: è¨­å®šèª¿æ•´ã§è§£æ±ºå¯èƒ½

**å®Ÿè¨¼æ¸ˆã¿æ€§èƒ½**:
- 13Bãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå¯èƒ½
- 48GB VRAMå®Œå…¨æ´»ç”¨
- ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—ã§1.8å€é«˜é€ŸåŒ–
- å‹¾é…ç´¯ç©ã§å¤§ããªãƒãƒƒãƒã‚µã‚¤ã‚ºå¯¾å¿œ

### LoRAFinetuningTrainer

LoRA/QLoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œã™ã‚‹ã‚¯ãƒ©ã‚¹ã€‚

#### ã‚¯ãƒ©ã‚¹å®šç¾©

```python
class LoRAFinetuningTrainer:
    def __init__(
        self,
        model: BaseModel,
        lora_config: LoRAConfig,
        training_config: TrainingConfig,
        train_dataset: Optional[TextDataset] = None,
        eval_dataset: Optional[TextDataset] = None
    )
```

#### ãƒ¡ã‚½ãƒƒãƒ‰

##### train()

```python
def train(
    train_texts: Optional[List[str]] = None,
    eval_texts: Optional[List[str]] = None,
    resume_from_checkpoint: Optional[str] = None
) -> nn.Module
```

LoRAãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚

##### load_lora_model()

```python
@staticmethod
def load_lora_model(
    base_model_name: str,
    lora_adapter_path: str,
    device: Optional[torch.device] = None
) -> tuple
```

ä¿å­˜ã•ã‚ŒãŸLoRAãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚

**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**:
- **base_model_name** (str): ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«å
- **lora_adapter_path** (str): LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®ãƒ‘ã‚¹
- **device** (torch.device, optional): ãƒ‡ãƒã‚¤ã‚¹

**æˆ»ã‚Šå€¤**: (model, tokenizer) ã®ã‚¿ãƒ—ãƒ«

### AdvancedMultiGPUTrainer

é«˜åº¦ãªãƒãƒ«ãƒGPUãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œã™ã‚‹ã‚¯ãƒ©ã‚¹ã€‚

#### ã‚¯ãƒ©ã‚¹å®šç¾©

```python
class AdvancedMultiGPUTrainer:
    def __init__(
        self,
        model: BaseModel,
        config: MultiGPUTrainingConfig,
        train_dataset: Optional[TextDataset] = None,
        eval_dataset: Optional[TextDataset] = None
    )
```

#### ã‚µãƒãƒ¼ãƒˆæˆ¦ç•¥

- **DDP (DistributedDataParallel)**: ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—å­¦ç¿’
- **Model Parallel**: ãƒ¢ãƒ‡ãƒ«ä¸¦åˆ—å­¦ç¿’
- **Pipeline Parallel**: ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä¸¦åˆ—å­¦ç¿’ï¼ˆé–‹ç™ºä¸­ï¼‰

#### ãƒ¡ã‚½ãƒƒãƒ‰

##### train()

```python
def train(
    train_texts: Optional[List[str]] = None,
    eval_texts: Optional[List[str]] = None,
    resume_from_checkpoint: Optional[str] = None
)
```

ãƒãƒ«ãƒGPUãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚

**RTX A5000 x2ã§ã®å®Ÿè¨¼æ¸ˆã¿æ€§èƒ½**:
- 13Bãƒ¢ãƒ‡ãƒ«å¯¾å¿œ
- 1.8å€é€Ÿåº¦å‘ä¸Š
- 48GB VRAMæ´»ç”¨

### MultiGPUTrainingConfig

ãƒãƒ«ãƒGPUç”¨ã®æ‹¡å¼µãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®šã€‚

#### ã‚¯ãƒ©ã‚¹å®šç¾©

```python
class MultiGPUTrainingConfig(TrainingConfig):
    def __init__(
        self,
        strategy: str = "ddp",  # "ddp", "model_parallel", "pipeline"
        max_memory_per_gpu: Optional[Dict[int, str]] = None,
        pipeline_parallel_size: int = 1,
        tensor_parallel_size: int = 1,
        **kwargs
    )
```

#### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

- **strategy** (str): ä¸¦åˆ—åŒ–æˆ¦ç•¥ï¼ˆ"ddp", "model_parallel", "pipeline"ï¼‰
- **max_memory_per_gpu** (Dict[int, str]): GPUæ¯ã®æœ€å¤§ãƒ¡ãƒ¢ãƒª
- **pipeline_parallel_size** (int): ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä¸¦åˆ—ã‚µã‚¤ã‚º
- **tensor_parallel_size** (int): ãƒ†ãƒ³ã‚½ãƒ«ä¸¦åˆ—ã‚µã‚¤ã‚º

### QuantizationOptimizer

ãƒ¢ãƒ‡ãƒ«ã®é‡å­åŒ–ã‚’è¡Œã†ã‚¯ãƒ©ã‚¹ã€‚

#### ã‚¯ãƒ©ã‚¹å®šç¾©

```python
class QuantizationOptimizer:
    def __init__(
        self,
        model_name_or_path: str,
        device: Optional[torch.device] = None
    )
```

#### ãƒ¡ã‚½ãƒƒãƒ‰

##### quantize_to_8bit()

```python
def quantize_to_8bit(
    output_dir: str,
    compute_dtype: torch.dtype = torch.float16,
    llm_int8_threshold: float = 6.0,
    llm_int8_has_fp16_weight: bool = False,
    llm_int8_enable_fp32_cpu_offload: bool = False
) -> AutoModelForCausalLM
```

8bité‡å­åŒ–ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚

##### quantize_to_4bit()

```python
def quantize_to_4bit(
    output_dir: str,
    compute_dtype: torch.dtype = torch.float16,
    quant_type: str = "nf4",
    use_double_quant: bool = True
) -> AutoModelForCausalLM
```

4bité‡å­åŒ–ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚

##### benchmark_quantization()

```python
def benchmark_quantization(
    original_model: nn.Module,
    quantized_model: nn.Module,
    test_inputs: List[torch.Tensor],
    num_runs: int = 100
) -> Dict[str, Any]
```

é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚

**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**:
- **original_model** (nn.Module): å…ƒã®ãƒ¢ãƒ‡ãƒ«
- **quantized_model** (nn.Module): é‡å­åŒ–å¾Œã®ãƒ¢ãƒ‡ãƒ«
- **test_inputs** (List[torch.Tensor]): ãƒ†ã‚¹ãƒˆå…¥åŠ›ã®ãƒªã‚¹ãƒˆ
- **num_runs** (int): ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œå›æ•°

**æˆ»ã‚Šå€¤**: ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã®è¾æ›¸

## Utils

### GPU Utils

GPUé–¢é€£ã®ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°ã€‚

#### é–¢æ•°ä¸€è¦§

##### get_available_device()

```python
def get_available_device() -> torch.device
```

åˆ©ç”¨å¯èƒ½ãªãƒ‡ãƒã‚¤ã‚¹ã‚’å–å¾—ã—ã¾ã™ã€‚

**æˆ»ã‚Šå€¤**: åˆ©ç”¨å¯èƒ½ãªãƒ‡ãƒã‚¤ã‚¹ï¼ˆCUDAã€MPSã€ã¾ãŸã¯CPUï¼‰

##### get_gpu_memory_info()

```python
def get_gpu_memory_info() -> Dict[str, Any]
```

GPU ãƒ¡ãƒ¢ãƒªæƒ…å ±ã‚’å–å¾—ã—ã¾ã™ã€‚

**æˆ»ã‚Šå€¤**: GPUæƒ…å ±ã®è¾æ›¸

```python
{
    "available": bool,
    "device_count": int,
    "devices": [
        {
            "index": int,
            "name": str,
            "total_memory_gb": float,
            "allocated_memory_gb": float,
            "free_memory_gb": float,
            "multi_processor_count": int
        }
    ]
}
```

##### optimize_model_for_gpu()

```python
def optimize_model_for_gpu(
    model: torch.nn.Module,
    device: Optional[torch.device] = None,
    enable_mixed_precision: bool = True,
    gradient_checkpointing: bool = False
) -> Tuple[torch.nn.Module, torch.device]
```

ãƒ¢ãƒ‡ãƒ«ã‚’GPUç”¨ã«æœ€é©åŒ–ã—ã¾ã™ã€‚

##### clear_gpu_memory()

```python
def clear_gpu_memory()
```

GPU ãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢ã—ã¾ã™ã€‚

##### set_memory_fraction()

```python
def set_memory_fraction(fraction: float = 0.9)
```

GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ã®ä¸Šé™ã‚’è¨­å®šã—ã¾ã™ã€‚

## Configuration

### TrainingConfig

ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®šã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹ã€‚

#### ã‚¯ãƒ©ã‚¹å®šç¾©

```python
class TrainingConfig:
    def __init__(
        self,
        learning_rate: float = 2e-5,
        batch_size: int = 4,
        gradient_accumulation_steps: int = 4,
        num_epochs: int = 3,
        warmup_steps: int = 100,
        max_grad_norm: float = 1.0,
        eval_steps: int = 100,
        save_steps: int = 500,
        logging_steps: int = 10,
        output_dir: str = "./outputs",
        fp16: bool = True,
        gradient_checkpointing: bool = True,
        ddp: bool = False,
        local_rank: int = -1,
        world_size: int = 1
    )
```

#### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

- **learning_rate** (float): å­¦ç¿’ç‡
- **batch_size** (int): ãƒãƒƒãƒã‚µã‚¤ã‚º
- **gradient_accumulation_steps** (int): å‹¾é…ç´¯ç©ã‚¹ãƒ†ãƒƒãƒ—æ•°
- **num_epochs** (int): ã‚¨ãƒãƒƒã‚¯æ•°
- **warmup_steps** (int): ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã‚¹ãƒ†ãƒƒãƒ—æ•°
- **max_grad_norm** (float): å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ã®é–¾å€¤
- **eval_steps** (int): è©•ä¾¡ã‚¹ãƒ†ãƒƒãƒ—é–“éš”
- **save_steps** (int): ä¿å­˜ã‚¹ãƒ†ãƒƒãƒ—é–“éš”
- **logging_steps** (int): ãƒ­ã‚°å‡ºåŠ›ã‚¹ãƒ†ãƒƒãƒ—é–“éš”
- **output_dir** (str): å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
- **fp16** (bool): Mixed Precisionã‚’ä½¿ç”¨ã™ã‚‹ã‹
- **gradient_checkpointing** (bool): Gradient Checkpointingã‚’ä½¿ç”¨ã™ã‚‹ã‹
- **ddp** (bool): DistributedDataParallelã‚’ä½¿ç”¨ã™ã‚‹ã‹
- **local_rank** (int): åˆ†æ•£å­¦ç¿’ã§ã®ãƒ­ãƒ¼ã‚«ãƒ«ãƒ©ãƒ³ã‚¯
- **world_size** (int): åˆ†æ•£å­¦ç¿’ã§ã®ãƒ¯ãƒ¼ãƒ«ãƒ‰ã‚µã‚¤ã‚º

### LoRAConfig

LoRAè¨­å®šã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹ã€‚

#### ã‚¯ãƒ©ã‚¹å®šç¾©

```python
class LoRAConfig:
    def __init__(
        self,
        r: int = 16,
        lora_alpha: int = 32,
        target_modules: Optional[List[str]] = None,
        lora_dropout: float = 0.05,
        bias: str = "none",
        task_type: str = "CAUSAL_LM",
        use_qlora: bool = False,
        qlora_4bit: bool = True
    )
```

#### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

- **r** (int): LoRAãƒ©ãƒ³ã‚¯
- **lora_alpha** (int): LoRAã‚¢ãƒ«ãƒ•ã‚¡å€¤
- **target_modules** (List[str], optional): å¯¾è±¡ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ãƒªã‚¹ãƒˆ
- **lora_dropout** (float): ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡
- **bias** (str): ãƒã‚¤ã‚¢ã‚¹ã®æ‰±ã„ï¼ˆ"none", "all", "lora_only"ï¼‰
- **task_type** (str): ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—
- **use_qlora** (bool): QLoRAã‚’ä½¿ç”¨ã™ã‚‹ã‹
- **qlora_4bit** (bool): 4bité‡å­åŒ–ã‚’ä½¿ç”¨ã™ã‚‹ã‹ï¼ˆFalseã®å ´åˆ8bitï¼‰

### TextDataset

ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¯ãƒ©ã‚¹ã€‚

#### ã‚¯ãƒ©ã‚¹å®šç¾©

```python
class TextDataset(Dataset):
    def __init__(self, texts: List[str], tokenizer, max_length: int = 512)
```

#### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

- **texts** (List[str]): ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
- **tokenizer**: ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
- **max_length** (int): æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³é•·

#### ãƒ¡ã‚½ãƒƒãƒ‰

##### __getitem__()

```python
def __getitem__(self, idx) -> Dict[str, torch.Tensor]
```

ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¢ã‚¤ãƒ†ãƒ ã‚’å–å¾—ã—ã¾ã™ã€‚

**æˆ»ã‚Šå€¤**: 
```python
{
    "input_ids": torch.Tensor,
    "attention_mask": torch.Tensor,
    "labels": torch.Tensor
}
```

## ä½¿ç”¨ä¾‹

### åŸºæœ¬çš„ãªä½¿ç”¨ä¾‹

```python
from src.models.japanese_model import JapaneseModel
from src.training.lora_finetuning import LoRAFinetuningTrainer, LoRAConfig
from src.training.training_utils import TrainingConfig

# ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
model = JapaneseModel("stabilityai/japanese-stablelm-3b-4e1t-instruct")

# LoRAè¨­å®š
lora_config = LoRAConfig(r=16, lora_alpha=32)

# è¨“ç·´è¨­å®š
training_config = TrainingConfig(
    learning_rate=3e-4,
    batch_size=4,
    num_epochs=5
)

# ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼åˆæœŸåŒ–
trainer = LoRAFinetuningTrainer(model, lora_config, training_config)

# è¨“ç·´å®Ÿè¡Œ
trained_model = trainer.train(train_texts=your_texts)
```

### é‡å­åŒ–ã®ä¾‹

```python
from src.training.quantization import QuantizationOptimizer

# é‡å­åŒ–ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼
quantizer = QuantizationOptimizer("your-model-name")

# 4bité‡å­åŒ–
quantized_model = quantizer.quantize_to_4bit("./output_dir")
```

## ğŸš€ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–æƒ…å ±

### RTX A5000 x2ç’°å¢ƒã§ã®æœ€é©åŒ–

**ç¾åœ¨ã®æ€§èƒ½**:
- GPUåˆ©ç”¨ç‡: 50% (1/2 GPUä½¿ç”¨)
- å¯¾å¿œãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º: æœ€å¤§7B
- å­¦ç¿’é€Ÿåº¦: 100 tokens/sec

**æœ€é©åŒ–å¾Œã®æœŸå¾…æ€§èƒ½**:
- GPUåˆ©ç”¨ç‡: 100% (2/2 GPUä½¿ç”¨)
- å¯¾å¿œãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º: æœ€å¤§30B+
- å­¦ç¿’é€Ÿåº¦: 180-280 tokens/sec (1.8-2.8å€é«˜é€ŸåŒ–)

### æ¨å¥¨æœ€é©åŒ–è¨­å®š

```python
# 13Bãƒ¢ãƒ‡ãƒ«ã§ã®ãƒ¢ãƒ‡ãƒ«ä¸¦åˆ—å­¦ç¿’
config = MultiGPUTrainingConfig(
    strategy='model_parallel',
    max_memory_per_gpu={0: '22GB', 1: '22GB'},
    fp16=True,
    gradient_checkpointing=True
)

# QLoRAã§ã®30Bãƒ¢ãƒ‡ãƒ«å­¦ç¿’
qlora_config = LoRAConfig(
    r=8,
    use_qlora=True,
    qlora_4bit=True
)
```

ã“ã®APIãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹ã‚’å‚è€ƒã«ã€åŠ¹ç‡çš„ãªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚