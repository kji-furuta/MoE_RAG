# å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ï¼ˆ17Bãƒ»22Bãƒ»32Bãƒ»70Bï¼‰ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¬ã‚¤ãƒ‰

## æ¦‚è¦

Webã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã§17Bã€22Bã€32Bã€70Bã®å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ãŒè¡¨ç¤ºã•ã‚Œãªã„å ´åˆã®æ‰‹å‹•ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ–¹æ³•ã‚’èª¬æ˜Žã—ã¾ã™ã€‚

### ðŸ†• æ–°è¦è¿½åŠ ãƒ¢ãƒ‡ãƒ«
- **CyberAgent CALM3-22B**: æ—¥æœ¬èªžç‰¹åŒ–ã®å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«

## æ‰‹å‹•ã§ã®ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ‰‹é †

### 1. Hugging Faceãƒˆãƒ¼ã‚¯ãƒ³ã®æº–å‚™ï¼ˆå¿…è¦ãªå ´åˆï¼‰

ä¸€éƒ¨ã®ãƒ¢ãƒ‡ãƒ«ã¯èªè¨¼ãŒå¿…è¦ã§ã™ã€‚ä»¥ä¸‹ã®æ‰‹é †ã§ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¨­å®šã—ã¦ãã ã•ã„ï¼š

```bash
# Hugging Faceã«ãƒ­ã‚°ã‚¤ãƒ³
huggingface-cli login
# ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å…¥åŠ›
```

ã¾ãŸã¯ç’°å¢ƒå¤‰æ•°ã§è¨­å®šï¼š
```bash
export HF_TOKEN="your_huggingface_token"
```

### 2. ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

#### æ–¹æ³•1: Pythonã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

**å®Ÿè¡Œæ–¹æ³•**:

```bash
# Dockerã‚³ãƒ³ãƒ†ãƒŠã«å…¥ã‚‹
docker exec -it ai-ft-container bash

# /workspaceãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç§»å‹•
cd /workspace

# Pythonã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½œæˆ
cat > download_models.py << 'EOF'
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# 22Bãƒ¢ãƒ‡ãƒ«ã®ä¾‹ï¼ˆCALM3ï¼‰
model_name = "cyberagent/calm3-22b-chat"  # ã¾ãŸã¯ "Qwen/Qwen2.5-17B-Instruct"
print(f"Downloading {model_name}...")

try:
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    print("âœ“ Tokenizer downloaded")
    
    # ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜ï¼‰
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16,
        device_map="cpu"  # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã®ã¿ãªã®ã§CPUæŒ‡å®š
    )
    print("âœ“ Model downloaded successfully!")
except Exception as e:
    print(f"âœ— Error: {e}")
EOF

# ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œ
python3 download_models.py
```

#### æ–¹æ³•2: Hugging Face CLIã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

**å®Ÿè¡Œå ´æ‰€**: Dockerã‚³ãƒ³ãƒ†ãƒŠå†…ã® `/workspace` ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª

```bash
# ã¾ãšã€Dockerã‚³ãƒ³ãƒ†ãƒŠã«å…¥ã‚‹
docker exec -it ai-ft-container bash

# /workspace ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç§»å‹•
cd /workspace

# modelsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆï¼ˆå­˜åœ¨ã—ãªã„å ´åˆï¼‰
mkdir -p models

# 17Bãƒ¢ãƒ‡ãƒ«
huggingface-cli download Qwen/Qwen2.5-17B-Instruct --local-dir ./models/Qwen2.5-17B

# 22Bãƒ¢ãƒ‡ãƒ«ï¼ˆCALM3ï¼‰ ðŸ†•
huggingface-cli download cyberagent/calm3-22b-chat --local-dir ./models/calm3-22b

# 32Bãƒ¢ãƒ‡ãƒ«
huggingface-cli download cyberagent/calm3-DeepSeek-R1-Distill-Qwen-32B --local-dir ./models/calm3-32B

# 70Bãƒ¢ãƒ‡ãƒ«ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
huggingface-cli download meta-llama/Llama-3.1-70B-Instruct --local-dir ./models/Llama-3.1-70B
```

**æ³¨æ„**: 
- ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ¢ãƒ‡ãƒ«ã¯è‡ªå‹•çš„ã«Hugging Faceã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚‚ä¿å­˜ã•ã‚Œã¾ã™
- `--local-dir`ã§æŒ‡å®šã—ãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¯å‚ç…§ç”¨ã®ã‚³ãƒ”ãƒ¼ã§ã™
- å®Ÿéš›ã®ä½¿ç”¨æ™‚ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰è‡ªå‹•çš„ã«èª­ã¿è¾¼ã¾ã‚Œã¾ã™

### 3. ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ã®ç¢ºèª

å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã«ã¯ååˆ†ãªãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ãŒå¿…è¦ã§ã™ï¼š

- 17Bãƒ¢ãƒ‡ãƒ«: ç´„35GB
- 22Bãƒ¢ãƒ‡ãƒ«: ç´„44GB
- 32Bãƒ¢ãƒ‡ãƒ«: ç´„65GB
- 70Bãƒ¢ãƒ‡ãƒ«: ç´„140GB

```bash
# ç©ºãå®¹é‡ç¢ºèª
df -h /workspace
```

### 4. ãƒ¢ãƒ‡ãƒ«ã®äº‹å‰ãƒ­ãƒ¼ãƒ‰ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

ã‚·ã‚¹ãƒ†ãƒ èµ·å‹•æ™‚ã«ãƒ¢ãƒ‡ãƒ«ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ãƒ­ãƒ¼ãƒ‰ã—ã¦ãŠãã“ã¨ã§ã€åˆå›žä½¿ç”¨æ™‚ã®å¾…ã¡æ™‚é–“ã‚’çŸ­ç¸®ã§ãã¾ã™ï¼š

```python
# preload_models.py
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

models_to_preload = [
    "Qwen/Qwen2.5-17B-Instruct",
    "cyberagent/calm3-DeepSeek-R1-Distill-Qwen-32B"
]

for model_name in models_to_preload:
    print(f"Preloading {model_name}...")
    try:
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="cpu"
        )
        print(f"âœ“ {model_name} preloaded successfully")
    except Exception as e:
        print(f"âœ— Failed to preload {model_name}: {e}")
```

### 5. Webã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®å†èµ·å‹•

ãƒ¢ãƒ‡ãƒ«ãŒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸã‚‰ã€Webã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’å†èµ·å‹•ã—ã¾ã™ï¼š

```bash
# Dockerã‚³ãƒ³ãƒ†ãƒŠã‚’å†èµ·å‹•
docker-compose restart

# ã¾ãŸã¯ã€ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ã¿å†èµ·å‹•
docker exec ai-ft-container supervisorctl restart web
```

## ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### å•é¡Œ: ãƒ¢ãƒ‡ãƒ«ãŒãƒªã‚¹ãƒˆã«è¡¨ç¤ºã•ã‚Œãªã„

1. **ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢**
   ```bash
   docker exec ai-ft-container rm -rf /tmp/cache/*
   ```

2. **ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ­ã‚°ã‚’ç¢ºèª**
   ```bash
   docker exec ai-ft-container tail -f /workspace/logs/web.log
   ```

### å•é¡Œ: ãƒ¡ãƒ¢ãƒªä¸è¶³ã‚¨ãƒ©ãƒ¼

1. **4bité‡å­åŒ–ã‚’ä½¿ç”¨**
   - Webã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã§QLoRAã‚’é¸æŠž
   - ã¾ãŸã¯æ‰‹å‹•ã§`load_in_4bit=True`ã‚’æŒ‡å®š

2. **ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã‚’çŸ­ç¸®**
   - ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®šã§æœ€å¤§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã‚’256ã«è¨­å®š

### å•é¡Œ: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒé…ã„

1. **ãƒŸãƒ©ãƒ¼ã‚µã‚¤ãƒˆã‚’ä½¿ç”¨**
   ```bash
   export HF_ENDPOINT="https://hf-mirror.com"
   ```

2. **éƒ¨åˆ†çš„ãªãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰**
   - å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆsafetensorsãƒ•ã‚¡ã‚¤ãƒ«ãªã©ï¼‰

## æŽ¨å¥¨äº‹é …

1. **åˆå›žã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ™‚**
   - ã¾ãšå°ã•ã„ãƒ¢ãƒ‡ãƒ«ï¼ˆ3Bï¼‰ã§å‹•ä½œç¢ºèª
   - æ®µéšŽçš„ã«å¤§ãã„ãƒ¢ãƒ‡ãƒ«ã‚’è¿½åŠ 

2. **ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ç’°å¢ƒ**
   - ãƒ¢ãƒ‡ãƒ«ã¯äº‹å‰ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»æ¤œè¨¼
   - å°‚ç”¨ã®ãƒ¢ãƒ‡ãƒ«ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚’ç”¨æ„

3. **é–‹ç™ºç’°å¢ƒ**
   - ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãƒªãƒ³ã‚¯ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å…±æœ‰
   ```bash
   ln -s /shared/models ~/.cache/huggingface
   ```

## ã‚µãƒãƒ¼ãƒˆ

å•é¡ŒãŒè§£æ±ºã—ãªã„å ´åˆã¯ã€ä»¥ä¸‹ã®æƒ…å ±ã‚’å«ã‚ã¦ã‚µãƒãƒ¼ãƒˆã«ãŠå•ã„åˆã‚ã›ãã ã•ã„ï¼š

- ä½¿ç”¨ã—ã¦ã„ã‚‹GPUã®ç¨®é¡žã¨ãƒ¡ãƒ¢ãƒªå®¹é‡
- ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å…¨æ–‡
- `/workspace/logs/`å†…ã®ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«