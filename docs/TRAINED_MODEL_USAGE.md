# ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®å®Ÿç”¨ã‚¬ã‚¤ãƒ‰

## ğŸ“‹ ç›®æ¬¡

1. [ä¿å­˜å ´æ‰€ã¨ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ](#ä¿å­˜å ´æ‰€ã¨ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ)
2. [åŸºæœ¬çš„ãªèª­ã¿è¾¼ã¿æ–¹æ³•](#åŸºæœ¬çš„ãªèª­ã¿è¾¼ã¿æ–¹æ³•)
3. [å®Ÿç”¨çš„ãªæ´»ç”¨ä¾‹](#å®Ÿç”¨çš„ãªæ´»ç”¨ä¾‹)
4. [æœ¬ç•ªé‹ç”¨ã§ã®å®Ÿè£…](#æœ¬ç•ªé‹ç”¨ã§ã®å®Ÿè£…)
5. [ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–](#ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–)
6. [ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°](#ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°)

## ğŸ“ ä¿å­˜å ´æ‰€ã¨ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ

### ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª

```bash
# Dockerã‚³ãƒ³ãƒ†ãƒŠå†…ã®ä¿å­˜å ´æ‰€
/workspace/lora_demo_YYYYMMDD_HHMMSS/

# ãƒ›ã‚¹ãƒˆãƒã‚·ãƒ³ã«ã‚³ãƒ”ãƒ¼
docker cp ai-ft-container:/workspace/lora_demo_20250626_074248 ./my_trained_model
```

### ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ

```
lora_demo_20250626_074248/
â”œâ”€â”€ adapter_model.safetensors    # LoRAé‡ã¿ï¼ˆ1.6MBï¼‰â­ æœ€é‡è¦
â”œâ”€â”€ adapter_config.json          # LoRAè¨­å®šï¼ˆ763Bï¼‰
â”œâ”€â”€ tokenizer.json              # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ï¼ˆ3.4MBï¼‰
â”œâ”€â”€ vocab.json                  # èªå½™è¾æ›¸ï¼ˆ780KBï¼‰
â”œâ”€â”€ merges.txt                  # BPEãƒãƒ¼ã‚¸ï¼ˆ446KBï¼‰
â”œâ”€â”€ special_tokens_map.json     # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆ131Bï¼‰
â”œâ”€â”€ tokenizer_config.json       # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼è¨­å®šï¼ˆ507Bï¼‰
â””â”€â”€ README.md                   # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ï¼ˆ5KBï¼‰
```

### é‡è¦ãƒ•ã‚¡ã‚¤ãƒ«ã®èª¬æ˜

| ãƒ•ã‚¡ã‚¤ãƒ« | ç”¨é€” | é‡è¦åº¦ |
|----------|------|--------|
| `adapter_model.safetensors` | **å®Ÿéš›ã®å­¦ç¿’çµæœ** | â­â­â­ |
| `adapter_config.json` | LoRAè¨­å®š | â­â­â­ |
| `tokenizer.json` | ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç† | â­â­ |
| `vocab.json` | èªå½™å¤‰æ› | â­â­ |
| ãã®ä»– | è£œåŠ©ãƒ•ã‚¡ã‚¤ãƒ« | â­ |

## ğŸ”§ åŸºæœ¬çš„ãªèª­ã¿è¾¼ã¿æ–¹æ³•

### æœ€å°é™ã®èª­ã¿è¾¼ã¿ã‚³ãƒ¼ãƒ‰

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

def load_trained_model(adapter_path, base_model_name="distilgpt2"):
    """ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿"""
    
    # ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
    tokenizer = AutoTokenizer.from_pretrained(base_model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_name,
        torch_dtype=torch.float16,
        device_map="auto"  # RTX A5000 x2ã§è‡ªå‹•æœ€é©åŒ–
    )
    
    # LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®èª­ã¿è¾¼ã¿
    model = PeftModel.from_pretrained(base_model, adapter_path)
    
    return model, tokenizer

# ä½¿ç”¨ä¾‹
model, tokenizer = load_trained_model("/path/to/lora_demo_20250626_074248")
```

### ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã®åŸºæœ¬

```python
def generate_text(model, tokenizer, prompt, max_new_tokens=50, temperature=0.7):
    """ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            inputs['input_ids'],
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=temperature,
            pad_token_id=tokenizer.eos_token_id
        )
    
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# ä½¿ç”¨ä¾‹
result = generate_text(model, tokenizer, "Hello, how are you")
print(result)
```

## ğŸš€ å®Ÿç”¨çš„ãªæ´»ç”¨ä¾‹

### 1. å¯¾è©±ã‚·ã‚¹ãƒ†ãƒ 

```python
class ChatBot:
    def __init__(self, adapter_path):
        self.model, self.tokenizer = load_trained_model(adapter_path)
        self.conversation_history = []
    
    def chat(self, user_input):
        # ä¼šè©±å±¥æ­´ã‚’å«ã‚ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
        prompt = self._build_prompt(user_input)
        
        # å¿œç­”ç”Ÿæˆ
        response = generate_text(
            self.model, 
            self.tokenizer, 
            prompt, 
            max_new_tokens=100
        )
        
        # å±¥æ­´æ›´æ–°
        self.conversation_history.append({
            'user': user_input,
            'bot': response
        })
        
        return response
    
    def _build_prompt(self, user_input):
        # ä¼šè©±å±¥æ­´ã‚’è€ƒæ…®ã—ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
        context = "\\n".join([
            f"User: {turn['user']}\\nBot: {turn['bot']}" 
            for turn in self.conversation_history[-3:]  # æœ€æ–°3ã‚¿ãƒ¼ãƒ³
        ])
        return f"{context}\\nUser: {user_input}\\nBot:"

# ä½¿ç”¨ä¾‹
bot = ChatBot("/path/to/lora_demo_20250626_074248")
response = bot.chat("ã“ã‚“ã«ã¡ã¯")
print(response)
```

### 2. æ–‡æ›¸è¦ç´„ã‚·ã‚¹ãƒ†ãƒ 

```python
class DocumentSummarizer:
    def __init__(self, adapter_path):
        self.model, self.tokenizer = load_trained_model(adapter_path)
    
    def summarize(self, document, max_summary_length=200):
        """æ–‡æ›¸è¦ç´„"""
        prompt = f"ä»¥ä¸‹ã®æ–‡æ›¸ã‚’è¦ç´„ã—ã¦ãã ã•ã„:\\n\\n{document}\\n\\nè¦ç´„:"
        
        # é•·ã„æ–‡æ›¸ã®å ´åˆã¯åˆ†å‰²å‡¦ç†
        if len(document) > 1000:
            return self._summarize_long_document(document, max_summary_length)
        
        return generate_text(
            self.model,
            self.tokenizer,
            prompt,
            max_new_tokens=max_summary_length
        )
    
    def _summarize_long_document(self, document, max_length):
        # é•·ã„æ–‡æ›¸ã‚’åˆ†å‰²ã—ã¦è¦ç´„
        chunks = [document[i:i+800] for i in range(0, len(document), 800)]
        summaries = []
        
        for chunk in chunks:
            summary = self.summarize(chunk, max_length//len(chunks))
            summaries.append(summary)
        
        # éƒ¨åˆ†è¦ç´„ã‚’çµ±åˆ
        final_prompt = f"ä»¥ä¸‹ã®è¦ç´„ã‚’ã¾ã¨ã‚ã¦ãã ã•ã„:\\n{' '.join(summaries)}\\n\\næœ€çµ‚è¦ç´„:"
        return generate_text(
            self.model,
            self.tokenizer,
            final_prompt,
            max_new_tokens=max_length
        )

# ä½¿ç”¨ä¾‹
summarizer = DocumentSummarizer("/path/to/lora_demo_20250626_074248")
summary = summarizer.summarize("é•·ã„æ–‡æ›¸...")
```

### 3. Q&Aã‚·ã‚¹ãƒ†ãƒ 

```python
class QASystem:
    def __init__(self, adapter_path, knowledge_base):
        self.model, self.tokenizer = load_trained_model(adapter_path)
        self.knowledge_base = knowledge_base  # çŸ¥è­˜ãƒ™ãƒ¼ã‚¹
    
    def answer_question(self, question):
        """è³ªå•å¿œç­”"""
        # é–¢é€£ã™ã‚‹çŸ¥è­˜ã‚’æ¤œç´¢
        relevant_info = self._search_knowledge(question)
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
        prompt = f"""
çŸ¥è­˜ãƒ™ãƒ¼ã‚¹:
{relevant_info}

è³ªå•: {question}
å›ç­”:"""
        
        return generate_text(
            self.model,
            self.tokenizer,
            prompt,
            max_new_tokens=150
        )
    
    def _search_knowledge(self, question):
        # ç°¡å˜ãªçŸ¥è­˜æ¤œç´¢ï¼ˆå®Ÿéš›ã¯ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ç­‰ã‚’ä½¿ç”¨ï¼‰
        relevant = []
        for item in self.knowledge_base:
            if any(keyword in item.lower() for keyword in question.lower().split()):
                relevant.append(item)
        return "\\n".join(relevant[:3])  # ä¸Šä½3ä»¶

# ä½¿ç”¨ä¾‹
knowledge = [
    "æ±äº¬ã¯æ—¥æœ¬ã®é¦–éƒ½ã§ã™ã€‚",
    "å¯Œå£«å±±ã¯æ—¥æœ¬ã§æœ€ã‚‚é«˜ã„å±±ã§ã™ã€‚",
    "æ¡œã¯æ—¥æœ¬ã®å›½èŠ±ã§ã™ã€‚"
]

qa_system = QASystem("/path/to/lora_demo_20250626_074248", knowledge)
answer = qa_system.answer_question("æ—¥æœ¬ã®é¦–éƒ½ã¯ã©ã“ã§ã™ã‹ï¼Ÿ")
```

## ğŸŒ æœ¬ç•ªé‹ç”¨ã§ã®å®Ÿè£…

### 1. Flask WebAPIã‚µãƒ¼ãƒãƒ¼

```python
from flask import Flask, request, jsonify
import threading
import time

app = Flask(__name__)

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¢ãƒ‡ãƒ«ï¼ˆèµ·å‹•æ™‚ã«1å›èª­ã¿è¾¼ã¿ï¼‰
model = None
tokenizer = None
model_lock = threading.Lock()

def initialize_model():
    """ã‚µãƒ¼ãƒãƒ¼èµ·å‹•æ™‚ã«ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–"""
    global model, tokenizer
    print("Loading model...")
    model, tokenizer = load_trained_model("/path/to/lora_demo_20250626_074248")
    print("Model loaded successfully!")

@app.route('/health', methods=['GET'])
def health_check():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
    return jsonify({
        'status': 'healthy',
        'model_loaded': model is not None,
        'timestamp': time.time()
    })

@app.route('/generate', methods=['POST'])
def generate_text_api():
    """ãƒ†ã‚­ã‚¹ãƒˆç”ŸæˆAPI"""
    try:
        data = request.get_json()
        prompt = data.get('prompt', '')
        max_tokens = data.get('max_tokens', 50)
        temperature = data.get('temperature', 0.7)
        
        if not prompt:
            return jsonify({'error': 'Prompt is required'}), 400
        
        # ã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ãªç”Ÿæˆ
        with model_lock:
            result = generate_text(
                model, 
                tokenizer, 
                prompt, 
                max_new_tokens=max_tokens,
                temperature=temperature
            )
        
        return jsonify({
            'result': result,
            'prompt': prompt,
            'timestamp': time.time()
        })
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/chat', methods=['POST'])
def chat_api():
    """ãƒãƒ£ãƒƒãƒˆAPI"""
    try:
        data = request.get_json()
        message = data.get('message', '')
        session_id = data.get('session_id', 'default')
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†ï¼ˆå®Ÿéš›ã¯Redisç­‰ã‚’ä½¿ç”¨ï¼‰
        chat_prompt = f"User: {message}\\nBot:"
        
        with model_lock:
            response = generate_text(
                model, 
                tokenizer, 
                chat_prompt, 
                max_new_tokens=100
            )
        
        # Botã®éƒ¨åˆ†ã®ã¿æŠ½å‡º
        bot_response = response.split("Bot:")[-1].strip()
        
        return jsonify({
            'response': bot_response,
            'session_id': session_id,
            'timestamp': time.time()
        })
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

if __name__ == '__main__':
    initialize_model()
    app.run(host='0.0.0.0', port=5000, threaded=True)
```

### 2. FastAPI ã‚µãƒ¼ãƒãƒ¼ï¼ˆéåŒæœŸå¯¾å¿œï¼‰

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import asyncio
import uvicorn

app = FastAPI(title="Fine-tuned Model API")

# ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«
class GenerateRequest(BaseModel):
    prompt: str
    max_tokens: int = 50
    temperature: float = 0.7

class ChatRequest(BaseModel):
    message: str
    session_id: str = "default"

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¢ãƒ‡ãƒ«
model = None
tokenizer = None

@app.on_event("startup")
async def startup_event():
    """èµ·å‹•æ™‚ã®ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿"""
    global model, tokenizer
    print("Loading model...")
    model, tokenizer = load_trained_model("/path/to/lora_demo_20250626_074248")
    print("Model loaded!")

@app.get("/health")
async def health_check():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
    return {
        "status": "healthy",
        "model_loaded": model is not None
    }

@app.post("/generate")
async def generate_text_async(request: GenerateRequest):
    """éåŒæœŸãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
    if model is None:
        raise HTTPException(status_code=503, detail="Model not loaded")
    
    try:
        # CPUé›†ç´„çš„ã‚¿ã‚¹ã‚¯ã‚’åˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§å®Ÿè¡Œ
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(
            None,
            generate_text,
            model,
            tokenizer,
            request.prompt,
            request.max_tokens,
            request.temperature
        )
        
        return {"result": result, "prompt": request.prompt}
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/chat")
async def chat_async(request: ChatRequest):
    """éåŒæœŸãƒãƒ£ãƒƒãƒˆ"""
    if model is None:
        raise HTTPException(status_code=503, detail="Model not loaded")
    
    try:
        chat_prompt = f"User: {request.message}\\nBot:"
        
        loop = asyncio.get_event_loop()
        response = await loop.run_in_executor(
            None,
            generate_text,
            model,
            tokenizer,
            chat_prompt,
            100,
            0.7
        )
        
        bot_response = response.split("Bot:")[-1].strip()
        
        return {
            "response": bot_response,
            "session_id": request.session_id
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### 3. Dockerã§ã®æœ¬ç•ªãƒ‡ãƒ—ãƒ­ã‚¤

```dockerfile
# Dockerfile
FROM nvidia/cuda:12.1-runtime-ubuntu20.04

# ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
RUN apt-get update && apt-get install -y python3 python3-pip
COPY requirements.txt .
RUN pip3 install -r requirements.txt

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚³ãƒ”ãƒ¼
COPY app/ /app
COPY models/ /models

WORKDIR /app

# ç’°å¢ƒå¤‰æ•°
ENV MODEL_PATH=/models/lora_demo_20250626_074248
ENV PORT=8000

# ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s \\
  CMD curl -f http://localhost:$PORT/health || exit 1

# ã‚µãƒ¼ãƒãƒ¼èµ·å‹•
CMD ["python3", "main.py"]
```

```yaml
# docker-compose.yml
version: '3.8'
services:
  model-api:
    build: .
    ports:
      - "8000:8000"
    volumes:
      - ./models:/models
    environment:
      - MODEL_PATH=/models/lora_demo_20250626_074248
      - CUDA_VISIBLE_DEVICES=0,1
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 2
              capabilities: [gpu]
    restart: unless-stopped
```

## âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

### 1. RTX A5000 x2ã§ã®æœ€é©åŒ–

```python
def load_optimized_model(adapter_path, base_model_name="distilgpt2"):
    """RTX A5000 x2æœ€é©åŒ–æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿"""
    
    # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªè¨­å®š
    tokenizer = AutoTokenizer.from_pretrained(
        base_model_name,
        use_fast=True  # é«˜é€Ÿãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ä½¿ç”¨
    )
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_name,
        torch_dtype=torch.float16,  # FP16ã§é«˜é€ŸåŒ–
        device_map="auto",          # ãƒãƒ«ãƒGPUè‡ªå‹•é…ç½®
        low_cpu_mem_usage=True,     # CPU ãƒ¡ãƒ¢ãƒªç¯€ç´„
        trust_remote_code=True
    )
    
    # LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼èª­ã¿è¾¼ã¿
    model = PeftModel.from_pretrained(
        base_model, 
        adapter_path,
        torch_dtype=torch.float16
    )
    
    # æ¨è«–ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š
    model.eval()
    
    return model, tokenizer
```

### 2. ãƒãƒƒãƒå‡¦ç†æœ€é©åŒ–

```python
def batch_generate(model, tokenizer, prompts, batch_size=4):
    """ãƒãƒƒãƒã§ã®åŠ¹ç‡çš„ãªç”Ÿæˆ"""
    results = []
    
    for i in range(0, len(prompts), batch_size):
        batch_prompts = prompts[i:i+batch_size]
        
        # ãƒãƒƒãƒã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        inputs = tokenizer(
            batch_prompts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=512
        ).to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                inputs['input_ids'],
                attention_mask=inputs['attention_mask'],
                max_new_tokens=50,
                do_sample=True,
                temperature=0.7,
                pad_token_id=tokenizer.eos_token_id,
                num_return_sequences=1
            )
        
        # ãƒ‡ã‚³ãƒ¼ãƒ‰
        batch_results = [
            tokenizer.decode(output, skip_special_tokens=True)
            for output in outputs
        ]
        results.extend(batch_results)
    
    return results
```

### 3. ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½

```python
import functools
import hashlib

@functools.lru_cache(maxsize=1000)
def cached_generate(prompt_hash, max_tokens, temperature):
    """ç”Ÿæˆçµæœã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥"""
    # å®Ÿéš›ã®ç”Ÿæˆå‡¦ç†
    return generate_text(model, tokenizer, prompt, max_tokens, temperature)

def generate_with_cache(prompt, max_tokens=50, temperature=0.7):
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãç”Ÿæˆ"""
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒãƒƒã‚·ãƒ¥åŒ–
    prompt_hash = hashlib.md5(f"{prompt}_{max_tokens}_{temperature}".encode()).hexdigest()
    return cached_generate(prompt_hash, max_tokens, temperature)
```

## ğŸ”§ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ã‚ˆãã‚ã‚‹å•é¡Œã¨è§£æ±ºç­–

#### 1. CUDA Out of Memory

```python
# è§£æ±ºç­–: ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å°ã•ãã™ã‚‹
def handle_memory_error():
    try:
        result = model.generate(...)
    except torch.cuda.OutOfMemoryError:
        torch.cuda.empty_cache()  # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
        # ã‚ˆã‚Šå°ã•ãªãƒãƒƒãƒã‚µã‚¤ã‚ºã§å†è©¦è¡Œ
        result = model.generate(..., batch_size=1)
    
    return result
```

#### 2. ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼

```python
def robust_model_loading(adapter_path, max_retries=3):
    """ãƒ­ãƒã‚¹ãƒˆãªãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿"""
    for attempt in range(max_retries):
        try:
            model, tokenizer = load_trained_model(adapter_path)
            return model, tokenizer
        except Exception as e:
            print(f"Attempt {attempt + 1} failed: {e}")
            if attempt == max_retries - 1:
                raise
            time.sleep(5)  # 5ç§’å¾…æ©Ÿ
```

#### 3. æ¨è«–é€Ÿåº¦ã®å•é¡Œ

```python
# è§£æ±ºç­–: ãƒ¢ãƒ‡ãƒ«ã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ï¼ˆPyTorch 2.0+ï¼‰
def optimize_model_for_inference(model):
    """æ¨è«–ç”¨ãƒ¢ãƒ‡ãƒ«æœ€é©åŒ–"""
    # TorchScriptå¤‰æ›
    try:
        model = torch.jit.script(model)
    except:
        print("TorchScript conversion failed, using original model")
    
    # PyTorch 2.0ã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
    try:
        model = torch.compile(model)
    except:
        print("Torch compile not available")
    
    return model
```

### ãƒ­ã‚°ã¨ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°

```python
import logging
import time
import psutil
import torch

def setup_logging():
    """ãƒ­ã‚°è¨­å®š"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('model_api.log'),
            logging.StreamHandler()
        ]
    )

def monitor_performance(func):
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿"""
    def wrapper(*args, **kwargs):
        start_time = time.time()
        start_memory = torch.cuda.memory_allocated()
        
        result = func(*args, **kwargs)
        
        end_time = time.time()
        end_memory = torch.cuda.memory_allocated()
        
        logger.info(f"Function: {func.__name__}")
        logger.info(f"Execution time: {end_time - start_time:.2f}s")
        logger.info(f"Memory usage: {(end_memory - start_memory) / 1024**2:.1f}MB")
        
        return result
    return wrapper

# ä½¿ç”¨ä¾‹
@monitor_performance
def generate_text_monitored(model, tokenizer, prompt):
    return generate_text(model, tokenizer, prompt)
```

## ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™

### RTX A5000 x2ç’°å¢ƒã§ã®å®Ÿæ¸¬å€¤

| é …ç›® | å€¤ |
|------|---|
| ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º | 6.2MBï¼ˆå…¨ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰ |
| LoRAé‡ã¿ | 1.6MB |
| æ¨è«–ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ | 2-4GB |
| å˜ä¸€ç”Ÿæˆé€Ÿåº¦ | 0.1-0.5ç§’/50ãƒˆãƒ¼ã‚¯ãƒ³ |
| ãƒãƒƒãƒç”Ÿæˆé€Ÿåº¦ | 1-3ç§’/4ãƒãƒƒãƒ |
| èµ·å‹•æ™‚é–“ | 5-10ç§’ |

### æ¨å¥¨è¨­å®š

| ç”¨é€” | batch_size | max_tokens | temperature |
|------|------------|------------|-------------|
| ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒãƒ£ãƒƒãƒˆ | 1 | 30-50 | 0.7-0.9 |
| æ–‡æ›¸è¦ç´„ | 2-4 | 100-200 | 0.5-0.7 |
| ãƒãƒƒãƒå‡¦ç† | 4-8 | 50-100 | 0.6-0.8 |

## ğŸ¯ ã¾ã¨ã‚

ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã¯ä»¥ä¸‹ã®ç‰¹å¾´ã‚’æŒã¡ã¾ã™ï¼š

### ğŸš€ **å„ªä½æ€§**
- **è»½é‡**: LoRAé‡ã¿ã¯1.6MBã®ã¿
- **é«˜é€Ÿ**: RTX A5000 x2ã§æœ€é©åŒ–æ¸ˆã¿
- **æŸ”è»Ÿ**: è¤‡æ•°ã®ã‚¿ã‚¹ã‚¯ã«å¯¾å¿œå¯èƒ½
- **ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«**: WebAPIã‚„ãƒã‚¤ã‚¯ãƒ­ã‚µãƒ¼ãƒ“ã‚¹ã«çµ„ã¿è¾¼ã¿å¯èƒ½

### ğŸ’¡ **å®Ÿç”¨åŒ–ã®ãƒã‚¤ãƒ³ãƒˆ**
- ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¯å…±æœ‰ã€LoRAã®ã¿å·®ã—æ›¿ãˆ
- ãƒãƒ«ãƒGPUç’°å¢ƒã§ã®è‡ªå‹•æœ€é©åŒ–
- ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¨ãƒãƒƒãƒå‡¦ç†ã«ã‚ˆã‚‹é«˜é€ŸåŒ–
- æœ¬ç•ªé‹ç”¨ã§ã®ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã¨ãƒ­ã‚°

### ğŸ¯ **æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—**
- ã‚ˆã‚Šå¤§ããªãƒ¢ãƒ‡ãƒ«ï¼ˆ8B-13Bï¼‰ã§ã®å­¦ç¿’
- ç‰¹å®šãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã§ã®å°‚é–€åŒ–
- è¤‡æ•°ã®LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®ç®¡ç†
- A/Bãƒ†ã‚¹ãƒˆã«ã‚ˆã‚‹å“è³ªè©•ä¾¡

ã“ã®å®Ÿç”¨ã‚¬ã‚¤ãƒ‰ã‚’å‚è€ƒã«ã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’åŠ¹æœçš„ã«æ´»ç”¨ã—ã¦ãã ã•ã„ã€‚