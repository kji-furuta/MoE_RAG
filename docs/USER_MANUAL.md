# ğŸ“– AI Fine-tuning Toolkit ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒãƒ‹ãƒ¥ã‚¢ãƒ«

## ğŸ“‹ ç›®æ¬¡

1. [ã¯ã˜ã‚ã«](#ã¯ã˜ã‚ã«)
2. [ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶](#ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶)
3. [ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨åˆæœŸè¨­å®š](#ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨åˆæœŸè¨­å®š)
4. [Webã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®åŸºæœ¬æ“ä½œ](#webã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®åŸºæœ¬æ“ä½œ)
5. [ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Œå…¨ã‚¬ã‚¤ãƒ‰](#ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Œå…¨ã‚¬ã‚¤ãƒ‰)
6. [ãƒ¢ãƒ‡ãƒ«ã®åˆ©ç”¨æ–¹æ³•](#ãƒ¢ãƒ‡ãƒ«ã®åˆ©ç”¨æ–¹æ³•)
7. [å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ï¼ˆ32Bï¼‰ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°](#å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«32bã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°)
8. [ãƒ‡ãƒ¼ã‚¿ç®¡ç†](#ãƒ‡ãƒ¼ã‚¿ç®¡ç†)
9. [ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°](#ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°)
10. [FAQ](#faq)
11. [é«˜åº¦ãªä½¿ç”¨æ–¹æ³•](#é«˜åº¦ãªä½¿ç”¨æ–¹æ³•)

---

## ğŸ¯ ã¯ã˜ã‚ã«

AI Fine-tuning Toolkitã¯ã€æ—¥æœ¬èªå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’åŠ¹ç‡çš„ã«è¡Œã†ãŸã‚ã®åŒ…æ‹¬çš„ãªã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã§ã™ã€‚ã“ã®ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã§ã¯ã€åˆå¿ƒè€…ã‹ã‚‰ä¸Šç´šè€…ã¾ã§ã€ã‚·ã‚¹ãƒ†ãƒ ã‚’åŠ¹æœçš„ã«æ´»ç”¨ã™ã‚‹ãŸã‚ã®è©³ç´°ãªæ‰‹é †ã‚’èª¬æ˜ã—ã¾ã™ã€‚

### âœ¨ ä¸»ãªç‰¹å¾´
- **ç°¡å˜æ“ä½œ**: Webãƒ–ãƒ©ã‚¦ã‚¶ã‹ã‚‰ç›´æ„Ÿçš„ã«æ“ä½œ
- **é«˜æ€§èƒ½**: GPUæœ€é©åŒ–ã¨ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–
- **å¤šæ§˜ãªæ‰‹æ³•**: ãƒ•ãƒ«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã€LoRAã€QLoRAå¯¾å¿œ
- **æ—¥æœ¬èªç‰¹åŒ–**: æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã«æœ€é©åŒ–ã•ã‚ŒãŸè¨­å®š

---

## ğŸ’» ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶

### ğŸ–¥ï¸ ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢è¦ä»¶

#### æœ€å°è¦ä»¶
- **CPU**: 4ã‚³ã‚¢ä»¥ä¸Š
- **ãƒ¡ãƒ¢ãƒª**: 16GBä»¥ä¸Š
- **GPU**: NVIDIA GPU 8GB VRAMä»¥ä¸Š
- **ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸**: 50GBä»¥ä¸Šã®ç©ºãå®¹é‡

#### æ¨å¥¨è¦ä»¶
- **CPU**: 8ã‚³ã‚¢ä»¥ä¸Š
- **ãƒ¡ãƒ¢ãƒª**: 32GBä»¥ä¸Š
- **GPU**: NVIDIA RTX A5000 (24GB) x2 ã¾ãŸã¯åŒç­‰
- **ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸**: 100GBä»¥ä¸Šã®SSD

### ğŸ”§ ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢è¦ä»¶
- **OS**: Ubuntu 18.04+ / Windows 10+ (WSL2) / macOS 10.15+
- **Docker**: 20.10+
- **Docker Compose**: 1.28+
- **NVIDIA Docker**: 2.0+ (GPUä½¿ç”¨æ™‚)
- **CUDA**: 11.8+

---

## ğŸš€ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨åˆæœŸè¨­å®š

### ã‚¹ãƒ†ãƒƒãƒ— 1: ãƒªãƒã‚¸ãƒˆãƒªã®ã‚¯ãƒ­ãƒ¼ãƒ³

```bash
# GitHubã‹ã‚‰ã‚¯ãƒ­ãƒ¼ãƒ³
git clone https://github.com/kji-furuta/AI_FT.git
cd AI_FT
```

### ã‚¹ãƒ†ãƒƒãƒ— 2: Dockerç’°å¢ƒã®æ§‹ç¯‰

```bash
# Dockerã‚¤ãƒ¡ãƒ¼ã‚¸ã®ãƒ“ãƒ«ãƒ‰ã¨èµ·å‹•
cd docker
docker-compose up -d

# èµ·å‹•ç¢ºèª
docker ps
```

### ã‚¹ãƒ†ãƒƒãƒ— 3: GPUç¢ºèª

```bash
# ã‚³ãƒ³ãƒ†ãƒŠã«å…¥ã‚‹
docker exec -it ai-ft-container bash

# GPUçŠ¶æ³ç¢ºèª
nvidia-smi

# PyTorchã§ã®GPUç¢ºèª
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}'); print(f'GPU count: {torch.cuda.device_count()}')"
```

### ã‚¹ãƒ†ãƒƒãƒ— 4: Webã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®èµ·å‹•

```bash
# Webã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹•
docker exec ai-ft-container bash /workspace/scripts/start_web_interface.sh
```

âœ… **ç¢ºèª**: ãƒ–ãƒ©ã‚¦ã‚¶ã§ `http://localhost:8050` ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãŒè¡¨ç¤ºã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª

---

## ğŸŒ Webã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®åŸºæœ¬æ“ä½œ

### ğŸ  ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç”»é¢

ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã§ã¯ä»¥ä¸‹ã®æƒ…å ±ã‚’ç¢ºèªã§ãã¾ã™ï¼š

1. **ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ³**
   - CPUä½¿ç”¨ç‡
   - ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
   - GPUæƒ…å ±ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰

2. **å®Ÿè¡Œä¸­ã®ã‚¿ã‚¹ã‚¯**
   - ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®é€²æ—
   - å®Œäº†ã—ãŸã‚¿ã‚¹ã‚¯ã®å±¥æ­´

3. **ã‚¯ã‚¤ãƒƒã‚¯ã‚¢ã‚¯ã‚»ã‚¹**
   - ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
   - ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ
   - ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ

### ğŸ“Š ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³

ç”»é¢ä¸Šéƒ¨ã®ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ãƒãƒ¼ã‹ã‚‰å„æ©Ÿèƒ½ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã™ï¼š

- **ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰**: ã‚·ã‚¹ãƒ†ãƒ æ¦‚è¦
- **ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°**: ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
- **ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ**: ãƒ¢ãƒ‡ãƒ«åˆ©ç”¨
- **ãƒ¢ãƒ‡ãƒ«ç®¡ç†**: ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ãƒ»ç®¡ç†

---

## ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Œå…¨ã‚¬ã‚¤ãƒ‰

### ğŸ“š äº‹å‰æº–å‚™: ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ

#### ãƒ‡ãƒ¼ã‚¿å½¢å¼
ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã¯ä»¥ä¸‹ã®å½¢å¼ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™ï¼š

```json
{"text": "è³ªå•: æ—¥æœ¬ã®é¦–éƒ½ã¯ã©ã“ã§ã™ã‹ï¼Ÿ\nå›ç­”: æ—¥æœ¬ã®é¦–éƒ½ã¯æ±äº¬ã§ã™ã€‚"}
{"text": "è³ªå•: å¯Œå£«å±±ã®é«˜ã•ã¯ä½•ãƒ¡ãƒ¼ãƒˆãƒ«ã§ã™ã‹ï¼Ÿ\nå›ç­”: å¯Œå£«å±±ã®é«˜ã•ã¯3,776ãƒ¡ãƒ¼ãƒˆãƒ«ã§ã™ã€‚"}
```

#### ãƒ‡ãƒ¼ã‚¿ä½œæˆã®ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³

1. **ä¸€è²«æ€§**: åŒã˜å½¢å¼ã§è³ªå•ã¨å›ç­”ã‚’è¨˜è¿°
2. **å“è³ª**: æ­£ç¢ºã§è‡ªç„¶ãªæ—¥æœ¬èªã‚’ä½¿ç”¨
3. **å¤šæ§˜æ€§**: æ§˜ã€…ãªãƒˆãƒ”ãƒƒã‚¯ã‚„è¡¨ç¾ã‚’å«ã‚ã‚‹
4. **é‡**: æœ€ä½100ä»¶ã€æ¨å¥¨1000ä»¶ä»¥ä¸Š

#### ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®æ´»ç”¨
ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«ã¯ä»¥ä¸‹ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ï¼š
- `data/raw/sample_training_data.jsonl`: åŸºæœ¬çš„ãªQ&A
- `data/raw/sample_chat_data.jsonl`: ä¼šè©±å½¢å¼
- `data/raw/sample_code_data.jsonl`: ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°é–¢é€£

### ğŸ”„ ã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—: ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ

#### ã‚¹ãƒ†ãƒƒãƒ— 1: ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰

1. **ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°**ã‚¿ãƒ–ã‚’ã‚¯ãƒªãƒƒã‚¯
2. **ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰**ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã€Œãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã€
3. JSONLå½¢å¼ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ
4. **ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰**ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯

âœ… **ç¢ºèª**: ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ãŒè¡¨ç¤ºã•ã‚Œã‚‹

#### ã‚¹ãƒ†ãƒƒãƒ— 2: ãƒ¢ãƒ‡ãƒ«è¨­å®š

1. **ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«**ã‚’é¸æŠï¼š
   - `distilgpt2`: è»½é‡ãƒ»ãƒ†ã‚¹ãƒˆç”¨ï¼ˆæ¨å¥¨ï¼šåˆå›ï¼‰
   - `japanese-stablelm-3b`: æ—¥æœ¬èª3Bãƒ¢ãƒ‡ãƒ«
   - `elyza-japanese-llama-7b`: é«˜æ€§èƒ½7Bãƒ¢ãƒ‡ãƒ«

2. **LoRAè¨­å®š**ï¼š
   - **LoRA Rank**: 16ï¼ˆæ¨™æº–ï¼‰
   - å°ã•ã„å€¤ï¼ˆ8ï¼‰ï¼šè»½é‡ãƒ»é«˜é€Ÿ
   - å¤§ãã„å€¤ï¼ˆ32ï¼‰ï¼šé«˜ç²¾åº¦ãƒ»é‡ã„

3. **å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**ï¼š
   - **å­¦ç¿’ç‡**: 0.0003ï¼ˆæ¨™æº–ï¼‰
   - **ãƒãƒƒãƒã‚µã‚¤ã‚º**: 4ï¼ˆGPU 8GBæ™‚ï¼‰
   - **ã‚¨ãƒãƒƒã‚¯æ•°**: 3ï¼ˆæ¨™æº–ï¼‰

#### ã‚¹ãƒ†ãƒƒãƒ— 3: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ

1. **ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹**ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯
2. é€²æ—ãƒãƒ¼ã§å­¦ç¿’çŠ¶æ³ã‚’ç¢ºèª
3. å®Œäº†ã¾ã§å¾…æ©Ÿï¼ˆé€šå¸¸10-30åˆ†ï¼‰

âœ… **ç¢ºèª**: ã€Œãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†ã€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒè¡¨ç¤ºã•ã‚Œã‚‹

### âš™ï¸ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ã‚¬ã‚¤ãƒ‰

#### GPU ãƒ¡ãƒ¢ãƒªåˆ¥æ¨å¥¨è¨­å®š

| GPU VRAM | ãƒ¢ãƒ‡ãƒ« | ãƒãƒƒãƒã‚µã‚¤ã‚º | LoRA Rank |
|----------|--------|-------------|-----------|
| 8GB | distilgpt2 | 8 | 16 |
| 16GB | japanese-stablelm-3b | 4 | 16 |
| 24GB | elyza-japanese-llama-7b | 2 | 32 |

#### å­¦ç¿’æ™‚é–“ã®ç›®å®‰

| ãƒ‡ãƒ¼ã‚¿é‡ | ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º | äºˆæƒ³æ™‚é–“ |
|----------|-------------|----------|
| 100ä»¶ | 3B | 5-10åˆ† |
| 1,000ä»¶ | 3B | 15-30åˆ† |
| 5,000ä»¶ | 7B | 1-2æ™‚é–“ |

---

## ğŸ¤– ãƒ¢ãƒ‡ãƒ«ã®åˆ©ç”¨æ–¹æ³•

### ğŸ“ åŸºæœ¬çš„ãªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ

#### ã‚¹ãƒ†ãƒƒãƒ— 1: ãƒ¢ãƒ‡ãƒ«é¸æŠ

1. **ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ**ã‚¿ãƒ–ã‚’ã‚¯ãƒªãƒƒã‚¯
2. **ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«**ã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ
3. ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª¿æ•´ï¼š
   - **Temperature**: 0.7ï¼ˆæ¨™æº–ï¼‰ã€ä½ã„å€¤=ç¢ºå®Ÿã€é«˜ã„å€¤=å‰µé€ çš„
   - **æœ€å¤§é•·**: 100ï¼ˆç”Ÿæˆã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼‰

#### ã‚¹ãƒ†ãƒƒãƒ— 2: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå…¥åŠ›ã¨ç”Ÿæˆ

1. **ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ**æ¬„ã«ç”Ÿæˆã—ãŸã„ãƒ†ã‚­ã‚¹ãƒˆã®å§‹ã¾ã‚Šã‚’å…¥åŠ›
2. **ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ**ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯
3. çµæœã‚’ç¢ºèª

### ğŸ’¡ åŠ¹æœçš„ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ›¸ãæ–¹

#### è‰¯ã„ä¾‹
```
è³ªå•: æ©Ÿæ¢°å­¦ç¿’ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ
å›ç­”: 
```

```
ä»¥ä¸‹ã®æ–‡ç« ã‚’è¦ç´„ã—ã¦ãã ã•ã„ï¼š
[æ–‡ç« å†…å®¹]
è¦ç´„: 
```

#### é¿ã‘ã‚‹ã¹ãä¾‹
- æ›–æ˜§ã™ãã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼šã€Œä½•ã‹æ•™ãˆã¦ã€
- é•·ã™ãã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼š500æ–‡å­—ä»¥ä¸Š
- æ–‡è„ˆã®ãªã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼šã€Œãã‚Œã¯ã€

### ğŸ”§ ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è©³ç´°

| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | èª¬æ˜ | æ¨å¥¨å€¤ |
|-----------|------|--------|
| **Temperature** | ãƒ©ãƒ³ãƒ€ãƒ æ€§ã®åˆ¶å¾¡ | 0.7-0.9 |
| **Top-p** | èªå½™ã®çµã‚Šè¾¼ã¿ | 0.9 |
| **Max Length** | æœ€å¤§ç”Ÿæˆé•· | 50-200 |
| **Repetition Penalty** | ç¹°ã‚Šè¿”ã—æŠ‘åˆ¶ | 1.1 |

---

## ğŸ“ ãƒ‡ãƒ¼ã‚¿ç®¡ç†

### ğŸ“¤ ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ–¹æ³•

#### æ–¹æ³•1: Webã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ï¼ˆæ¨å¥¨ï¼‰
1. ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒšãƒ¼ã‚¸ã§ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠ
2. JSONLå½¢å¼ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‰ãƒ©ãƒƒã‚°&ãƒ‰ãƒ­ãƒƒãƒ—ã¾ãŸã¯é¸æŠ
3. è‡ªå‹•æ¤œè¨¼ã«ã‚ˆã‚Šå³åº§ã«çµæœã‚’ç¢ºèª

#### æ–¹æ³•2: ç›´æ¥ã‚³ãƒ”ãƒ¼
```bash
# ãƒ›ã‚¹ãƒˆã‹ã‚‰ã‚³ãƒ³ãƒ†ãƒŠã¸ãƒ•ã‚¡ã‚¤ãƒ«ã‚³ãƒ”ãƒ¼
docker cp your_data.jsonl ai-ft-container:/workspace/data/raw/
```

#### æ–¹æ³•3: URL ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
```bash
# ã‚³ãƒ³ãƒ†ãƒŠå†…ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
docker exec ai-ft-container wget -O /workspace/data/raw/remote_data.jsonl https://example.com/data.jsonl
```

### ğŸ” ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼

ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã¯è‡ªå‹•çš„ã«æ¤œè¨¼ã•ã‚Œã¾ã™ï¼š

âœ… **æ¤œè¨¼é …ç›®**:
- JSONå½¢å¼ã®æ­£ç¢ºæ€§
- å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®å­˜åœ¨
- æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
- ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º

âŒ **ã‚ˆãã‚ã‚‹ã‚¨ãƒ©ãƒ¼**:
- JSONæ§‹æ–‡ã‚¨ãƒ©ãƒ¼
- æ”¹è¡ŒåŒºåˆ‡ã‚Šã§ãªã„
- UTF-8ä»¥å¤–ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°

### ğŸ“Š ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†

å¤§é‡ãƒ‡ãƒ¼ã‚¿ã‚„è¤‡é›‘ãªå½¢å¼ã®å ´åˆã¯å‰å‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½¿ç”¨ï¼š

```bash
# ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†
python scripts/prepare_training_data.py \
  --input data/raw/your_data.jsonl \
  --output data/processed/training_data.jsonl \
  --format text \
  --validate
```

---

## ğŸ”§ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### â— ã‚ˆãã‚ã‚‹å•é¡Œã¨è§£æ±ºæ–¹æ³•

#### å•é¡Œ1: GPU ãƒ¡ãƒ¢ãƒªä¸è¶³
**ç—‡çŠ¶**: "CUDA out of memory" ã‚¨ãƒ©ãƒ¼

**è§£æ±ºæ–¹æ³•**:
1. ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’æ¸›ã‚‰ã™ï¼ˆ4â†’2â†’1ï¼‰
2. LoRA Rankã‚’ä¸‹ã’ã‚‹ï¼ˆ16â†’8ï¼‰
3. QLoRAã‚’æœ‰åŠ¹ã«ã™ã‚‹
4. Gradient Checkpointingã‚’æœ‰åŠ¹ã«ã™ã‚‹

```python
# ãƒ¡ãƒ¢ãƒªä¸è¶³æ™‚ã®è¨­å®šä¾‹
training_config = {
    "batch_size": 1,
    "gradient_accumulation_steps": 16,
    "gradient_checkpointing": True,
    "fp16": True
}
```

#### å•é¡Œ2: Webã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ããªã„
**ç—‡çŠ¶**: ãƒ–ãƒ©ã‚¦ã‚¶ã§ç”»é¢ãŒè¡¨ç¤ºã•ã‚Œãªã„

**è§£æ±ºæ–¹æ³•**:
1. ãƒãƒ¼ãƒˆ8000ãŒä½¿ç”¨ä¸­ã§ãªã„ã‹ç¢ºèª
```bash
netstat -tulpn | grep :8000
```

2. Dockerã‚³ãƒ³ãƒ†ãƒŠãŒèµ·å‹•ã—ã¦ã„ã‚‹ã‹ç¢ºèª
```bash
docker ps | grep ai-ft-container
```

3. ãƒ•ã‚¡ã‚¤ã‚¢ã‚¦ã‚©ãƒ¼ãƒ«è¨­å®šã‚’ç¢ºèª

#### å•é¡Œ3: ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼
**ç—‡çŠ¶**: ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„

**è§£æ±ºæ–¹æ³•**:
1. ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã‚’ç¢ºèª
```bash
docker exec ai-ft-container ls -la /workspace/lora_demo_*
```

2. Hugging Faceèªè¨¼ã‚’ç¢ºèª
```bash
docker exec ai-ft-container python -c "from huggingface_hub import whoami; print(whoami())"
```

#### å•é¡Œ4: å­¦ç¿’ãŒé€²ã¾ãªã„
**ç—‡çŠ¶**: é€²æ—ãŒ0%ã®ã¾ã¾

**è§£æ±ºæ–¹æ³•**:
1. ãƒ­ã‚°ã‚’ç¢ºèª
```bash
docker exec ai-ft-container tail -f /workspace/logs/training.log
```

2. ãƒ—ãƒ­ã‚»ã‚¹ãŒå®Ÿè¡Œä¸­ã‹ç¢ºèª
```bash
docker exec ai-ft-container ps aux | grep python
```

### ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

#### GPUä½¿ç”¨ç‡å‘ä¸Š
1. **Mixed Precision**: FP16ä½¿ç”¨
2. **Flash Attention**: é«˜é€Ÿæ³¨æ„æ©Ÿæ§‹
3. **Gradient Accumulation**: å¤§ããªå®ŸåŠ¹ãƒãƒƒãƒã‚µã‚¤ã‚º

#### ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å‰Šæ¸›
1. **QLoRA**: 4bité‡å­åŒ–
2. **Gradient Checkpointing**: ãƒ¡ãƒ¢ãƒª/è¨ˆç®—ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•
3. **é©åˆ‡ãªãƒãƒƒãƒã‚µã‚¤ã‚º**: GPU VRAMã«åˆã‚ã›ã¦èª¿æ•´

---

## â“ FAQ

### ğŸ¤” ä¸€èˆ¬çš„ãªè³ªå•

**Q: ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã¯ã©ã®ãã‚‰ã„ã®æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ã‹ï¼Ÿ**
A: ãƒ‡ãƒ¼ã‚¿é‡ã¨ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã«ã‚ˆã‚Šç•°ãªã‚Šã¾ã™ï¼š
- 100ä»¶ã€3Bãƒ¢ãƒ‡ãƒ«: 5-10åˆ†
- 1,000ä»¶ã€3Bãƒ¢ãƒ‡ãƒ«: 15-30åˆ†
- 5,000ä»¶ã€7Bãƒ¢ãƒ‡ãƒ«: 1-2æ™‚é–“

**Q: ã©ã®ãã‚‰ã„ã®ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ã§ã™ã‹ï¼Ÿ**
A: ç›®çš„ã«ã‚ˆã£ã¦ç•°ãªã‚Šã¾ã™ï¼š
- ãƒ†ã‚¹ãƒˆ/å­¦ç¿’: 50-100ä»¶
- å®Ÿç”¨ãƒ¬ãƒ™ãƒ«: 500-1,000ä»¶
- é«˜å“è³ª: 5,000ä»¶ä»¥ä¸Š

**Q: è¤‡æ•°ã®GPUã‚’ä½¿ç”¨ã§ãã¾ã™ã‹ï¼Ÿ**
A: ã¯ã„ã€‚ã‚·ã‚¹ãƒ†ãƒ ã¯è‡ªå‹•çš„ã«Multi-GPUã‚’æ¤œå‡ºã—ã¦ä½¿ç”¨ã—ã¾ã™ã€‚

**Q: å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã¯ã©ã“ã«ä¿å­˜ã•ã‚Œã¾ã™ã‹ï¼Ÿ**
A: `/workspace/lora_demo_YYYYMMDD_HHMMSS/` å½¢å¼ã§ä¿å­˜ã•ã‚Œã¾ã™ã€‚

### ğŸ”§ æŠ€è¡“çš„ãªè³ªå•

**Q: ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«ã‚’è¿½åŠ ã§ãã¾ã™ã‹ï¼Ÿ**
A: ã¯ã„ã€‚`src/models/japanese_model.py`ã‚’ä¿®æ­£ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’è¿½åŠ ã§ãã¾ã™ã€‚

**Q: APIçµŒç”±ã§ã®æ“ä½œã¯å¯èƒ½ã§ã™ã‹ï¼Ÿ**
A: ã¯ã„ã€‚Webã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã¨åŒã˜APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’ç›´æ¥å‘¼ã³å‡ºã›ã¾ã™ã€‚

**Q: åˆ†æ•£å­¦ç¿’ã¯å¯¾å¿œã—ã¦ã„ã¾ã™ã‹ï¼Ÿ**
A: Multi-GPUã«ã‚ˆã‚‹ä¸¦åˆ—åŒ–ã«å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚

### ğŸ› ã‚¨ãƒ©ãƒ¼é–¢é€£

**Q: "No module named 'peft'" ã‚¨ãƒ©ãƒ¼ãŒå‡ºã¾ã™**
A: PEFTãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒä¸è¶³ã—ã¦ã„ã¾ã™ï¼š
```bash
docker exec ai-ft-container pip install peft
```

**Q: "HTTP 500 Internal Server Error" ãŒå‡ºã¾ã™**
A: ã‚µãƒ¼ãƒãƒ¼ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼š
```bash
docker exec ai-ft-container tail -f /workspace/logs/web.log
```

---

## ğŸ”¥ å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ï¼ˆ32Bï¼‰ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

### ğŸ“Š æ¦‚è¦

æœ¬ã‚·ã‚¹ãƒ†ãƒ ã¯ã€æœ€å¤§32Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚cyberagent/calm3-DeepSeek-R1-Distill-Qwen-32Bãªã©ã®æœ€å…ˆç«¯ãƒ¢ãƒ‡ãƒ«ã‚’ã€é™ã‚‰ã‚ŒãŸGPUãƒªã‚½ãƒ¼ã‚¹ã§åŠ¹ç‡çš„ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã§ãã¾ã™ã€‚

### ğŸ› ï¸ ä¸»ãªæœ€é©åŒ–æŠ€è¡“

#### 1. **4-bité‡å­åŒ–ï¼ˆQLoRAï¼‰**
- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ç´„75%å‰Šæ¸›
- æ€§èƒ½ã‚’ã»ã¼ç¶­æŒã—ãªãŒã‚‰å­¦ç¿’å¯èƒ½
- INT4é‡å­åŒ–ã¨FP16è¨ˆç®—ã®çµ„ã¿åˆã‚ã›

#### 2. **DeepSpeed ZeRO Stage 3**
- ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åˆ†æ•£
- CPU/NVMeã¸ã®è‡ªå‹•ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰
- è¤‡æ•°GPUã§ã®åŠ¹ç‡çš„ãªä¸¦åˆ—åŒ–

#### 3. **ã‚°ãƒ©ãƒ‡ã‚£ã‚¨ãƒ³ãƒˆãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒ†ã‚£ãƒ³ã‚°**
- ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚’å¤§å¹…ã«å‘ä¸Š
- è¨ˆç®—æ™‚é–“ã¨ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’æœ€é©åŒ–

#### 4. **Flash Attention 2**
- é«˜é€Ÿãªã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³è¨ˆç®—
- ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã®æ”¹å–„
- é•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®å‡¦ç†ãŒå¯èƒ½

### ğŸ“ 32Bãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ‰‹é †

#### ã‚¹ãƒ†ãƒƒãƒ—1: ç’°å¢ƒæº–å‚™

```bash
# è¿½åŠ ã®ä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install deepspeed>=0.12.0 flash-attn>=2.0.0 optimum>=1.13.0

# DeepSpeedè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ
python3 scripts/generate_deepspeed_configs.py
```

#### ã‚¹ãƒ†ãƒƒãƒ—2: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™

```bash
# JSONLå½¢å¼ã§ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
# å„è¡Œ: {"instruction": "æŒ‡ç¤º", "response": "å¿œç­”"}
python3 scripts/prepare_training_data.py \
    --input_file raw_data.txt \
    --output_file data/training_32b.jsonl
```

#### ã‚¹ãƒ†ãƒƒãƒ—3: 32Bãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ

```bash
# å°‚ç”¨ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½¿ç”¨ï¼ˆæ¨å¥¨ï¼‰
./scripts/train_32b_model.sh

# ã¾ãŸã¯ã€ã‚«ã‚¹ã‚¿ãƒ è¨­å®šã§å®Ÿè¡Œ
python3 scripts/train_large_model.py \
    --model_name "cyberagent/calm3-DeepSeek-R1-Distill-Qwen-32B" \
    --output_dir "outputs/my_32b_model" \
    --dataset_path "data/training_32b.jsonl" \
    --use_4bit \
    --use_qlora \
    --qlora_r 128 \
    --qlora_alpha 32 \
    --use_deepspeed \
    --deepspeed_config configs/deepspeed/ds_config_large.json \
    --gradient_checkpointing \
    --cpu_offload \
    --max_seq_length 512 \
    --num_epochs 2 \
    --batch_size 1 \
    --gradient_accumulation_steps 32
```

### ğŸ’¾ ãƒ¡ãƒ¢ãƒªè¦ä»¶ã¨æ¨å¥¨æ§‹æˆ

| ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º | æœ€å°GPUè¦ä»¶ | æ¨å¥¨GPUæ§‹æˆ | å­¦ç¿’æ™‚é–“ã®ç›®å®‰ |
|------------|-----------|------------|-------------|
| 32B | 1x A100 80GB | 2x A100 80GB | 24-48æ™‚é–“ |
| 32B (4-bit) | 1x A100 40GB | 1x A100 80GB | 36-72æ™‚é–“ |
| 17B | 1x A100 40GB | 1x A100 80GB | 12-24æ™‚é–“ |
| 7B | 1x RTX 4090 | 1x A100 40GB | 6-12æ™‚é–“ |

### âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã®ãƒ’ãƒ³ãƒˆ

#### 1. ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã®èª¿æ•´
```bash
# ãƒ¡ãƒ¢ãƒªä¸è¶³ã®å ´åˆã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã‚’çŸ­ç¸®
--max_seq_length 256  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 512
```

#### 2. ãƒãƒƒãƒã‚µã‚¤ã‚ºã¨ã‚°ãƒ©ãƒ‡ã‚£ã‚¨ãƒ³ãƒˆç´¯ç©
```bash
# å®ŸåŠ¹ãƒãƒƒãƒã‚µã‚¤ã‚º = batch_size * gradient_accumulation_steps
--batch_size 1 --gradient_accumulation_steps 64  # å®ŸåŠ¹ãƒãƒƒãƒã‚µã‚¤ã‚º: 64
```

#### 3. ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰è¨­å®š
```bash
# NVMeã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã‚’æœ‰åŠ¹åŒ–ï¼ˆé«˜é€ŸSSDæ¨å¥¨ï¼‰
--disk_offload_dir "/path/to/nvme/offload"
```

#### 4. æ··åˆç²¾åº¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
- FP16ã¯è‡ªå‹•çš„ã«æœ‰åŠ¹åŒ–ã•ã‚Œã¾ã™
- BF16ã¯A100ã§ã®ã¿æ¨å¥¨

### ğŸ“Š ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã¨ãƒ­ã‚°

```bash
# TensorBoardã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é€²æ—ã‚’ç›£è¦–
tensorboard --logdir outputs/my_32b_model/logs

# GPUä½¿ç”¨çŠ¶æ³ã‚’ç›£è¦–
watch -n 1 nvidia-smi

# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ­ã‚°ã‚’ç¢ºèª
tail -f outputs/my_32b_model/training.log
```

### ğŸ› ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ï¼ˆå¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ï¼‰

#### OOMï¼ˆOut of Memoryï¼‰ã‚¨ãƒ©ãƒ¼

1. **ã‚ˆã‚Šç©æ¥µçš„ãªé‡å­åŒ–ã‚’ä½¿ç”¨**
   ```bash
   --use_4bit --qlora_r 64  # rã‚’æ¸›ã‚‰ã—ã¦ãƒ¡ãƒ¢ãƒªå‰Šæ¸›
   ```

2. **ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã‚’çŸ­ç¸®**
   ```bash
   --max_seq_length 256
   ```

3. **DeepSpeed Stage 3ã‚’ç¢ºèª**
   ```bash
   --deepspeed_config configs/deepspeed/ds_config_ultra_large.json
   ```

#### å­¦ç¿’ãŒé…ã„å ´åˆ

1. Flash AttentionãŒæœ‰åŠ¹ã‹ç¢ºèª
2. ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã‚’èª¿æ•´
3. ãƒ‡ã‚£ã‚¹ã‚¯I/Oã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã‚’ç¢ºèª

### ğŸ”® æ¨è«–ã§ã®ä½¿ç”¨

```python
from src.models.japanese_model import JapaneseModel

# ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿32Bãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
model = JapaneseModel(
    model_name="outputs/my_32b_model",
    load_in_4bit=True,  # æ¨è«–æ™‚ã‚‚4bité‡å­åŒ–ã‚’ä½¿ç”¨
    use_flash_attention=True
)

model.load_model()
model.load_tokenizer()

# æ¨è«–å®Ÿè¡Œ
response = model.generate_japanese(
    instruction="é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„",
    max_new_tokens=512,
    temperature=0.7
)
print(response)
```

### ğŸ“ˆ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ

| ãƒ¢ãƒ‡ãƒ« | GPUæ§‹æˆ | å­¦ç¿’æ™‚é–“ | ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ | ãƒ‘ãƒ¼ãƒ—ãƒ¬ã‚­ã‚·ãƒ†ã‚£æ”¹å–„ |
|-------|---------|----------|-------------|------------------|
| 32B (4-bit) | 1x A100 80GB | 48æ™‚é–“ | 65GB | -15.2% |
| 32B (4-bit) | 2x A100 80GB | 26æ™‚é–“ | 32GBÃ—2 | -15.5% |
| 17B | 1x A100 80GB | 18æ™‚é–“ | 45GB | -12.8% |
| 7B | 1x RTX 4090 | 8æ™‚é–“ | 22GB | -10.5% |

---

## ğŸš€ é«˜åº¦ãªä½¿ç”¨æ–¹æ³•

### ğŸ’» ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³æ“ä½œ

Webã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã«åŠ ãˆã¦ã€ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‹ã‚‰ã‚‚æ“ä½œå¯èƒ½ã§ã™ï¼š

#### ç›´æ¥çš„ãªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
```bash
# ã‚³ãƒ³ãƒ†ãƒŠã«å…¥ã‚‹
docker exec -it ai-ft-container bash

# LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ
python /workspace/working_lora_demo.py

# ãƒ•ãƒ«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ
python /workspace/test_full_finetuning_improved.py
```

#### ãƒãƒƒãƒå‡¦ç†
```bash
# è¤‡æ•°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®é€£ç¶šå­¦ç¿’
for dataset in data1.jsonl data2.jsonl data3.jsonl; do
    python scripts/prepare_training_data.py --input $dataset --output processed_$dataset
    python train_model.py --data processed_$dataset
done
```

### ğŸ”Œ APIç›´æ¥åˆ©ç”¨

Webã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®APIã‚’ç›´æ¥å‘¼ã³å‡ºã™ã“ã¨ã‚‚å¯èƒ½ã§ã™ï¼š

```python
import requests

# ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±å–å¾—
response = requests.get("http://localhost:8000/api/system-info")
print(response.json())

# ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
generation_request = {
    "model_path": "/workspace/lora_demo_20250626_074248",
    "prompt": "è³ªå•: AIã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ\nå›ç­”: ",
    "max_length": 100,
    "temperature": 0.7
}

response = requests.post(
    "http://localhost:8000/api/generate",
    json=generation_request
)
print(response.json())
```

### ğŸ“ˆ ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã¨åˆ†æ

#### TensorBoard ã§ã®ç›£è¦–
```bash
# TensorBoardèµ·å‹•
docker exec ai-ft-container tensorboard --logdir=/workspace/logs --host=0.0.0.0 --port=6006
# ãƒ–ãƒ©ã‚¦ã‚¶ã§ http://localhost:6006 ã«ã‚¢ã‚¯ã‚»ã‚¹
```

#### Weights & Biasesé€£æº
```bash
# W&Bè¨­å®š
docker exec ai-ft-container python -c "import wandb; wandb.login()"
```

### ğŸ”„ ç¶™ç¶šçš„å­¦ç¿’

#### ãƒ¢ãƒ‡ãƒ«ã®è¿½åŠ å­¦ç¿’
æ—¢å­˜ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ™ãƒ¼ã‚¹ã«ã€ã•ã‚‰ã«å­¦ç¿’ã‚’ç¶šã‘ã‚‹ã“ã¨ã‚‚å¯èƒ½ã§ã™ï¼š

```python
# æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
base_model_path = "/workspace/lora_demo_20250626_074248"
additional_data = ["æ–°ã—ã„å­¦ç¿’ãƒ‡ãƒ¼ã‚¿1", "æ–°ã—ã„å­¦ç¿’ãƒ‡ãƒ¼ã‚¿2"]

# è¿½åŠ å­¦ç¿’å®Ÿè¡Œ
# (è©³ç´°ãªå®Ÿè£…ã¯é–‹ç™ºã‚¬ã‚¤ãƒ‰ã‚’å‚ç…§)
```

---

## ğŸ“ ã‚µãƒãƒ¼ãƒˆ

### ğŸ†˜ å•é¡ŒãŒè§£æ±ºã—ãªã„å ´åˆ

1. **ãƒ­ã‚°ã®ç¢ºèª**: è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’å–å¾—
2. **GitHub Issues**: https://github.com/kji-furuta/AI_FT/issues
3. **ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£**: ä»–ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã®æƒ…å ±å…±æœ‰

### ğŸ“š è¿½åŠ ãƒªã‚½ãƒ¼ã‚¹

- **API ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹**: `docs/API_REFERENCE.md`
- **é–‹ç™ºè€…ã‚¬ã‚¤ãƒ‰**: `docs/DEVELOPER_GUIDE.md`
- **è¨­å®šãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹**: `docs/CONFIG_REFERENCE.md`

---

## ğŸ‰ ãŠã‚ã‚Šã«

ã“ã®ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã‚’æ´»ç”¨ã—ã¦ã€AI Fine-tuning Toolkitã‚’æœ€å¤§é™ã«æ´»ç”¨ã—ã¦ãã ã•ã„ã€‚è³ªå•ã‚„æ”¹å–„ææ¡ˆãŒã”ã–ã„ã¾ã—ãŸã‚‰ã€ãŠæ°—è»½ã«ãŠå•ã„åˆã‚ã›ãã ã•ã„ã€‚

**Happy Fine-tuning! ğŸš€**

---

*ã“ã®ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã¯ç¶™ç¶šçš„ã«æ›´æ–°ã•ã‚Œã¾ã™ã€‚æœ€æ–°ç‰ˆã¯å¸¸ã«ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒªãƒã‚¸ãƒˆãƒªã§ç¢ºèªã—ã¦ãã ã•ã„ã€‚*