#!/usr/bin/env python3
"""
RTX A5000 x2 å®Ÿè·µãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¬ã‚¤ãƒ‰
å‹•ä½œç¢ºèªæ¸ˆã¿ã®ã‚·ãƒ³ãƒ—ãƒ«ãªæ‰‹é †
"""

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model, TaskType
import os
from datetime import datetime

class SimpleTextDataset(Dataset):
    def __init__(self, texts, tokenizer, max_length=256):
        self.encodings = []
        for text in texts:
            encoding = tokenizer(
                text,
                truncation=True,
                padding='max_length',
                max_length=max_length,
                return_tensors='pt'
            )
            self.encodings.append({
                'input_ids': encoding['input_ids'].flatten(),
                'attention_mask': encoding['attention_mask'].flatten(),
                'labels': encoding['input_ids'].flatten()
            })
    
    def __len__(self):
        return len(self.encodings)
    
    def __getitem__(self, idx):
        return self.encodings[idx]

def main():
    print("ğŸš€ RTX A5000 x2 å®Ÿè·µãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¬ã‚¤ãƒ‰")
    print("=" * 60)
    
    # GPUç¢ºèª
    print("ğŸ”§ GPUç’°å¢ƒç¢ºèª:")
    if torch.cuda.is_available():
        device_count = torch.cuda.device_count()
        print(f"âœ… åˆ©ç”¨å¯èƒ½GPU: {device_count}å°")
        
        for i in range(device_count):
            props = torch.cuda.get_device_properties(i)
            memory_gb = props.total_memory / (1024**3)
            print(f"   GPU {i}: {props.name} ({memory_gb:.1f}GB)")
    else:
        print("âŒ CUDAãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
        return False
    
    print("\nğŸ“š æ¨å¥¨å®Ÿè¡Œæ‰‹é †:")
    print("=" * 40)
    
    print("\nğŸ¥‡ **æ–¹æ³•1: è»½é‡ãƒ¢ãƒ‡ãƒ«ã§å­¦ç¿’ï¼ˆæ¨å¥¨ï¼‰**")
    print("   ãƒ¢ãƒ‡ãƒ«: distilgpt2")
    print("   ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: ~2GB")
    print("   å®Ÿè¡Œæ™‚é–“: 5-10åˆ†")
    print("   å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰:")
    print("   ```")
    print("   docker exec ai-ft-container python /workspace/run_distilgpt2_lora.py")
    print("   ```")
    
    print("\nğŸ¥ˆ **æ–¹æ³•2: æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã§å­¦ç¿’ï¼ˆä¸­ç´šï¼‰**")
    print("   ãƒ¢ãƒ‡ãƒ«: stabilityai/japanese-stablelm-3b-4e1t-instruct")
    print("   ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: ~8GB")
    print("   å®Ÿè¡Œæ™‚é–“: 15-30åˆ†")
    print("   æ³¨æ„: HuggingFaceèªè¨¼ãŒå¿…è¦ãªå ´åˆãŒã‚ã‚Šã¾ã™")
    
    print("\nğŸ¥‰ **æ–¹æ³•3: å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ï¼ˆä¸Šç´šï¼‰**")
    print("   ãƒ¢ãƒ‡ãƒ«: 7B-13Bãƒ¢ãƒ‡ãƒ«")
    print("   ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: 16-32GB")
    print("   æ‰‹æ³•: QLoRAæ¨å¥¨")
    print("   æ³¨æ„: Model Parallelismä½¿ç”¨")
    
    print("\n" + "=" * 60)
    print("ğŸ¯ ä»Šã™ãè©¦ã›ã‚‹æ–¹æ³•1ã®å®Ÿè£…:")
    print("=" * 60)
    
    return demo_simple_lora()

def demo_simple_lora():
    """ã‚·ãƒ³ãƒ—ãƒ«ãªLoRAãƒ‡ãƒ¢ï¼ˆdistilgpt2ä½¿ç”¨ï¼‰"""
    
    try:
        print("\nğŸš€ æ–¹æ³•1: DistilGPT-2 LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹")
        
        # ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
        model_name = "distilgpt2"
        print(f"ğŸ“š ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿: {model_name}")
        
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        
        print("âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")
        
        # LoRAè¨­å®š
        peft_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            inference_mode=False,
            r=8,  # å°ã•ã‚ã®ãƒ©ãƒ³ã‚¯
            lora_alpha=16,
            lora_dropout=0.1,
            target_modules=["c_attn", "c_proj"]  # GPT-2ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
        )
        
        model = get_peft_model(model, peft_config)
        print("âš™ï¸ LoRAè¨­å®šå®Œäº†")
        model.print_trainable_parameters()
        
        # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ï¼ˆè‹±èªã§ç°¡å˜ã«ï¼‰
        train_texts = [
            "Hello, how are you today? I am doing well, thank you.",
            "What is the weather like? The weather is sunny and warm.",
            "Can you help me? Of course, I would be happy to help.",
            "Thank you very much. You are welcome, anytime.",
            "Good morning! Good morning, have a great day!"
        ]
        
        print(f"ğŸ“ è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(train_texts)}ä»¶")
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™
        dataset = SimpleTextDataset(train_texts, tokenizer, max_length=128)
        
        # è¨“ç·´è¨­å®š
        output_dir = f"./outputs/distilgpt2_lora_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        training_args = TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=3,
            per_device_train_batch_size=2,
            gradient_accumulation_steps=2,
            warmup_steps=10,
            learning_rate=3e-4,
            fp16=True,
            logging_steps=1,
            save_steps=50,
            save_total_limit=2,
            prediction_loss_only=True,
            remove_unused_columns=False,
        )
        
        # ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ã‚¯ã‚¿ãƒ¼
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=tokenizer,
            mlm=False,
        )
        
        # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=dataset,
            data_collator=data_collator,
        )
        
        print("ğŸ‹ï¸ è¨“ç·´é–‹å§‹...")
        
        # è¨“ç·´å‰ãƒ†ã‚¹ãƒˆ
        print("\n--- è¨“ç·´å‰ãƒ†ã‚¹ãƒˆ ---")
        test_input = "Hello, how are you"
        inputs = tokenizer(test_input, return_tensors="pt")
        
        with torch.no_grad():
            outputs = model.generate(
                inputs['input_ids'],
                max_new_tokens=20,
                do_sample=True,
                temperature=0.7,
                pad_token_id=tokenizer.eos_token_id
            )
        
        before_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        print(f"å…¥åŠ›: {test_input}")
        print(f"å‡ºåŠ›: {before_text}")
        
        # è¨“ç·´å®Ÿè¡Œ
        trainer.train()
        
        print("\nâœ… è¨“ç·´å®Œäº†ï¼")
        
        # è¨“ç·´å¾Œãƒ†ã‚¹ãƒˆ
        print("\n--- è¨“ç·´å¾Œãƒ†ã‚¹ãƒˆ ---")
        with torch.no_grad():
            outputs = model.generate(
                inputs['input_ids'],
                max_new_tokens=20,
                do_sample=True,
                temperature=0.7,
                pad_token_id=tokenizer.eos_token_id
            )
        
        after_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        print(f"å…¥åŠ›: {test_input}")
        print(f"å‡ºåŠ›: {after_text}")
        
        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        model.save_pretrained(output_dir)
        tokenizer.save_pretrained(output_dir)
        
        print(f"\nğŸ’¾ ãƒ¢ãƒ‡ãƒ«ä¿å­˜: {output_dir}")
        print("âœ… ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†ï¼")
        
        return True
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return False

def show_advanced_options():
    """ä¸Šç´šè€…å‘ã‘ã‚ªãƒ—ã‚·ãƒ§ãƒ³"""
    print("\nğŸ”¥ ä¸Šç´šè€…å‘ã‘ã‚ªãƒ—ã‚·ãƒ§ãƒ³:")
    print("=" * 40)
    
    print("\nğŸ“Š **ãƒãƒ«ãƒGPUæœ€é©åŒ–:**")
    print("```python")
    print("# Model Parallelism (å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ç”¨)")
    print("model = AutoModelForCausalLM.from_pretrained(")
    print("    model_name,")
    print("    device_map='auto',  # è‡ªå‹•GPUé…ç½®")
    print("    torch_dtype=torch.float16,")
    print("    load_in_8bit=True  # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–")
    print(")")
    print("```")
    
    print("\nğŸ’ **QLoRAè¨­å®š (13B+ãƒ¢ãƒ‡ãƒ«ç”¨):**")
    print("```python")
    print("from transformers import BitsAndBytesConfig")
    print("")
    print("bnb_config = BitsAndBytesConfig(")
    print("    load_in_4bit=True,")
    print("    bnb_4bit_quant_type='nf4',")
    print("    bnb_4bit_compute_dtype=torch.float16")
    print(")")
    print("")
    print("peft_config = LoraConfig(")
    print("    r=8,  # å°ã•ã‚ã®ãƒ©ãƒ³ã‚¯")
    print("    lora_alpha=16,")
    print("    target_modules=['q_proj', 'v_proj'],")
    print("    lora_dropout=0.1")
    print(")")
    print("```")
    
    print("\nğŸ¯ **å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨ä¾‹:**")
    print("```python")
    print("# CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿")
    print("import pandas as pd")
    print("df = pd.read_csv('your_data.csv')")
    print("train_texts = df['text'].tolist()")
    print("")
    print("# JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿")
    print("import json")
    print("with open('data.json', 'r') as f:")
    print("    data = json.load(f)")
    print("train_texts = [item['text'] for item in data]")
    print("```")

def show_troubleshooting():
    """ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°"""
    print("\nğŸ”§ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°:")
    print("=" * 40)
    
    print("\nâŒ **ã‚ˆãã‚ã‚‹ã‚¨ãƒ©ãƒ¼ã¨è§£æ±ºæ³•:**")
    print("\n1. CUDA Out of Memory:")
    print("   è§£æ±ºæ³•: batch_sizeã‚’1ã«ã€gradient_accumulation_stepsã‚’å¢—ã‚„ã™")
    print("   ```python")
    print("   per_device_train_batch_size=1,")
    print("   gradient_accumulation_steps=8")
    print("   ```")
    
    print("\n2. ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼:")
    print("   è§£æ±ºæ³•: ã‚ˆã‚Šè»½é‡ãªãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨")
    print("   ```python")
    print("   model_name = 'distilgpt2'  # æœ€ã‚‚è»½é‡")
    print("   ```")
    
    print("\n3. ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚¨ãƒ©ãƒ¼:")
    print("   è§£æ±ºæ³•: use_fast=Falseã‚’è¿½åŠ ")
    print("   ```python")
    print("   tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)")
    print("   ```")
    
    print("\nâœ… **æˆåŠŸã®ç¢ºèªæ–¹æ³•:**")
    print("â€¢ è¨“ç·´Loss ãŒä¸‹ãŒã£ã¦ã„ã‚‹")
    print("â€¢ ç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆãŒæ”¹å–„ã•ã‚Œã¦ã„ã‚‹")
    print("â€¢ ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ­£å¸¸ã«ä¿å­˜ã•ã‚Œã¦ã„ã‚‹")

if __name__ == "__main__":
    try:
        success = main()
        if success:
            show_advanced_options()
            show_troubleshooting()
            
            print("\n" + "=" * 60)
            print("ğŸ‰ ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè·µã‚¬ã‚¤ãƒ‰å®Œäº†ï¼")
            print("=" * 60)
            print("\næ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
            print("â€¢ ã‚ˆã‚Šå¤§ããªãƒ¢ãƒ‡ãƒ«ã«æŒ‘æˆ¦")
            print("â€¢ ç‹¬è‡ªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®å­¦ç¿’")
            print("â€¢ ãƒãƒ«ãƒGPUè¨­å®šã®æœ€é©åŒ–")
            print("â€¢ é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã®å­¦ç¿’")
            
    except Exception as e:
        print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()