#!/usr/bin/env python3
"""
MoEå®Ÿè£…ã®å•é¡Œã‚’ä¿®æ­£ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
- ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èªå½™ã‚µã‚¤ã‚ºä¸ä¸€è‡´
- ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ç¯„å›²å¤–ã‚¢ã‚¯ã‚»ã‚¹
- embeddingå±¤ã®ã‚µã‚¤ã‚ºèª¿æ•´
"""

import sys
import os
sys.path.append('/workspace')

def fix_run_training():
    """run_training.pyã®ãƒ€ãƒŸãƒ¼ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ä¿®æ­£"""
    file_path = "/workspace/scripts/moe/run_training.py"
    
    with open(file_path, 'r') as f:
        content = f.read()
    
    # ãƒ€ãƒŸãƒ¼ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ä¿®æ­£
    old_tokenizer = """        class DummyTokenizer:
            def __call__(self, text, **kwargs):
                max_length = kwargs.get('max_length', 100)
                return {
                    'input_ids': torch.randint(0, 1000, (1, max_length)),
                    'attention_mask': torch.ones(1, max_length)
                }"""
    
    new_tokenizer = """        class DummyTokenizer:
            vocab_size = 32000  # ãƒ¢ãƒ‡ãƒ«ã®embeddingå±¤ã¨ä¸€è‡´ã•ã›ã‚‹
            def __call__(self, text, **kwargs):
                max_length = kwargs.get('max_length', 100)
                return {
                    'input_ids': torch.randint(0, self.vocab_size, (1, max_length)),
                    'attention_mask': torch.ones(1, max_length)
                }
            def __len__(self):
                return self.vocab_size"""
    
    content = content.replace(old_tokenizer, new_tokenizer)
    
    with open(file_path, 'w') as f:
        f.write(content)
    
    print(f"âœ“ Fixed tokenizer in run_training.py")

def fix_moe_architecture():
    """moe_architecture.pyã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å•é¡Œã‚’ä¿®æ­£"""
    file_path = "/workspace/src/moe/moe_architecture.py"
    
    with open(file_path, 'r') as f:
        lines = f.readlines()
    
    # TopKRouterã®forwardé–¢æ•°ã‚’ä¿®æ­£ï¼ˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ã‚¯ãƒ©ãƒ³ãƒ—ï¼‰
    for i, line in enumerate(lines):
        # expert_indicesã‚’ã‚¯ãƒ©ãƒ³ãƒ—ã™ã‚‹å‡¦ç†ã‚’è¿½åŠ 
        if "expert_indices = topk_indices % self.num_experts" in line:
            lines[i] = "        expert_indices = topk_indices % self.num_experts  # ç¯„å›²å†…ã«åˆ¶é™\n"
            lines.insert(i+1, "        expert_indices = torch.clamp(expert_indices, 0, self.num_experts - 1)  # è¿½åŠ ã®å®‰å…¨å¯¾ç­–\n")
            break
    
    with open(file_path, 'w') as f:
        f.writelines(lines)
    
    print(f"âœ“ Fixed expert indices clamping in moe_architecture.py")

def fix_moe_training():
    """moe_training.pyã®ãƒãƒƒãƒå‡¦ç†ã‚’ä¿®æ­£"""
    file_path = "/workspace/src/moe/moe_training.py"
    
    with open(file_path, 'r') as f:
        content = f.read()
    
    # input_idsã®ç¯„å›²ãƒã‚§ãƒƒã‚¯ã‚’è¿½åŠ 
    if "def train(" in content and "for batch in pbar:" in content:
        # ãƒãƒƒãƒå‡¦ç†ã®å‰ã«input_idsã‚’ã‚¯ãƒ©ãƒ³ãƒ—
        old_section = """            # Forward pass
            outputs = self.model("""
        
        new_section = """            # input_idsã‚’èªå½™ã‚µã‚¤ã‚ºå†…ã«åˆ¶é™
            if hasattr(self.tokenizer, 'vocab_size'):
                vocab_size = self.tokenizer.vocab_size
            else:
                vocab_size = len(self.tokenizer) if hasattr(self.tokenizer, '__len__') else 32000
            
            batch['input_ids'] = torch.clamp(batch['input_ids'], 0, vocab_size - 1)
            
            # Forward pass
            outputs = self.model("""
        
        content = content.replace(old_section, new_section)
    
    with open(file_path, 'w') as f:
        f.write(content)
    
    print(f"âœ“ Fixed input_ids clamping in moe_training.py")

def create_safe_test_script():
    """å®‰å…¨ãªãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½œæˆ"""
    test_script = """#!/usr/bin/env python3
'''
ä¿®æ­£å¾Œã®MoEãƒ¢ãƒ‡ãƒ«ã®å®‰å…¨ãªãƒ†ã‚¹ãƒˆ
'''
import torch
import sys
sys.path.insert(0, '/workspace')

from src.moe.moe_architecture import CivilEngineeringMoEModel, MoEConfig

def test_safe_moe():
    print("=" * 60)
    print("Safe MoE Test - ä¿®æ­£ç‰ˆ")
    print("=" * 60)
    
    # æœ€å°æ§‹æˆã§ãƒ†ã‚¹ãƒˆ
    config = MoEConfig(
        hidden_size=256,
        num_experts=8,
        num_experts_per_tok=2,
        expert_capacity_factor=1.25,
        domain_specific_routing=True
    )
    
    try:
        # ãƒ¢ãƒ‡ãƒ«ä½œæˆ
        model = CivilEngineeringMoEModel(config, base_model=None)
        print(f"âœ“ Model created")
        print(f"  Parameters: {sum(p.numel() for p in model.parameters()):,}")
        
        # GPUç§»å‹•
        if torch.cuda.is_available():
            model = model.cuda()
            device = 'cuda'
            print(f"âœ“ Model moved to GPU")
        else:
            device = 'cpu'
            print(f"! Running on CPU")
        
        # å®‰å…¨ãªå…¥åŠ›ãƒ‡ãƒ¼ã‚¿ä½œæˆ
        batch_size = 2
        seq_len = 32
        vocab_size = 32000  # embeddingå±¤ã¨ä¸€è‡´
        
        # input_idsã‚’ç¢ºå®Ÿã«ç¯„å›²å†…ã«
        input_ids = torch.randint(0, vocab_size, (batch_size, seq_len)).to(device)
        attention_mask = torch.ones(batch_size, seq_len).to(device)
        
        print(f"\\nâœ“ Input data created")
        print(f"  input_ids shape: {input_ids.shape}")
        print(f"  input_ids range: [{input_ids.min().item()}, {input_ids.max().item()}]")
        print(f"  vocab_size: {vocab_size}")
        
        # Forward pass
        with torch.no_grad():
            outputs = model(input_ids, attention_mask)
        
        print(f"\\nâœ“ Forward pass successful!")
        print(f"  Output shape: {outputs['hidden_states'].shape}")
        print(f"  Aux loss: {outputs['aux_loss'].item():.6f}")
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
        if torch.cuda.is_available():
            print(f"\\nğŸ“Š GPU Memory:")
            print(f"  Allocated: {torch.cuda.memory_allocated() / 1024**2:.2f} MB")
            print(f"  Reserved: {torch.cuda.memory_reserved() / 1024**2:.2f} MB")
        
        return True
        
    except Exception as e:
        print(f"\\nâœ— Test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = test_safe_moe()
    if success:
        print("\\n" + "=" * 60)
        print("âœ… All tests passed! MoE is ready for training.")
        print("=" * 60)
    else:
        print("\\n" + "=" * 60)
        print("âŒ Tests failed. Please check the errors above.")
        print("=" * 60)
"""
    
    with open("/workspace/scripts/moe/test_moe_safe.py", "w") as f:
        f.write(test_script)
    
    print(f"âœ“ Created safe test script: test_moe_safe.py")

def main():
    print("=" * 60)
    print("MoE Issue Fixes")
    print("=" * 60)
    
    try:
        # 1. run_training.pyã®ä¿®æ­£
        fix_run_training()
        
        # 2. moe_architecture.pyã®ä¿®æ­£
        fix_moe_architecture()
        
        # 3. moe_training.pyã®ä¿®æ­£
        fix_moe_training()
        
        # 4. ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆä½œæˆ
        create_safe_test_script()
        
        print("\n" + "=" * 60)
        print("âœ… All fixes applied successfully!")
        print("=" * 60)
        print("\nNext steps:")
        print("1. Test the fixes: python scripts/moe/test_moe_safe.py")
        print("2. Run training: bash scripts/moe/train_moe.sh demo 1 1")
        
    except Exception as e:
        print(f"\nâœ— Error applying fixes: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()