#!/usr/bin/env python3
"""
Run MoE Training
MoEãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import sys
import os
sys.path.append('/home/kjifu/AI_FT_7')

import torch
import argparse
from pathlib import Path
from src.moe.moe_architecture import create_civil_engineering_moe, MoEConfig
from src.moe.moe_training import MoETrainer, MoETrainingConfig, CivilEngineeringDataset
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def parse_args():
    """ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã®ãƒ‘ãƒ¼ã‚¹"""
    parser = argparse.ArgumentParser(description="Train MoE model for civil engineering")
    
    # ãƒ¢ãƒ‡ãƒ«è¨­å®š
    parser.add_argument("--base_model", type=str, default="cyberagent/calm3-22b-chat",
                      help="Base model name")
    parser.add_argument("--num_experts", type=int, default=8,
                      help="Number of experts")
    parser.add_argument("--num_experts_per_tok", type=int, default=2,
                      help="Number of active experts per token")
    
    # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®š
    parser.add_argument("--batch_size", type=int, default=2,
                      help="Batch size")
    parser.add_argument("--gradient_accumulation_steps", type=int, default=16,
                      help="Gradient accumulation steps")
    parser.add_argument("--learning_rate", type=float, default=2e-5,
                      help="Learning rate")
    parser.add_argument("--num_epochs", type=int, default=3,
                      help="Number of epochs")
    parser.add_argument("--max_seq_length", type=int, default=512,
                      help="Maximum sequence length")
    
    # ãƒ‘ã‚¹è¨­å®š
    parser.add_argument("--data_path", type=str, default="./data/civil_engineering",
                      help="Path to training data")
    parser.add_argument("--output_dir", type=str, default="./outputs/moe_civil",
                      help="Output directory")
    parser.add_argument("--checkpoint_dir", type=str, default="./checkpoints/moe_civil",
                      help="Checkpoint directory")
    
    # ãã®ä»–
    parser.add_argument("--use_mixed_precision", action="store_true",
                      help="Use mixed precision training")
    parser.add_argument("--gradient_checkpointing", action="store_true",
                      help="Use gradient checkpointing")
    parser.add_argument("--demo_mode", action="store_true",
                      help="Run in demo mode with smaller model")
    
    return parser.parse_args()


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    args = parse_args()
    
    print("=" * 60)
    print("MoEåœŸæœ¨ãƒ»å»ºè¨­ç‰¹åŒ–ãƒ¢ãƒ‡ãƒ« ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°")
    print("=" * 60)
    
    # GPUç¢ºèª
    if torch.cuda.is_available():
        print(f"âœ“ GPUåˆ©ç”¨å¯èƒ½: {torch.cuda.device_count()} devices")
        for i in range(torch.cuda.device_count()):
            print(f"  Device {i}: {torch.cuda.get_device_name(i)}")
    else:
        print("! GPUãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚CPUãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã—ã¾ã™ã€‚")
    
    # è¨­å®šã®ä½œæˆ
    config = MoETrainingConfig(
        base_model_name=args.base_model,
        num_experts=args.num_experts,
        num_experts_per_tok=args.num_experts_per_tok,
        batch_size=args.batch_size,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        learning_rate=args.learning_rate,
        num_epochs=args.num_epochs,
        max_seq_length=args.max_seq_length,
        dataset_path=args.data_path,
        output_dir=args.output_dir,
        checkpoint_dir=args.checkpoint_dir,
        mixed_precision="bf16" if args.use_mixed_precision else None,
        gradient_checkpointing=args.gradient_checkpointing,
    )
    
    # ãƒ‡ãƒ¢ãƒ¢ãƒ¼ãƒ‰ã®è¨­å®š
    if args.demo_mode:
        print("\nğŸ“ ãƒ‡ãƒ¢ãƒ¢ãƒ¼ãƒ‰: å°è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã§å®Ÿè¡Œã—ã¾ã™")
        config.num_epochs = 1
        config.max_seq_length = 128
        config.logging_steps = 1
        config.save_steps = 10
        config.eval_steps = 5
    
    print("\nè¨­å®š:")
    print(f"  ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆæ•°: {config.num_experts}")
    print(f"  ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆ: {config.num_experts_per_tok}")
    print(f"  ãƒãƒƒãƒã‚µã‚¤ã‚º: {config.batch_size}")
    print(f"  å­¦ç¿’ç‡: {config.learning_rate}")
    print(f"  ã‚¨ãƒãƒƒã‚¯æ•°: {config.num_epochs}")
    
    # ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ
    print("\nãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆä¸­...")
    
    if args.demo_mode:
        # ãƒ‡ãƒ¢ç”¨ã®å°è¦æ¨¡ãƒ¢ãƒ‡ãƒ«
        moe_config = MoEConfig(
            hidden_size=512,  # å°è¦æ¨¡
            num_experts=config.num_experts,
            num_experts_per_tok=config.num_experts_per_tok,
            domain_specific_routing=True
        )
        from src.moe.moe_architecture import CivilEngineeringMoEModel
        model = CivilEngineeringMoEModel(moe_config, base_model=None)
    else:
        model = create_civil_engineering_moe(
            base_model_name=config.base_model_name,
            num_experts=config.num_experts
        )
    
    print(f"âœ“ ãƒ¢ãƒ‡ãƒ«ä½œæˆå®Œäº†")
    print(f"  ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in model.parameters()):,}")
    
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®æº–å‚™
    print("\nãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’æº–å‚™ä¸­...")
    try:
        from transformers import AutoTokenizer
        tokenizer = AutoTokenizer.from_pretrained(config.base_model_name)
    except:
        logger.warning("Failed to load tokenizer, using dummy tokenizer")
        # ãƒ€ãƒŸãƒ¼ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
        class DummyTokenizer:
            vocab_size = 32000  # ãƒ¢ãƒ‡ãƒ«ã®embeddingå±¤ã¨ä¸€è‡´ã•ã›ã‚‹
            safe_vocab_size = 30000  # å®‰å…¨ãƒãƒ¼ã‚¸ãƒ³ï¼ˆ93.75%ï¼‰ã‚’ç¢ºä¿
            
            def __call__(self, text, **kwargs):
                max_length = kwargs.get('max_length', 100)
                # å®‰å…¨ãªç¯„å›²ã§ã®ã¿ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç”Ÿæˆï¼ˆ0-29999ï¼‰
                # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ç”¨ã«0-10ã‚’äºˆç´„ã€é€šå¸¸ãƒˆãƒ¼ã‚¯ãƒ³ã¯11-29999
                return {
                    'input_ids': torch.randint(11, self.safe_vocab_size, (1, max_length)),
                    'attention_mask': torch.ones(1, max_length)
                }
            
            def __len__(self):
                return self.vocab_size  # embeddingå±¤ã®ã‚µã‚¤ã‚ºã¯32000ã®ã¾ã¾
        tokenizer = DummyTokenizer()
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™
    print("\nãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æº–å‚™ä¸­...")
    
    train_dataset = CivilEngineeringDataset(
        data_path=f"{config.dataset_path}/train",
        tokenizer=tokenizer,
        max_length=config.max_seq_length
    )
    
    val_dataset = CivilEngineeringDataset(
        data_path=f"{config.dataset_path}/val",
        tokenizer=tokenizer,
        max_length=config.max_seq_length
    )
    
    print(f"âœ“ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™å®Œäº†")
    print(f"  ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚µãƒ³ãƒ—ãƒ«: {len(train_dataset)}")
    print(f"  æ¤œè¨¼ã‚µãƒ³ãƒ—ãƒ«: {len(val_dataset)}")
    
    # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã®åˆæœŸåŒ–
    print("\nãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚’åˆæœŸåŒ–ä¸­...")
    trainer = MoETrainer(model, config, tokenizer)
    
    # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ
    print("\n" + "=" * 60)
    print("ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã—ã¾ã™...")
    print("=" * 60)
    
    try:
        trainer.train(train_dataset, val_dataset)
        print("\nâœ“ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸï¼")
    except KeyboardInterrupt:
        print("\n! ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        print(f"\nâœ— ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()
    
    print("\n" + "=" * 60)
    print("ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°çµ‚äº†")
    print("=" * 60)
    
    # çµæœã®ä¿å­˜å ´æ‰€
    print("\nçµæœã®ä¿å­˜å ´æ‰€:")
    print(f"  ãƒ¢ãƒ‡ãƒ«: {config.output_dir}")
    print(f"  ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ: {config.checkpoint_dir}")


if __name__ == "__main__":
    main()
