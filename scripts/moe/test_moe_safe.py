#!/usr/bin/env python3
'''
ä¿®æ­£å¾Œã®MoEãƒ¢ãƒ‡ãƒ«ã®å®‰å…¨ãªãƒ†ã‚¹ãƒˆ
'''
import torch
import sys
sys.path.insert(0, '/workspace')

from src.moe.moe_architecture import CivilEngineeringMoEModel, MoEConfig

def test_safe_moe():
    print("=" * 60)
    print("Safe MoE Test - ä¿®æ­£ç‰ˆ")
    print("=" * 60)
    
    # æœ€å°æ§‹æˆã§ãƒ†ã‚¹ãƒˆ
    config = MoEConfig(
        hidden_size=256,
        num_experts=8,
        num_experts_per_tok=2,
        expert_capacity_factor=1.25,
        domain_specific_routing=True
    )
    
    try:
        # ãƒ¢ãƒ‡ãƒ«ä½œæˆ
        model = CivilEngineeringMoEModel(config, base_model=None)
        print(f"âœ“ Model created")
        print(f"  Parameters: {sum(p.numel() for p in model.parameters()):,}")
        
        # GPUç§»å‹•
        if torch.cuda.is_available():
            model = model.cuda()
            device = 'cuda'
            print(f"âœ“ Model moved to GPU")
        else:
            device = 'cpu'
            print(f"! Running on CPU")
        
        # å®‰å…¨ãªå…¥åŠ›ãƒ‡ãƒ¼ã‚¿ä½œæˆ
        batch_size = 2
        seq_len = 32
        vocab_size = 32000  # embeddingå±¤ã¨ä¸€è‡´
        
        # input_idsã‚’ç¢ºå®Ÿã«ç¯„å›²å†…ã«
        input_ids = torch.randint(0, vocab_size, (batch_size, seq_len)).to(device)
        attention_mask = torch.ones(batch_size, seq_len).to(device)
        
        print(f"\nâœ“ Input data created")
        print(f"  input_ids shape: {input_ids.shape}")
        print(f"  input_ids range: [{input_ids.min().item()}, {input_ids.max().item()}]")
        print(f"  vocab_size: {vocab_size}")
        
        # Forward pass
        with torch.no_grad():
            outputs = model(input_ids, attention_mask)
        
        print(f"\nâœ“ Forward pass successful!")
        print(f"  Output shape: {outputs['hidden_states'].shape}")
        print(f"  Aux loss: {outputs['aux_loss'].item():.6f}")
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
        if torch.cuda.is_available():
            print(f"\nğŸ“Š GPU Memory:")
            print(f"  Allocated: {torch.cuda.memory_allocated() / 1024**2:.2f} MB")
            print(f"  Reserved: {torch.cuda.memory_reserved() / 1024**2:.2f} MB")
        
        return True
        
    except Exception as e:
        print(f"\nâœ— Test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = test_safe_moe()
    if success:
        print("\n" + "=" * 60)
        print("âœ… All tests passed! MoE is ready for training.")
        print("=" * 60)
    else:
        print("\n" + "=" * 60)
        print("âŒ Tests failed. Please check the errors above.")
        print("=" * 60)
