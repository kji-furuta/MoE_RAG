#!/usr/bin/env python3
"""
Ollama ãƒ¢ãƒ‡ãƒ«çµ±åˆè¨­å®š
ãƒ¡ãƒ¢ãƒªä¸è¶³æ™‚ã«Ollamaãƒ¢ãƒ‡ãƒ«ã‚’è‡ªå‹•çš„ã«ä½¿ç”¨
"""

import os
import json
import requests
from typing import Dict, Optional, List
from loguru import logger

class OllamaModelManager:
    """Ollamaãƒ¢ãƒ‡ãƒ«ç®¡ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, base_url: str = "http://localhost:11434"):
        self.base_url = base_url
        self.available_models = {}
        self.model_mapping = {
            # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ« -> Ollamaãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«
            "calm3-22b": "llama3.2:3b",
            "qwen2.5-14b": "llama3.2:3b", 
            "qwen2.5-32b": "llama3.2:3b",
            "deepseek-r1": "llama3.2:3b",
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            "default": "llama3.2:3b"
        }
        
    def check_ollama_status(self) -> bool:
        """Ollamaã‚µãƒ¼ãƒ“ã‚¹ã®çŠ¶æ…‹ã‚’ç¢ºèª"""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            if response.status_code == 200:
                self.available_models = {
                    model["name"]: model 
                    for model in response.json().get("models", [])
                }
                logger.info(f"âœ… Ollamaåˆ©ç”¨å¯èƒ½: {list(self.available_models.keys())}")
                return True
        except Exception as e:
            logger.warning(f"âš ï¸ Ollamaæ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")
        return False
    
    def get_fallback_model(self, original_model: str) -> Optional[str]:
        """ãƒ¡ãƒ¢ãƒªä¸è¶³æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—"""
        # ãƒ¢ãƒ‡ãƒ«åã‚’æ­£è¦åŒ–
        model_key = original_model.lower()
        for key in self.model_mapping:
            if key in model_key:
                fallback = self.model_mapping[key]
                if fallback in self.available_models:
                    logger.info(f"ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {original_model} -> {fallback}")
                    return fallback
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        default = self.model_mapping.get("default")
        if default in self.available_models:
            logger.info(f"ğŸ”„ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä½¿ç”¨: {default}")
            return default
        
        logger.error("âŒ åˆ©ç”¨å¯èƒ½ãªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
        return None
    
    def generate_with_ollama(self, 
                           model: str, 
                           prompt: str, 
                           max_tokens: int = 512,
                           temperature: float = 0.7) -> Optional[str]:
        """Ollamaã‚’ä½¿ç”¨ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
        try:
            response = requests.post(
                f"{self.base_url}/api/generate",
                json={
                    "model": model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "num_predict": max_tokens,
                        "temperature": temperature,
                        "top_p": 0.9,
                    }
                },
                timeout=120
            )
            
            if response.status_code == 200:
                result = response.json()
                return result.get("response", "")
            else:
                logger.error(f"Ollamaç”Ÿæˆã‚¨ãƒ©ãƒ¼: {response.status_code}")
                return None
                
        except Exception as e:
            logger.error(f"Ollamaç”Ÿæˆä¾‹å¤–: {e}")
            return None
    
    def setup_model_files(self):
        """ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¨­å®š"""
        modelfiles_dir = "/workspace/ollama_modelfiles"
        os.makedirs(modelfiles_dir, exist_ok=True)
        
        # æ—¥æœ¬èªå¯¾å¿œModelfile
        japanese_modelfile = '''FROM llama3.2:3b

TEMPLATE """[INST] <<SYS>>
You are a helpful and capable Japanese language assistant.
<</SYS>>

{{ .Prompt }} [/INST]"""

PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER num_predict 1024
PARAMETER stop "[/INST]"
PARAMETER stop "</s>"

SYSTEM """You are an assistant that answers questions in Japanese politely and explains technical content clearly."""
'''
        
        # Modelfileã‚’ä¿å­˜
        with open(f"{modelfiles_dir}/japanese_assistant.modelfile", "w") as f:
            f.write(japanese_modelfile)
        
        logger.info(f"âœ… Modelfileã‚’ä½œæˆ: {modelfiles_dir}")
        
    def register_custom_models(self):
        """ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«ã‚’Ollamaã«ç™»éŒ²"""
        modelfiles = [
            ("japanese-assistant", "/workspace/ollama_modelfiles/japanese_assistant.modelfile")
        ]
        
        for model_name, modelfile_path in modelfiles:
            if os.path.exists(modelfile_path):
                try:
                    # ollama create ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œ
                    import subprocess
                    result = subprocess.run(
                        ["ollama", "create", model_name, "-f", modelfile_path],
                        capture_output=True,
                        text=True,
                        timeout=30
                    )
                    if result.returncode == 0:
                        logger.info(f"âœ… ãƒ¢ãƒ‡ãƒ«ç™»éŒ²æˆåŠŸ: {model_name}")
                    else:
                        logger.warning(f"âš ï¸ ãƒ¢ãƒ‡ãƒ«ç™»éŒ²å¤±æ•—: {model_name} - {result.stderr}")
                except Exception as e:
                    logger.error(f"âŒ ãƒ¢ãƒ‡ãƒ«ç™»éŒ²ã‚¨ãƒ©ãƒ¼: {e}")

def integrate_with_app():
    """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã¨ã®çµ±åˆè¨­å®š"""
    config = {
        "ollama_integration": {
            "enabled": True,
            "base_url": "http://localhost:11434",
            "fallback_on_oom": True,  # OOMæ™‚ã«è‡ªå‹•ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            "models": {
                "primary": "llama3.2:3b",
                "fallback": "llama3.2:3b",
                "custom": ["japanese-assistant"]
            },
            "memory_threshold_gb": 8,  # ã“ã®é–¾å€¤ä»¥ä¸‹ã§Ollamaã‚’ä½¿ç”¨
            "auto_select": True  # è‡ªå‹•ãƒ¢ãƒ‡ãƒ«é¸æŠ
        }
    }
    
    # è¨­å®šã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    config_path = "/workspace/config/ollama_config.json"
    with open(config_path, "w") as f:
        json.dump(config, f, indent=2, ensure_ascii=False)
    
    logger.info(f"âœ… Ollamaçµ±åˆè¨­å®šã‚’ä¿å­˜: {config_path}")
    return config

if __name__ == "__main__":
    # Ollamaãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’åˆæœŸåŒ–
    manager = OllamaModelManager()
    
    # ã‚µãƒ¼ãƒ“ã‚¹çŠ¶æ…‹ã‚’ç¢ºèª
    if manager.check_ollama_status():
        print("âœ… Ollamaã‚µãƒ¼ãƒ“ã‚¹ãŒæ­£å¸¸ã«å‹•ä½œã—ã¦ã„ã¾ã™")
        print(f"ğŸ“¦ åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«: {list(manager.available_models.keys())}")
        
        # Modelfileã‚’è¨­å®š
        manager.setup_model_files()
        
        # ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³çµ±åˆè¨­å®š
        config = integrate_with_app()
        print(f"âš™ï¸ çµ±åˆè¨­å®šå®Œäº†: {config}")
        
        # ãƒ†ã‚¹ãƒˆç”Ÿæˆ
        test_prompt = "æ—¥æœ¬ã®é¦–éƒ½ã¯ã©ã“ã§ã™ã‹ï¼Ÿ"
        response = manager.generate_with_ollama("llama3.2:3b", test_prompt, max_tokens=100)
        if response:
            print(f"\nğŸ“ ãƒ†ã‚¹ãƒˆç”Ÿæˆçµæœ:\n{response}")
    else:
        print("âŒ Ollamaã‚µãƒ¼ãƒ“ã‚¹ã«æ¥ç¶šã§ãã¾ã›ã‚“")