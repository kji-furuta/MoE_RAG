#!/usr/bin/env python3
"""
QLoRA to Ollamaå¤‰æ›ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
QLoRAã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’Ollamaã§ä½¿ç”¨å¯èƒ½ã«ã™ã‚‹

ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼:
1. LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã¨ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’FP16ã§ãƒãƒ¼ã‚¸
2. ãƒãƒ¼ã‚¸ãƒ¢ãƒ‡ãƒ«ã‚’GGUFå½¢å¼ã«å¤‰æ›
3. Q4_K_Må½¢å¼ã§é‡å­åŒ–
4. Ollamaã«ç™»éŒ²
"""

import os
import sys
import subprocess
import torch
import gc
import shutil
from pathlib import Path
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import json

# AI_FT_7ã®ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã‚’ä½¿ç”¨
sys.path.append('/workspace')
from app.memory_optimized_loader import get_optimal_quantization_config

# llama.cppã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆæ—¢å­˜ã®ã‚‚ã®ã‚’å„ªå…ˆä½¿ç”¨ï¼‰
if os.path.exists("/workspace/llama.cpp"):
    LLAMA_CPP_DIR = "/workspace/llama.cpp"
else:
    LLAMA_CPP_DIR = "/tmp/llama.cpp"

def run_command(cmd):
    """ã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œã¨çµæœè¡¨ç¤º"""
    print(f"å®Ÿè¡Œä¸­: {cmd}")
    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
    
    if result.stdout:
        print(result.stdout)
    if result.stderr and result.returncode != 0:
        print(f"ã‚¨ãƒ©ãƒ¼: {result.stderr}")
    
    return result.returncode == 0

def find_lora_adapter():
    """æœ€æ–°ã®LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’æ¢ã™"""
    import glob
    import tarfile
    import tempfile
    
    lora_dirs = []
    
    # å¯èƒ½ãªLoRAãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒã‚§ãƒƒã‚¯
    possible_dirs = [
        "/workspace/outputs/lora_*",
        "/workspace/outputs/*/final_lora_model",
        "/workspace/outputs/*/best_lora_model",
        "/workspace/outputs/continual_task_*/lora"
    ]
    
    for pattern in possible_dirs:
        paths = glob.glob(pattern)
        for path in paths:
            # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å ´åˆ
            if os.path.isdir(path):
                # adapter_config.jsonãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
                if os.path.exists(os.path.join(path, "adapter_config.json")):
                    lora_dirs.append(path)
            # tar.gzãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆ
            elif path.endswith('.tar.gz'):
                # è§£å‡å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
                extract_dir = path.replace('.tar.gz', '_extracted')
                
                # æ—¢ã«è§£å‡æ¸ˆã¿ã®å ´åˆã¯ãã‚Œã‚’ä½¿ç”¨
                if os.path.exists(extract_dir) and os.path.exists(os.path.join(extract_dir, "adapter_config.json")):
                    lora_dirs.append(extract_dir)
                else:
                    # è§£å‡ãŒå¿…è¦
                    print(f"ğŸ“¦ åœ§ç¸®ã•ã‚ŒãŸLoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’è§£å‡ä¸­: {os.path.basename(path)}")
                    try:
                        os.makedirs(extract_dir, exist_ok=True)
                        with tarfile.open(path, 'r:gz') as tar:
                            tar.extractall(extract_dir)
                        
                        # adapter_config.jsonã®å­˜åœ¨ç¢ºèª
                        if os.path.exists(os.path.join(extract_dir, "adapter_config.json")):
                            print(f"âœ… è§£å‡æˆåŠŸ: {extract_dir}")
                            lora_dirs.append(extract_dir)
                        else:
                            print(f"âš ï¸ adapter_config.jsonãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {extract_dir}")
                    except Exception as e:
                        print(f"âŒ è§£å‡ã‚¨ãƒ©ãƒ¼: {e}")
    
    if not lora_dirs:
        print("âŒ æœ‰åŠ¹ãªLoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        print("   æ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³:")
        for pattern in possible_dirs:
            print(f"   - {pattern}")
        return None
    
    # æœ€æ–°ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’é¸æŠï¼ˆä½œæˆæ™‚åˆ»é †ï¼‰
    lora_dirs.sort(key=lambda x: os.path.getmtime(x), reverse=True)
    selected_dir = lora_dirs[0]
    
    print(f"âœ… LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼æ¤œå‡º: {selected_dir}")
    return selected_dir

def get_base_model_from_adapter(lora_path):
    """LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‹ã‚‰å…ƒã®ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—"""
    config_path = os.path.join(lora_path, "adapter_config.json")
    
    if not os.path.exists(config_path):
        print("âŒ adapter_config.jsonãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return None
    
    with open(config_path, 'r') as f:
        config = json.load(f)
    
    base_model = config.get("base_model_name_or_path")
    if not base_model:
        print("âŒ ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return None
    
    print(f"âœ… ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«: {base_model}")
    return base_model

def merge_lora_to_fp16(lora_path, base_model_name, output_path):
    """LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’FP16ã§ãƒãƒ¼ã‚¸"""
    
    print("\n" + "="*60)
    print("LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®FP16ãƒãƒ¼ã‚¸")
    print("="*60)
    
    # æ—¢å­˜ã®FP16ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
    if os.path.exists(f"{output_path}/model.safetensors.index.json") or \
       os.path.exists(f"{output_path}/pytorch_model.bin.index.json"):
        print("âœ… FP16ãƒãƒ¼ã‚¸æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒæ—¢ã«å­˜åœ¨ã—ã¾ã™")
        return True
    
    print("\n1. ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
    
    # 32Bãƒ¢ãƒ‡ãƒ«ã®å ´åˆã¯4bité‡å­åŒ–ã§ãƒ­ãƒ¼ãƒ‰ï¼ˆãƒ¡ãƒ¢ãƒªåˆ¶ç´„ã®ãŸã‚ï¼‰
    model_size_gb = 64  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼ˆ32Bãƒ¢ãƒ‡ãƒ«æƒ³å®šï¼‰
    use_4bit = True  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§4bitä½¿ç”¨
    
    # ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã‚’æ¨å®š
    if "7b" in base_model_name.lower() or "8b" in base_model_name.lower():
        model_size_gb = 14
        use_4bit = False  # å°ã•ã„ãƒ¢ãƒ‡ãƒ«ã¯FP16å¯èƒ½
    elif "14b" in base_model_name.lower() or "13b" in base_model_name.lower():
        model_size_gb = 28
        use_4bit = False  # ä¸­è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã‚‚FP16å¯èƒ½
    elif "22b" in base_model_name.lower():
        model_size_gb = 44
        use_4bit = True  # å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã¯4bitå¿…é ˆ
    elif "32b" in base_model_name.lower():
        model_size_gb = 64
        use_4bit = True  # 32Bã¯å¿…ãš4bit
    
    if use_4bit:
        print(f"   å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ï¼ˆæ¨å®š{model_size_gb}GBï¼‰ã®ãŸã‚4bité‡å­åŒ–ã§ãƒ­ãƒ¼ãƒ‰")
        print("   æ³¨æ„: 4bitãƒãƒ¼ã‚¸ã¯ç²¾åº¦ãŒä½ä¸‹ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
        
        from transformers import BitsAndBytesConfig
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4"
        )
        
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            quantization_config=bnb_config,
            device_map="auto",
            trust_remote_code=True,
            low_cpu_mem_usage=True,
            offload_folder="/tmp/offload"
        )
        print("âœ… 4bité‡å­åŒ–ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰æˆåŠŸ")
        print("âš ï¸ è­¦å‘Š: 4bitã§ã®ãƒãƒ¼ã‚¸ã¯æ¨è«–æ™‚ã«è‹¥å¹²ã®ç²¾åº¦ä½ä¸‹ãŒç™ºç”Ÿã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
    else:
        print(f"   ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºï¼ˆæ¨å®š{model_size_gb}GBï¼‰ã¯FP16ã§ãƒ­ãƒ¼ãƒ‰å¯èƒ½")
        
        try:
            base_model = AutoModelForCausalLM.from_pretrained(
                base_model_name,
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True,
                low_cpu_mem_usage=True,
                offload_folder="/tmp/offload"
            )
            print("âœ… FP16ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ FP16ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
            print("ãƒ¡ãƒ¢ãƒªä¸è¶³ã®ãŸã‚4bitã§ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™")
            
            from transformers import BitsAndBytesConfig
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4"
            )
            
            base_model = AutoModelForCausalLM.from_pretrained(
                base_model_name,
                quantization_config=bnb_config,
                device_map="auto",
                trust_remote_code=True,
                low_cpu_mem_usage=True,
                offload_folder="/tmp/offload"
            )
            print("âœ… 4bité‡å­åŒ–ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰æˆåŠŸï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰")
    
    print("\n2. LoRAã‚¢ãƒ€ãƒ—ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
    model = PeftModel.from_pretrained(
        base_model,
        lora_path,
        torch_dtype=torch.float16
    )
    
    print("\n3. LoRAã‚’ãƒãƒ¼ã‚¸ä¸­...")
    model = model.merge_and_unload()
    
    print("\n4. FP16å½¢å¼ã§ä¿å­˜ä¸­...")
    print("   é‡è¦: é‡å­åŒ–ã›ãšã«FP16ã§ä¿å­˜")
    os.makedirs(output_path, exist_ok=True)
    
    # é‡å­åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’FP16ã«å¤‰æ›ã—ã¦ã‹ã‚‰ä¿å­˜
    # 4bitã§ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸå ´åˆã€ãƒ¢ãƒ‡ãƒ«ã‚’å†æ§‹ç¯‰
    if hasattr(model, 'is_quantized') or 'bnb' in str(type(model)).lower():
        print("   âš ï¸ 4bitãƒ¢ãƒ‡ãƒ«ã‚’FP16ã«å¤‰æ›ä¸­...")
        print("   æ³¨æ„: ã“ã‚Œã«ã¯è¿½åŠ ã®ãƒ¡ãƒ¢ãƒªãŒå¿…è¦ã§ã™")
        
        # ãƒ¢ãƒ‡ãƒ«ã‚’CPUã«ç§»å‹•ã—ã¦FP16ã«å¤‰æ›
        try:
            model = model.to(torch.float16)
        except Exception as e:
            print(f"   âš ï¸ ç›´æ¥å¤‰æ›å¤±æ•—: {e}")
            print("   ä»£æ›¿æ–¹æ³•: state_dictã®ã¿ä¿å­˜")
    
    # FP16ã§ä¿å­˜ï¼ˆé‡å­åŒ–ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãªã—ï¼‰
    print("   ä¿å­˜å½¢å¼: safetensorsï¼ˆFP16ï¼‰")
    model.save_pretrained(
        output_path,
        torch_dtype=torch.float16,
        safe_serialization=True,
        max_shard_size="2GB",
        offload_state_dict=True  # ãƒ¡ãƒ¢ãƒªç¯€ç´„
    )
    
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã‚‚ä¿å­˜
    tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)
    tokenizer.save_pretrained(output_path)
    
    # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    del model, base_model
    gc.collect()
    torch.cuda.empty_cache()
    
    print(f"âœ… FP16ãƒãƒ¼ã‚¸ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†: {output_path}")
    return True

def setup_llama_cpp():
    """llama.cppã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆãƒ“ãƒ«ãƒ‰æ¸ˆã¿ã®ã‚‚ã®ã‚’ä½¿ç”¨ï¼‰"""
    
    global LLAMA_CPP_DIR
    
    # æ—¢å­˜ã®llama.cppã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ“ãƒ«ãƒ‰æ¸ˆã¿ã‹ã‚‚ç¢ºèªï¼‰
    if os.path.exists("/workspace/llama.cpp/build/bin/llama-quantize"):
        print("âœ… /workspace/llama.cppã¯æ—¢ã«ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ¸ˆã¿ï¼ˆãƒ“ãƒ«ãƒ‰æ¸ˆã¿ï¼‰")
        LLAMA_CPP_DIR = "/workspace/llama.cpp"
        return True
    elif os.path.exists("/tmp/llama.cpp/build/bin/llama-quantize"):
        print("âœ… /tmp/llama.cppã¯æ—¢ã«ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ¸ˆã¿ï¼ˆãƒ“ãƒ«ãƒ‰æ¸ˆã¿ï¼‰")
        LLAMA_CPP_DIR = "/tmp/llama.cpp"
        return True
    
    print("\n" + "="*60)
    print("âš ï¸ llama.cppãŒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã•ã‚Œã¦ã„ã¾ã›ã‚“")
    print("="*60)
    print("\nä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’åˆ¥ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼š")
    print("\n  docker exec -it ai-ft-container bash")
    print("  /workspace/scripts/setup_llama_cpp_standalone.sh")
    print("\nã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å¾Œã€å†åº¦é‡å­åŒ–ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    print("="*60)
    
    # ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å­˜åœ¨ã‚’ç¢ºèª
    setup_script = "/workspace/scripts/setup_llama_cpp_standalone.sh"
    if not os.path.exists(setup_script):
        print("\nâŒ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        print("æ‰‹å‹•ã§llama.cppã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã—ã¦ãã ã•ã„")
    else:
        # è‡ªå‹•å®Ÿè¡Œã‚’è©¦ã¿ã‚‹ï¼ˆãŸã ã—Webã‚µãƒ¼ãƒãƒ¼ã«å½±éŸ¿ã—ãªã„ã‚ˆã†ã«æ³¨æ„ï¼‰
        print("\nè‡ªå‹•ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚’è©¦ã¿ã¾ã™...")
        print("æ³¨æ„: ã“ã‚Œã«ã¯æ•°åˆ†ã‹ã‹ã‚Šã¾ã™")
        
        # ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œå¯èƒ½ã«ã™ã‚‹
        os.chmod(setup_script, 0o755)
        
        # ã‚µãƒ–ãƒ—ãƒ­ã‚»ã‚¹ã¨ã—ã¦å®Ÿè¡Œï¼ˆã‚·ã‚§ãƒ«ã‚’ä½¿ã‚ãªã„ï¼‰
        result = subprocess.run(
            ["/bin/bash", setup_script],
            capture_output=True,
            text=True,
            timeout=600  # 10åˆ†ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
        )
        
        if result.returncode == 0:
            print("âœ… llama.cppã®è‡ªå‹•ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãŒå®Œäº†ã—ã¾ã—ãŸ")
            LLAMA_CPP_DIR = "/workspace/llama.cpp"
            return True
        else:
            print(f"âŒ è‡ªå‹•ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã«å¤±æ•—ã—ã¾ã—ãŸ")
            print(f"ã‚¨ãƒ©ãƒ¼: {result.stderr}")
            return False
    
    return False

def convert_to_gguf(model_path, output_file):
    """FP16ãƒ¢ãƒ‡ãƒ«ã‚’GGUFå½¢å¼ã«å¤‰æ›"""
    
    print("\n" + "="*60)
    print("GGUFå¤‰æ›")
    print("="*60)
    
    if os.path.exists(output_file):
        print(f"âœ… GGUFæ—¢ã«å­˜åœ¨: {output_file}")
        return True
    
    print("GGUFå¤‰æ›ä¸­...")
    print(f"ä½¿ç”¨ã™ã‚‹llama.cpp: {LLAMA_CPP_DIR}")
    
    # å¿…è¦ãªä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
    print("ä¾å­˜é–¢ä¿‚ã‚’ç¢ºèªä¸­...")
    if not run_command("pip install -q gguf sentencepiece protobuf"):
        print("âš ï¸ ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã«å¤±æ•—ã—ã¾ã—ãŸãŒç¶šè¡Œã—ã¾ã™")
    
    # convert_hf_to_gguf.pyã®å­˜åœ¨ç¢ºèª
    convert_script = f"{LLAMA_CPP_DIR}/convert_hf_to_gguf.py"
    if not os.path.exists(convert_script):
        print(f"âŒ {convert_script}ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        # ä»£æ›¿ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ç¢ºèª
        alt_script = f"{LLAMA_CPP_DIR}/convert-hf-to-gguf.py"
        if os.path.exists(alt_script):
            print(f"ğŸ“Œ ä»£æ›¿ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½¿ç”¨: {alt_script}")
            convert_script = alt_script
        else:
            print("âŒ GGUFå¤‰æ›ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return False
    
    # convert_hf_to_gguf.pyã‚’ä½¿ç”¨
    cmd = f"cd {LLAMA_CPP_DIR} && python {os.path.basename(convert_script)} {model_path} --outfile {output_file} --outtype f16"
    
    print(f"å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰è©³ç´°:")
    print(f"  ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {LLAMA_CPP_DIR}")
    print(f"  ã‚¹ã‚¯ãƒªãƒ—ãƒˆ: {os.path.basename(convert_script)}")
    print(f"  å…¥åŠ›ãƒ¢ãƒ‡ãƒ«: {model_path}")
    print(f"  å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {output_file}")
    
    if not run_command(cmd):
        print("âŒ GGUFå¤‰æ›å¤±æ•—")
        print("\nãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°:")
        print("1. llama.cppãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ç¢ºèª:")
        run_command(f"ls -la {LLAMA_CPP_DIR}/convert*.py 2>/dev/null | head -5")
        print("2. ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ç¢ºèª:")
        run_command(f"ls -la {model_path}/ 2>/dev/null | head -5")
        return False
    
    print(f"âœ… GGUFå¤‰æ›æˆåŠŸ: {output_file}")
    return True

def quantize_gguf(input_file, output_file, quant_type="Q4_K_M"):
    """GGUFãƒ•ã‚¡ã‚¤ãƒ«ã‚’é‡å­åŒ–"""
    
    print("\n" + "="*60)
    print(f"é‡å­åŒ– ({quant_type})")
    print("="*60)
    
    if os.path.exists(output_file):
        print(f"âœ… é‡å­åŒ–æ¸ˆã¿: {output_file}")
        return True
    
    print(f"{quant_type}å½¢å¼ã§é‡å­åŒ–ä¸­...")
    
    # llama-quantizeã‚’ä½¿ç”¨ï¼ˆ/tmpã®llama.cppã‚’ä½¿ç”¨ï¼‰
    cmd = f"{LLAMA_CPP_DIR}/build/bin/llama-quantize {input_file} {output_file} {quant_type}"
    
    if not run_command(cmd):
        print("âŒ é‡å­åŒ–å¤±æ•—")
        return False
    
    print(f"âœ… é‡å­åŒ–æˆåŠŸ: {output_file}")
    return True

def create_modelfile(gguf_path, output_path):
    """Ollamaç”¨ã®Modelfileã‚’ä½œæˆ"""
    
    print("\nModelfileä½œæˆä¸­...")
    
    # GGUFãƒ•ã‚¡ã‚¤ãƒ«åã‚’å–å¾—ï¼ˆç›¸å¯¾ãƒ‘ã‚¹ç”¨ï¼‰
    gguf_filename = os.path.basename(gguf_path)
    
    modelfile_content = f'''FROM ./{gguf_filename}

# æ—¥æœ¬èªå¯¾å¿œDeepSeekãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ï¼‰
PARAMETER stop "<|im_start|>"
PARAMETER stop "<|im_end|>"
PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER repeat_penalty 1.1

TEMPLATE """{{{{ if .System }}}}<|im_start|>system
{{{{ .System }}}}<|im_end|>
{{{{ end }}}}<|im_start|>user
{{{{ .Prompt }}}}<|im_end|>
<|im_start|>assistant
"""
'''
    
    with open(output_path, 'w') as f:
        f.write(modelfile_content)
    
    print(f"âœ… Modelfileä½œæˆ: {output_path}")
    print(f"   GGUFãƒ•ã‚¡ã‚¤ãƒ«: {gguf_filename}")
    return True

def register_with_ollama(modelfile_path, model_name="deepseek-finetuned"):
    """Ollamaã«ãƒ¢ãƒ‡ãƒ«ã‚’ç™»éŒ²"""
    
    print("\n" + "="*60)
    print("Ollamaç™»éŒ²")
    print("="*60)
    
    # Ollamaã‚µãƒ¼ãƒ“ã‚¹ãŒèµ·å‹•ã—ã¦ã„ã‚‹ã‹ç¢ºèª
    if not run_command("curl -s http://localhost:11434/api/tags > /dev/null 2>&1"):
        print("âš ï¸ Ollamaã‚µãƒ¼ãƒ“ã‚¹ãŒèµ·å‹•ã—ã¦ã„ã¾ã›ã‚“")
        print("Ollamaã‚µãƒ¼ãƒ“ã‚¹ã‚’è‡ªå‹•èµ·å‹•ä¸­...")
        
        # Ollamaã‚’è‡ªå‹•èµ·å‹•
        if run_command("nohup ollama serve > /dev/null 2>&1 &"):
            print("Ollamaã‚µãƒ¼ãƒ“ã‚¹èµ·å‹•ã‚’å¾…æ©Ÿä¸­...")
            import time
            time.sleep(5)  # èµ·å‹•å¾…æ©Ÿ
            
            # å†åº¦ç¢ºèª
            if not run_command("curl -s http://localhost:11434/api/tags > /dev/null 2>&1"):
                print("âŒ Ollamaã‚µãƒ¼ãƒ“ã‚¹ã®èµ·å‹•ã«å¤±æ•—ã—ã¾ã—ãŸ")
                print("æ‰‹å‹•ã§ 'ollama serve' ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
                return False
            print("âœ… Ollamaã‚µãƒ¼ãƒ“ã‚¹ãŒèµ·å‹•ã—ã¾ã—ãŸ")
        else:
            print("âŒ Ollamaã‚µãƒ¼ãƒ“ã‚¹ã®è‡ªå‹•èµ·å‹•ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return False
    
    print(f"ãƒ¢ãƒ‡ãƒ«å '{model_name}' ã¨ã—ã¦ç™»éŒ²ä¸­...")
    
    # ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¨ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ã‚’ç¢ºèª
    model_dir = os.path.dirname(modelfile_path)
    modelfile_name = os.path.basename(modelfile_path)
    
    if not os.path.exists(modelfile_path):
        print(f"âŒ ModelfileãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {modelfile_path}")
        return False
    
    # GGUFãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ã‚‚ç¢ºèª
    with open(modelfile_path, 'r') as f:
        content = f.read()
        # FROMè¡Œã‹ã‚‰GGUFãƒ•ã‚¡ã‚¤ãƒ«åã‚’æŠ½å‡º
        import re
        match = re.search(r'FROM\s+\.?/?(.+\.gguf)', content)
        if match:
            gguf_file = match.group(1)
            gguf_path = os.path.join(model_dir, os.path.basename(gguf_file))
            if not os.path.exists(gguf_path):
                print(f"âŒ GGUFãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {gguf_path}")
                return False
            print(f"âœ… GGUFãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª: {os.path.basename(gguf_path)}")
    
    # ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’é•·ãã™ã‚‹ï¼‰
    cmd = f"cd {model_dir} && timeout 600 ollama create {model_name} -f {modelfile_name}"
    
    print("æ³¨æ„: ãƒ¢ãƒ‡ãƒ«ç™»éŒ²ã«ã¯æ•°åˆ†ã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™...")
    if not run_command(cmd):
        print("âŒ Ollamaç™»éŒ²å¤±æ•—")
        print("ãƒ‡ãƒãƒƒã‚°æƒ…å ±:")
        print(f"  ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {model_dir}")
        print(f"  Modelfile: {modelfile_name}")
        # ã‚¨ãƒ©ãƒ¼ã®è©³ç´°ã‚’ç¢ºèª
        run_command(f"cd {model_dir} && ls -la")
        return False
    
    print(f"âœ… Ollamaç™»éŒ²æˆåŠŸ: {model_name}")
    
    # ç™»éŒ²ç¢ºèª
    run_command("ollama list")
    
    return True

def update_rag_config(model_name="deepseek-finetuned"):
    """RAGã‚·ã‚¹ãƒ†ãƒ ã®è¨­å®šã‚’æ›´æ–°"""
    
    config_path = "/workspace/src/rag/config/rag_config.yaml"
    
    if not os.path.exists(config_path):
        print(f"âš ï¸ RAGè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {config_path}")
        return False
    
    print(f"\nRAGè¨­å®šã‚’æ›´æ–°ä¸­: {model_name}")
    
    # YAMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°
    import yaml
    
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    
    # Ollamaãƒ¢ãƒ‡ãƒ«ã‚’è¨­å®š
    config['llm']['provider'] = 'ollama'
    config['llm']['ollama'] = {
        'model': model_name,
        'base_url': 'http://localhost:11434',
        'temperature': 0.7,
        'top_p': 0.9
    }
    
    with open(config_path, 'w') as f:
        yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
    
    print("âœ… RAGè¨­å®šæ›´æ–°å®Œäº†")
    return True

def main():
    """ãƒ¡ã‚¤ãƒ³ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
    
    print("="*60)
    print("QLoRA â†’ Ollama å¤‰æ›ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³")
    print("="*60)
    
    # 1. LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’æ¢ã™
    lora_path = find_lora_adapter()
    if not lora_path:
        return 1
    
    # 2. ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—
    base_model_name = get_base_model_from_adapter(lora_path)
    if not base_model_name:
        return 1
    
    # 3. å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æº–å‚™
    output_base = "/workspace/outputs/ollama_conversion"
    os.makedirs(output_base, exist_ok=True)
    
    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’è¿½åŠ ã—ã¦ä¸€æ„ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã™ã‚‹
    from datetime import datetime
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    fp16_model_path = f"{output_base}/merged_model_fp16_{timestamp}"
    gguf_f16_path = f"{output_base}/model-f16_{timestamp}.gguf"
    gguf_q4_path = f"{output_base}/deepseek-finetuned-q4_k_m_{timestamp}.gguf"
    modelfile_path = f"{output_base}/Modelfile_{timestamp}"
    
    # 4. llama.cppã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
    if not setup_llama_cpp():
        return 1
    
    # 5. LoRAã‚’FP16ã§ãƒãƒ¼ã‚¸
    if not merge_lora_to_fp16(lora_path, base_model_name, fp16_model_path):
        return 1
    
    # 6. GGUFå¤‰æ›
    if not convert_to_gguf(fp16_model_path, gguf_f16_path):
        return 1
    
    # 7. é‡å­åŒ–
    if not quantize_gguf(gguf_f16_path, gguf_q4_path):
        return 1
    
    # 8. Modelfileä½œæˆ
    if not create_modelfile(gguf_q4_path, modelfile_path):
        return 1
    
    # 9. Ollamaç™»éŒ²
    if not register_with_ollama(modelfile_path):
        return 1
    
    # 10. RAGè¨­å®šæ›´æ–°
    if not update_rag_config():
        return 1
    
    print("\n" + "="*60)
    print("âœ… å®Œäº†ï¼")
    print("="*60)
    print("\nRAGã‚·ã‚¹ãƒ†ãƒ ã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒä½¿ç”¨å¯èƒ½ã«ãªã‚Šã¾ã—ãŸ")
    print("RAGã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã§è³ªå•å¿œç­”ã‚’ãƒ†ã‚¹ãƒˆã—ã¦ãã ã•ã„")
    
    return 0

if __name__ == "__main__":
    sys.exit(main())