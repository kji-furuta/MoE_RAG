#!/usr/bin/env python3
"""
RAGã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‡ãƒ¼ã‚¿æ°¸ç¶šæ€§ã‚’æ”¹å–„ã™ã‚‹åŒ…æ‹¬çš„ãªã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆä¿®æ­£ç‰ˆv3ï¼‰
"""

import os
import sys
import json
import sqlite3
import shutil
from pathlib import Path
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
import hashlib
import logging

# ãƒ‘ã‚¹ã®è¨­å®šã‚’ä¿®æ­£
current_dir = Path(__file__).parent.parent.parent
sys.path.insert(0, str(current_dir))

# ç’°å¢ƒã«å¿œã˜ãŸãƒ‘ã‚¹è¨­å®š
if os.path.exists("/workspace"):
    sys.path.insert(0, "/workspace")
elif os.path.exists("/home/kjifu/MoE_RAG"):
    sys.path.insert(0, "/home/kjifu/MoE_RAG")

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class PersistentVectorStore:
    """æ°¸ç¶šæ€§ã‚’å¼·åŒ–ã—ãŸãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢"""
    
    def __init__(self, base_path: str = "./data/rag_persistent"):
        self.base_path = Path(base_path)
        self.base_path.mkdir(parents=True, exist_ok=True)
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã®åˆæœŸåŒ–
        self.vector_path = self.base_path / "vectors"
        self.metadata_path = self.base_path / "metadata"
        self.backup_path = self.base_path / "backups"
        self.checkpoint_path = self.base_path / "checkpoints"
        
        for path in [self.vector_path, self.metadata_path, 
                    self.backup_path, self.checkpoint_path]:
            path.mkdir(parents=True, exist_ok=True)
        
        # æ°¸ç¶šåŒ–DBåˆæœŸåŒ–
        self.db_path = self.metadata_path / "persistent_store.db"
        self._init_database()
        
        # Qdrantã‚¹ãƒˆã‚¢ã®åˆæœŸåŒ–
        self._init_vector_store()
        
        logger.info(f"PersistentVectorStore initialized at {self.base_path}")
    
    def _init_database(self):
        """æ°¸ç¶šåŒ–ç”¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®åˆæœŸåŒ–"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ†ãƒ¼ãƒ–ãƒ«
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS documents (
                id TEXT PRIMARY KEY,
                title TEXT NOT NULL,
                content TEXT NOT NULL,
                file_path TEXT,
                file_hash TEXT UNIQUE,
                embedding_id TEXT,
                vector_dims INTEGER,
                category TEXT,
                subcategory TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                indexed_at TIMESTAMP,
                status TEXT DEFAULT 'pending',
                metadata TEXT
            )
        ''')
        
        # ãƒ™ã‚¯ãƒˆãƒ«ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ç”¨ï¼‰
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS vectors (
                id TEXT PRIMARY KEY,
                document_id TEXT,
                vector_data BLOB,
                dimensions INTEGER,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (document_id) REFERENCES documents(id)
            )
        ''')
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ†ãƒ¼ãƒ–ãƒ«
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS index_status (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                collection_name TEXT,
                total_documents INTEGER,
                indexed_documents INTEGER,
                failed_documents INTEGER,
                last_update TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                status TEXT,
                error_log TEXT
            )
        ''')
        
        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å±¥æ­´ãƒ†ãƒ¼ãƒ–ãƒ«
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS backup_history (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                backup_name TEXT UNIQUE,
                backup_path TEXT,
                backup_size INTEGER,
                document_count INTEGER,
                vector_count INTEGER,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                status TEXT,
                restore_count INTEGER DEFAULT 0
            )
        ''')
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_doc_status ON documents(status)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_doc_category ON documents(category)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_vec_doc_id ON vectors(document_id)')
        
        conn.commit()
        conn.close()
        
        logger.info("Persistent database initialized")
    
    def _init_vector_store(self):
        """Qdrantãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®åˆæœŸåŒ–ï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä»˜ãï¼‰"""
        try:
            from src.rag.indexing.vector_store import QdrantVectorStore
            
            self.vector_store = QdrantVectorStore(
                collection_name="persistent_docs",
                embedding_dim=1024,
                path=str(self.vector_path)
            )
            
            # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®çŠ¶æ…‹ã‚’ç¢ºèª
            try:
                info = self.vector_store.get_collection_info()
                logger.info(f"Vector store initialized: {info.get('vectors_count', 0)} vectors")
            except Exception as e:
                logger.warning(f"Vector store initialization warning: {e}")
                self.vector_store._ensure_collection()
                
        except ImportError as e:
            logger.warning(f"Could not import QdrantVectorStore, using fallback: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè£…
            self.vector_store = None
    
    def _calculate_hash(self, text: str) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¨ˆç®—"""
        return hashlib.sha256(text.encode()).hexdigest()
    
    def add_document_with_persistence(self, 
                                     text: str,
                                     title: str,
                                     embedding: Any,
                                     metadata: Dict[str, Any],
                                     file_path: Optional[str] = None) -> str:
        """æ–‡æ›¸ã‚’æ°¸ç¶šçš„ã«è¿½åŠ """
        import uuid
        import pickle
        
        # NumPyã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«ï¼‰
        try:
            import numpy as np
            has_numpy = True
        except ImportError:
            has_numpy = False
            logger.warning("NumPy not available, using list for embeddings")
        
        # æ–‡æ›¸ã®ãƒãƒƒã‚·ãƒ¥å€¤è¨ˆç®—ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨ï¼‰
        doc_hash = self._calculate_hash(text)
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            # é‡è¤‡ãƒã‚§ãƒƒã‚¯
            cursor.execute("SELECT id FROM documents WHERE file_hash = ?", (doc_hash,))
            existing = cursor.fetchone()
            
            if existing:
                logger.warning(f"Document already exists: {existing[0]}")
                return existing[0]
            
            # æ–°è¦æ–‡æ›¸IDç”Ÿæˆ
            doc_id = str(uuid.uuid4())
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æ–‡æ›¸ã‚’ä¿å­˜
            cursor.execute('''
                INSERT INTO documents 
                (id, title, content, file_path, file_hash, category, subcategory, metadata, status)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                doc_id,
                title,
                text,
                file_path,
                doc_hash,
                metadata.get('category', 'general'),
                metadata.get('subcategory'),
                json.dumps(metadata),
                'indexing'
            ))
            
            # ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ï¼ˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ç”¨ï¼‰
            if has_numpy and isinstance(embedding, np.ndarray):
                vector_data = pickle.dumps(embedding)
                dimensions = embedding.shape[-1]
            else:
                # ãƒªã‚¹ãƒˆã¨ã—ã¦å‡¦ç†
                if hasattr(embedding, '__len__'):
                    vector_data = pickle.dumps(list(embedding))
                    dimensions = len(embedding)
                else:
                    vector_data = pickle.dumps([embedding])
                    dimensions = 1
            
            cursor.execute('''
                INSERT INTO vectors (id, document_id, vector_data, dimensions)
                VALUES (?, ?, ?, ?)
            ''', (str(uuid.uuid4()), doc_id, vector_data, dimensions))
            
            # Qdrantã«ã‚‚è¿½åŠ ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
            if self.vector_store:
                try:
                    self.vector_store.add_documents(
                        texts=[text],
                        embeddings=[embedding],
                        metadatas=[{**metadata, 'doc_id': doc_id, 'title': title}],
                        ids=[doc_id]
                    )
                except Exception as e:
                    logger.warning(f"Failed to add to vector store: {e}")
            
            # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°
            cursor.execute('''
                UPDATE documents 
                SET status = 'indexed', indexed_at = CURRENT_TIMESTAMP, embedding_id = ?
                WHERE id = ?
            ''', (doc_id, doc_id))
            
            conn.commit()
            logger.info(f"Document persisted: {doc_id} - {title}")
            
            # è‡ªå‹•ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼ˆ10æ–‡æ›¸ã”ã¨ï¼‰
            cursor.execute("SELECT COUNT(*) FROM documents WHERE status = 'indexed'")
            count = cursor.fetchone()[0]
            if count % 10 == 0:
                self.create_backup(f"auto_backup_{count}")
            
            return doc_id
            
        except Exception as e:
            conn.rollback()
            logger.error(f"Failed to persist document: {e}")
            raise
        finally:
            conn.close()
    
    def _generate_unique_backup_name(self, base_name: str) -> str:
        """ä¸€æ„ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—åã‚’ç”Ÿæˆ"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ãƒ™ãƒ¼ã‚¹åã«ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãŒå«ã¾ã‚Œã¦ã„ãªã„å ´åˆã¯è¿½åŠ 
        if timestamp[:8] not in base_name:  # æ—¥ä»˜éƒ¨åˆ†ã‚’ãƒã‚§ãƒƒã‚¯
            unique_name = f"{base_name}_{timestamp}"
        else:
            unique_name = base_name
            
        # æ—¢å­˜ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã¨é‡è¤‡ã—ãªã„ã‹ç¢ºèª
        counter = 1
        original_name = unique_name
        while (self.backup_path / unique_name).exists():
            unique_name = f"{original_name}_{counter}"
            counter += 1
            
        return unique_name
    
    def create_backup(self, backup_name: Optional[str] = None) -> str:
        """å®Œå…¨ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®ä½œæˆï¼ˆæ”¹å–„ç‰ˆï¼‰"""
        if not backup_name:
            backup_name = f"backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # ä¸€æ„ã®åå‰ã‚’ç”Ÿæˆ
        unique_backup_name = self._generate_unique_backup_name(backup_name)
        backup_dir = self.backup_path / unique_backup_name
        
        # æ—¢å­˜ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒã‚ã‚‹å ´åˆã¯å‰Šé™¤
        if backup_dir.exists():
            logger.warning(f"Removing existing backup directory: {backup_dir}")
            shutil.rmtree(backup_dir)
        
        backup_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"Creating backup: {unique_backup_name}")
        
        try:
            # 1. SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
            if self.db_path.exists():
                shutil.copy2(self.db_path, backup_dir / "persistent_store.db")
                logger.info("  âœ… Database backed up")
            
            # 2. Qdrantãƒ‡ãƒ¼ã‚¿ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆï¼‰
            if self.vector_path.exists() and any(self.vector_path.iterdir()):
                qdrant_backup = backup_dir / "qdrant_data"
                # dirs_exist_ok=True ã‚’ä½¿ç”¨ã—ã¦æ—¢å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¨±å¯
                shutil.copytree(self.vector_path, qdrant_backup, dirs_exist_ok=True)
                logger.info("  âœ… Vector data backed up")
            else:
                logger.info("  â„¹ï¸  No vector data to backup")
            
            # 3. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # æ–‡æ›¸ä¸€è¦§ã‚’JSONå½¢å¼ã§ä¿å­˜
            cursor.execute('''
                SELECT id, title, category, file_path, created_at, status 
                FROM documents
            ''')
            documents = cursor.fetchall()
            
            metadata = {
                'backup_name': unique_backup_name,
                'original_name': backup_name,
                'created_at': datetime.now().isoformat(),
                'document_count': len(documents),
                'documents': [
                    {
                        'id': doc[0],
                        'title': doc[1],
                        'category': doc[2],
                        'file_path': doc[3],
                        'created_at': doc[4],
                        'status': doc[5]
                    }
                    for doc in documents
                ]
            }
            
            with open(backup_dir / "metadata.json", 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            logger.info(f"  âœ… Metadata exported ({len(documents)} documents)")
            
            # 4. ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æƒ…å ±ã‚’è¨˜éŒ²
            backup_size = sum(f.stat().st_size for f in backup_dir.rglob('*') if f.is_file())
            
            cursor.execute('''
                INSERT OR REPLACE INTO backup_history 
                (backup_name, backup_path, backup_size, document_count, vector_count, status)
                VALUES (?, ?, ?, ?, ?, ?)
            ''', (
                unique_backup_name,
                str(backup_dir),
                backup_size,
                len(documents),
                len(documents),  # ãƒ™ã‚¯ãƒˆãƒ«æ•°ã¯æ–‡æ›¸æ•°ã¨åŒã˜ã¨ä»®å®š
                'completed'
            ))
            
            conn.commit()
            conn.close()
            
            logger.info(f"âœ… Backup completed: {unique_backup_name} ({backup_size:,} bytes)")
            return str(backup_dir)
            
        except Exception as e:
            logger.error(f"Backup failed: {e}")
            # å¤±æ•—ã—ãŸãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å‰Šé™¤
            if backup_dir.exists():
                shutil.rmtree(backup_dir)
            raise
    
    def restore_from_backup(self, backup_name: str) -> bool:
        """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰ãƒªã‚¹ãƒˆã‚¢"""
        backup_dir = self.backup_path / backup_name
        
        if not backup_dir.exists():
            logger.error(f"Backup not found: {backup_name}")
            return False
        
        logger.info(f"Restoring from backup: {backup_name}")
        
        try:
            # 1. ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼ˆå®‰å…¨ã®ãŸã‚ï¼‰
            pre_restore_name = f"pre_restore_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            self.create_backup(pre_restore_name)
            
            # 2. SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ãƒªã‚¹ãƒˆã‚¢
            backup_db = backup_dir / "persistent_store.db"
            if backup_db.exists():
                shutil.copy2(backup_db, self.db_path)
                logger.info("  âœ… Database restored")
            
            # 3. Qdrantãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆã‚¢
            qdrant_backup = backup_dir / "qdrant_data"
            if qdrant_backup.exists():
                if self.vector_path.exists():
                    shutil.rmtree(self.vector_path)
                shutil.copytree(qdrant_backup, self.vector_path)
                logger.info("  âœ… Vector data restored")
            
            # 4. ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®å†åˆæœŸåŒ–
            self._init_vector_store()
            
            # 5. ãƒªã‚¹ãƒˆã‚¢å±¥æ­´ã‚’æ›´æ–°
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            cursor.execute('''
                UPDATE backup_history 
                SET restore_count = restore_count + 1 
                WHERE backup_name = ?
            ''', (backup_name,))
            conn.commit()
            conn.close()
            
            logger.info(f"âœ… Restore completed: {backup_name}")
            return True
            
        except Exception as e:
            logger.error(f"Restore failed: {e}")
            return False
    
    def list_backups(self) -> List[Dict[str, Any]]:
        """åˆ©ç”¨å¯èƒ½ãªãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®ãƒªã‚¹ãƒˆã‚’å–å¾—"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT backup_name, backup_path, backup_size, document_count, 
                   created_at, status, restore_count
            FROM backup_history
            ORDER BY created_at DESC
        ''')
        
        backups = []
        for row in cursor.fetchall():
            backups.append({
                'name': row[0],
                'path': row[1],
                'size': row[2],
                'documents': row[3],
                'created': row[4],
                'status': row[5],
                'restored': row[6]
            })
        
        conn.close()
        return backups
    
    def verify_persistence(self) -> Dict[str, Any]:
        """æ°¸ç¶šæ€§ã®æ¤œè¨¼"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # çµ±è¨ˆæƒ…å ±ã®åé›†
        cursor.execute("SELECT COUNT(*) FROM documents")
        total_docs = cursor.fetchone()[0]
        
        cursor.execute("SELECT COUNT(*) FROM documents WHERE status = 'indexed'")
        indexed_docs = cursor.fetchone()[0]
        
        cursor.execute("SELECT COUNT(*) FROM vectors")
        total_vectors = cursor.fetchone()[0]
        
        cursor.execute("SELECT COUNT(*) FROM backup_history")
        total_backups = cursor.fetchone()[0]
        
        # Qdrantã®ç¢ºèª
        if self.vector_store:
            try:
                vector_info = self.vector_store.get_collection_info()
                qdrant_vectors = vector_info.get('vectors_count', 0)
            except:
                qdrant_vectors = 0
        else:
            qdrant_vectors = 0
        
        conn.close()
        
        return {
            'total_documents': total_docs,
            'indexed_documents': indexed_docs,
            'total_vectors': total_vectors,
            'total_backups': total_backups,
            'qdrant_vectors': qdrant_vectors,
            'persistence_status': 'healthy' if indexed_docs > 0 else 'empty',
            'backup_status': 'available' if total_backups > 0 else 'no_backups'
        }
    
    def cleanup_old_backups(self, keep_days: int = 7):
        """å¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®å‰Šé™¤"""
        cutoff_date = datetime.now() - timedelta(days=keep_days)
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # å¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’å–å¾—
        cursor.execute('''
            SELECT backup_name, backup_path 
            FROM backup_history 
            WHERE created_at < ?
        ''', (cutoff_date.isoformat(),))
        
        old_backups = cursor.fetchall()
        
        for backup_name, backup_path in old_backups:
            backup_dir = Path(backup_path)
            if backup_dir.exists():
                shutil.rmtree(backup_dir)
                logger.info(f"Deleted old backup: {backup_name}")
            
            # å±¥æ­´ã‹ã‚‰å‰Šé™¤
            cursor.execute("DELETE FROM backup_history WHERE backup_name = ?", (backup_name,))
        
        conn.commit()
        conn.close()
        
        logger.info(f"Cleaned up {len(old_backups)} old backups")


def implement_persistence_improvements():
    """æ°¸ç¶šæ€§æ”¹å–„ã®å®Ÿè£…"""
    print("=" * 60)
    print("ğŸ”§ ãƒ‡ãƒ¼ã‚¿æ°¸ç¶šæ€§ã®æ”¹å–„å®Ÿè£…")
    print("=" * 60)
    
    # æ—¢å­˜ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    backup_path = Path("data/rag_persistent/backups")
    if backup_path.exists():
        initial_backup = backup_path / "initial_backup"
        if initial_backup.exists():
            print("\nâš ï¸  æ—¢å­˜ã®initial_backupã‚’å‰Šé™¤ã—ã¦ã„ã¾ã™...")
            shutil.rmtree(initial_backup)
            print("  âœ… å‰Šé™¤å®Œäº†")
    
    # 1. PersistentVectorStoreã®åˆæœŸåŒ–
    print("\n1. æ°¸ç¶šåŒ–ã‚¹ãƒˆã‚¢ã®åˆæœŸåŒ–")
    persistent_store = PersistentVectorStore()
    print("  âœ… åˆæœŸåŒ–å®Œäº†")
    
    # 2. ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®è¿½åŠ 
    print("\n2. ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®è¿½åŠ ")
    
    # NumPyãŒåˆ©ç”¨ã§ããªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    try:
        import numpy as np
        use_numpy = True
    except ImportError:
        use_numpy = False
        print("  âš ï¸  NumPyãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ãƒªã‚¹ãƒˆã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
    
    test_documents = [
        {
            'title': 'é“è·¯è¨­è¨ˆåŸºæº–æ›¸ ç¬¬1ç« ',
            'text': 'é“è·¯è¨­è¨ˆã«ãŠã‘ã‚‹æœ€å°æ›²ç·šåŠå¾„ã¯ã€è¨­è¨ˆé€Ÿåº¦ã«å¿œã˜ã¦æ±ºå®šã•ã‚Œã‚‹ã€‚è¨­è¨ˆé€Ÿåº¦60km/hã®å ´åˆã€æœ€å°æ›²ç·šåŠå¾„ã¯150mã¨ã™ã‚‹ã€‚',
            'category': 'è¨­è¨ˆåŸºæº–',
            'subcategory': 'å¹¾ä½•æ§‹é€ '
        },
        {
            'title': 'ç¸¦æ–­å‹¾é…ã®åˆ¶é™',
            'text': 'ç¸¦æ–­å‹¾é…ã¯åŸå‰‡ã¨ã—ã¦5%ä»¥ä¸‹ã¨ã™ã‚‹ã€‚ãŸã ã—ã€åœ°å½¢ã®çŠ¶æ³ã«ã‚ˆã‚Šã‚„ã‚€ã‚’å¾—ãªã„å ´åˆã¯8%ã¾ã§è¨±å®¹ã•ã‚Œã‚‹ã€‚',
            'category': 'è¨­è¨ˆåŸºæº–',
            'subcategory': 'ç¸¦æ–­è¨­è¨ˆ'
        },
        {
            'title': 'æ¨ªæ–­å‹¾é…ã®åŸºæº–',
            'text': 'æ¨ªæ–­å‹¾é…ã¯ã€æ’æ°´ã‚’è€ƒæ…®ã—ã¦ç‰‡å‹¾é…2%ã‚’æ¨™æº–ã¨ã™ã‚‹ã€‚æ›²ç·šéƒ¨ã«ãŠã„ã¦ã¯ã€è¶…é«˜ã‚’è¨­ã‘ã‚‹ã€‚',
            'category': 'è¨­è¨ˆåŸºæº–',
            'subcategory': 'æ¨ªæ–­è¨­è¨ˆ'
        }
    ]
    
    for doc in test_documents:
        # ãƒ€ãƒŸãƒ¼ã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«
        if use_numpy:
            import numpy as np
            embedding = np.random.randn(1024).astype(np.float32)
        else:
            import random
            embedding = [random.random() for _ in range(1024)]
        
        doc_id = persistent_store.add_document_with_persistence(
            text=doc['text'],
            title=doc['title'],
            embedding=embedding,
            metadata={
                'category': doc['category'],
                'subcategory': doc['subcategory']
            }
        )
        print(f"  âœ… è¿½åŠ : {doc['title']} (ID: {doc_id[:8]}...)")
    
    # 3. ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®ä½œæˆ
    print("\n3. ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®ä½œæˆ")
    backup_path = persistent_store.create_backup("initial_backup")
    print(f"  âœ… ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ: {backup_path}")
    
    # 4. ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒªã‚¹ãƒˆã®è¡¨ç¤º
    print("\n4. åˆ©ç”¨å¯èƒ½ãªãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—")
    backups = persistent_store.list_backups()
    for backup in backups[:5]:  # æœ€æ–°5ä»¶ã‚’è¡¨ç¤º
        print(f"  - {backup['name']}: {backup['documents']}æ–‡æ›¸, {backup['size']:,}bytes")
    
    # 5. æ°¸ç¶šæ€§ã®æ¤œè¨¼
    print("\n5. æ°¸ç¶šæ€§ã®æ¤œè¨¼")
    verification = persistent_store.verify_persistence()
    
    print(f"  ğŸ“Š æ¤œè¨¼çµæœ:")
    print(f"    ç·æ–‡æ›¸æ•°: {verification['total_documents']}")
    print(f"    ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ¸ˆã¿: {verification['indexed_documents']}")
    print(f"    ãƒ™ã‚¯ãƒˆãƒ«æ•°: {verification['total_vectors']}")
    print(f"    ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ•°: {verification['total_backups']}")
    print(f"    Qdrantãƒ™ã‚¯ãƒˆãƒ«æ•°: {verification['qdrant_vectors']}")
    print(f"    æ°¸ç¶šåŒ–çŠ¶æ…‹: {verification['persistence_status']}")
    print(f"    ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—çŠ¶æ…‹: {verification['backup_status']}")
    
    print("\nâœ… ãƒ‡ãƒ¼ã‚¿æ°¸ç¶šæ€§ã®æ”¹å–„ãŒå®Œäº†ã—ã¾ã—ãŸ")
    
    return persistent_store


if __name__ == "__main__":
    try:
        implement_persistence_improvements()
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
