#!/usr/bin/env python3
"""
æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã¨ã®çµ±åˆã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼
MetadataManagerã¨QdrantVectorStoreã‚’çµ±åˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆã‚¢ã«ç§»è¡Œ
"""

import os
import sys
import json
import sqlite3
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
import logging
import hashlib

# ãƒ‘ã‚¹è¨­å®š
current_dir = Path(__file__).parent.parent.parent
sys.path.insert(0, str(current_dir))

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

from scripts.rag_fixes.unified_metadata_store import (
    UnifiedMetadataStore,
    UnifiedDocument,
    DocumentType,
    DocumentStatus
)


class MetadataIntegrationAdapter:
    """æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã¨ã®çµ±åˆã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼"""
    
    def __init__(self):
        # çµ±åˆã‚¹ãƒˆã‚¢ã®åˆæœŸåŒ–
        self.unified_store = UnifiedMetadataStore()
        
        # æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–ï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä»˜ãï¼‰
        self._init_legacy_systems()
        
        self.migration_log = []
        
    def _init_legacy_systems(self):
        """æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–"""
        # QdrantVectorStore
        try:
            from src.rag.indexing.vector_store import QdrantVectorStore
            self.legacy_qdrant = QdrantVectorStore(
                collection_name="road_design_docs",
                embedding_dim=1024,
                path="./data/qdrant"
            )
            logger.info("Legacy Qdrant initialized")
        except Exception as e:
            logger.warning(f"Could not initialize legacy Qdrant: {e}")
            self.legacy_qdrant = None
        
        # MetadataManager
        try:
            from src.rag.indexing.metadata_manager import MetadataManager
            self.legacy_metadata_manager = MetadataManager(
                db_path="./metadata/metadata.db"
            )
            logger.info("Legacy MetadataManager initialized")
        except Exception as e:
            logger.warning(f"Could not initialize MetadataManager: {e}")
            self.legacy_metadata_manager = None
    
    def add_document(self,
                    text: str,
                    title: str,
                    embedding: Optional[Any] = None,
                    metadata: Optional[Dict[str, Any]] = None,
                    file_path: Optional[str] = None) -> str:
        """
        çµ±åˆAPIã§æ–‡æ›¸ã‚’è¿½åŠ 
        æ—¢å­˜ã®ä¸¡ã‚·ã‚¹ãƒ†ãƒ ã¨ã®äº’æ›æ€§ã‚’ç¶­æŒ
        """
        if metadata is None:
            metadata = {}
        
        # çµ±åˆæ–‡æ›¸ã®ä½œæˆ
        doc = UnifiedDocument(
            title=title,
            content=text,
            file_path=file_path,
            file_name=Path(file_path).name if file_path else None,
            document_type=DocumentType(metadata.get('document_type', 'other')),
            category=metadata.get('category', 'general'),
            subcategory=metadata.get('subcategory'),
            tags=metadata.get('tags', []),
            metadata=metadata,
            status=DocumentStatus.PENDING
        )
        
        # çµ±åˆã‚¹ãƒˆã‚¢ã«è¿½åŠ 
        if embedding is not None:
            doc_id = self.unified_store.add_document_with_vector(doc, embedding)
        else:
            doc_id = self.unified_store.add_document(doc)
        
        # æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã«ã‚‚è¿½åŠ ï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ï¼‰
        self._add_to_legacy_systems(doc, embedding)
        
        logger.info(f"Document added with unified ID: {doc_id}")
        return doc_id
    
    def _add_to_legacy_systems(self,
                              doc: UnifiedDocument,
                              embedding: Optional[Any] = None):
        """æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã«ã‚‚æ–‡æ›¸ã‚’è¿½åŠ ï¼ˆå¾Œæ–¹äº’æ›æ€§ï¼‰"""
        # Qdrantã«è¿½åŠ 
        if self.legacy_qdrant and embedding is not None:
            try:
                self.legacy_qdrant.add_documents(
                    texts=[doc.content],
                    embeddings=[embedding],
                    metadatas=[{
                        'doc_id': doc.id,
                        'title': doc.title,
                        'category': doc.category,
                        **doc.metadata
                    }],
                    ids=[doc.id]
                )
                logger.info(f"Added to legacy Qdrant: {doc.id}")
            except Exception as e:
                logger.warning(f"Failed to add to legacy Qdrant: {e}")
        
        # MetadataManagerã«è¿½åŠ 
        if self.legacy_metadata_manager:
            try:
                from src.rag.indexing.metadata_manager import DocumentMetadata
                
                legacy_metadata = DocumentMetadata(
                    id=doc.id,
                    title=doc.title,
                    filename=doc.file_name or "unknown",
                    file_path=doc.file_path or "",
                    file_hash=doc.content_hash,
                    document_type=doc.document_type,
                    category=doc.category,
                    subcategory=doc.subcategory
                )
                
                self.legacy_metadata_manager.add_metadata(legacy_metadata)
                logger.info(f"Added to legacy MetadataManager: {doc.id}")
            except Exception as e:
                logger.warning(f"Failed to add to MetadataManager: {e}")
    
    def search(self,
              query: str,
              top_k: int = 5,
              filters: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """
        çµ±åˆæ¤œç´¢API
        ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ¤œç´¢ã‚’çµ±åˆ
        """
        results = []
        
        # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ï¼ˆQdrantçµŒç”±ï¼‰
        if self.unified_store.vector_store:
            try:
                # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®å®Ÿè¡Œï¼ˆembedding ãŒå¿…è¦ï¼‰
                # ã“ã“ã§ã¯ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ¤œç´¢ã®ã¿å®Ÿè£…
                pass
            except Exception as e:
                logger.warning(f"Vector search failed: {e}")
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ¤œç´¢
        docs = self.unified_store.search_documents(
            query=query,
            category=filters.get('category') if filters else None,
            limit=top_k
        )
        
        # çµæœã®æ•´å½¢
        for doc in docs:
            results.append({
                'id': doc.id,
                'title': doc.title,
                'text': doc.content[:200] + "..." if len(doc.content) > 200 else doc.content,
                'metadata': doc.metadata,
                'category': doc.category,
                'score': 1.0  # ãƒ€ãƒŸãƒ¼ã‚¹ã‚³ã‚¢
            })
        
        return results
    
    def migrate_existing_data(self) -> Dict[str, int]:
        """æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆã‚¹ãƒˆã‚¢ã«ç§»è¡Œ"""
        print("\n" + "=" * 60)
        print("ğŸ“¦ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®ç§»è¡Œ")
        print("=" * 60)
        
        stats = {
            'qdrant_migrated': 0,
            'metadata_migrated': 0,
            'duplicates_skipped': 0,
            'errors': 0
        }
        
        # 1. MetadataManagerã‹ã‚‰ã®ç§»è¡Œ
        if self.legacy_metadata_manager:
            print("\n1. MetadataManagerã‹ã‚‰ã®ç§»è¡Œ")
            stats['metadata_migrated'] = self._migrate_from_metadata_manager()
        
        # 2. Qdrantã‹ã‚‰ã®ç§»è¡Œ
        if self.legacy_qdrant:
            print("\n2. Qdrantã‹ã‚‰ã®ç§»è¡Œ")
            stats['qdrant_migrated'] = self._migrate_from_qdrant()
        
        # 3. IDé‡è¤‡ã®è§£æ±º
        print("\n3. IDé‡è¤‡ã®è§£æ±º")
        self._resolve_duplicates()
        
        return stats
    
    def _migrate_from_metadata_manager(self) -> int:
        """MetadataManagerã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’ç§»è¡Œ"""
        if not self.legacy_metadata_manager:
            print("  âš ï¸  MetadataManagerãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
            return 0
        
        try:
            # MetadataManagerã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹
            db_path = Path("./metadata/metadata.db")
            if not db_path.exists():
                print("  âš ï¸  MetadataManagerã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return 0
            
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            
            # æ–‡æ›¸ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            cursor.execute("""
                SELECT id, title, filename, file_path, file_hash,
                       document_type, category, subcategory, version,
                       status, created_at
                FROM document_metadata
            """)
            
            rows = cursor.fetchall()
            migrated = 0
            
            for row in rows:
                try:
                    # UnifiedDocumentã«å¤‰æ›
                    doc = UnifiedDocument(
                        id=row[0],  # æ—¢å­˜IDã‚’ä¿æŒ
                        title=row[1],
                        content="",  # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯åˆ¥é€”å–å¾—ãŒå¿…è¦
                        file_name=row[2],
                        file_path=row[3],
                        content_hash=row[4],
                        document_type=DocumentType(row[5]) if row[5] else DocumentType.OTHER,
                        category=row[6] or "general",
                        subcategory=row[7],
                        version=row[8] or "1.0",
                        status=DocumentStatus.INDEXED,
                        created_at=datetime.fromisoformat(row[10]) if row[10] else None
                    )
                    
                    # çµ±åˆã‚¹ãƒˆã‚¢ã«è¿½åŠ 
                    self.unified_store.add_document(doc)
                    
                    # IDãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆ
                    self.unified_store.add_id_mapping(
                        unified_id=doc.id,
                        original_id=row[0],
                        source_system='metadata_manager'
                    )
                    
                    migrated += 1
                    
                except Exception as e:
                    logger.warning(f"Failed to migrate document {row[0]}: {e}")
            
            conn.close()
            print(f"  âœ… {migrated}ä»¶ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ç§»è¡Œã—ã¾ã—ãŸ")
            return migrated
            
        except Exception as e:
            logger.error(f"MetadataManager migration failed: {e}")
            return 0
    
    def _migrate_from_qdrant(self) -> int:
        """Qdrantã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’ç§»è¡Œ"""
        if not self.legacy_qdrant:
            print("  âš ï¸  QdrantãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
            return 0
        
        try:
            # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æƒ…å ±ã®å–å¾—
            info = self.legacy_qdrant.get_collection_info()
            vectors_count = info.get('vectors_count', 0)
            
            if vectors_count == 0:
                print("  âš ï¸  Qdrantã«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                return 0
            
            print(f"  Qdrantãƒ™ã‚¯ãƒˆãƒ«æ•°: {vectors_count}")
            
            # ç°¡æ˜“çš„ãªå…¨ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆå®Ÿéš›ã¯ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«APIã‚’ä½¿ç”¨ã™ã¹ãï¼‰
            import numpy as np
            dummy_query = np.random.randn(1024).astype(np.float32)
            
            results = self.legacy_qdrant.search(
                query_embedding=dummy_query,
                top_k=min(vectors_count, 100),
                score_threshold=0.0
            )
            
            migrated = 0
            for result in results:
                try:
                    # æ—¢å­˜ã®æ–‡æ›¸ã‚’ç¢ºèª
                    existing = self.unified_store.get_document(result.id)
                    
                    if existing:
                        # ãƒ™ã‚¯ãƒˆãƒ«æƒ…å ±ã‚’æ›´æ–°
                        self.unified_store.update_vector_info(
                            doc_id=result.id,
                            vector_id=result.id,
                            vector_dims=1024
                        )
                    else:
                        # æ–°è¦æ–‡æ›¸ã¨ã—ã¦è¿½åŠ 
                        doc = UnifiedDocument(
                            id=result.id,
                            title=result.metadata.get('title', 'Untitled'),
                            content=result.text,
                            category=result.metadata.get('category', 'general'),
                            metadata=result.metadata,
                            status=DocumentStatus.INDEXED
                        )
                        
                        self.unified_store.add_document(doc)
                    
                    # IDãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆ
                    self.unified_store.add_id_mapping(
                        unified_id=result.id,
                        original_id=result.id,
                        source_system='qdrant'
                    )
                    
                    migrated += 1
                    
                except Exception as e:
                    logger.warning(f"Failed to migrate vector {result.id}: {e}")
            
            print(f"  âœ… {migrated}ä»¶ã®ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ç§»è¡Œã—ã¾ã—ãŸ")
            return migrated
            
        except Exception as e:
            logger.error(f"Qdrant migration failed: {e}")
            return 0
    
    def _resolve_duplicates(self):
        """é‡è¤‡IDã®è§£æ±º"""
        conn = sqlite3.connect(self.unified_store.db_path)
        cursor = conn.cursor()
        
        # é‡è¤‡ã™ã‚‹content_hashã‚’æ¤œç´¢
        cursor.execute("""
            SELECT content_hash, COUNT(*) as cnt
            FROM documents
            GROUP BY content_hash
            HAVING cnt > 1
        """)
        
        duplicates = cursor.fetchall()
        
        if duplicates:
            print(f"  âš ï¸  {len(duplicates)}ä»¶ã®é‡è¤‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
            
            for content_hash, count in duplicates:
                # æœ€æ–°ã®ã‚‚ã®ã‚’æ®‹ã—ã€ä»–ã¯ archived ã«ã™ã‚‹
                cursor.execute("""
                    UPDATE documents
                    SET status = 'archived'
                    WHERE content_hash = ?
                    AND id NOT IN (
                        SELECT id FROM documents
                        WHERE content_hash = ?
                        ORDER BY updated_at DESC
                        LIMIT 1
                    )
                """, (content_hash, content_hash))
            
            conn.commit()
            print(f"  âœ… é‡è¤‡ã‚’è§£æ±ºã—ã¾ã—ãŸ")
        else:
            print(f"  âœ… é‡è¤‡ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        
        conn.close()
    
    def get_migration_report(self) -> Dict[str, Any]:
        """ç§»è¡Œãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        stats = self.unified_store.get_statistics()
        
        report = {
            'timestamp': datetime.now().isoformat(),
            'total_documents': stats['total_documents'],
            'vectorized_documents': stats['vectorized_documents'],
            'status_distribution': stats['status_counts'],
            'category_distribution': stats['category_counts'],
            'type_distribution': stats['type_counts'],
            'vectorization_rate': stats['vectorization_rate'],
            'migration_log': self.migration_log
        }
        
        return report


def run_metadata_integration():
    """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿çµ±åˆã‚’å®Ÿè¡Œ"""
    print("=" * 60)
    print("ğŸ”§ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç®¡ç†ã®çµ±åˆ")
    print("=" * 60)
    
    # ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®åˆæœŸåŒ–
    adapter = MetadataIntegrationAdapter()
    
    # 1. ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®è¿½åŠ 
    print("\n1. ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®è¿½åŠ ï¼ˆçµ±åˆAPIï¼‰")
    
    test_doc_id = adapter.add_document(
        text="çµ±åˆAPIã«ã‚ˆã‚‹ãƒ†ã‚¹ãƒˆæ–‡æ›¸ã§ã™ã€‚ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¨ãƒ™ã‚¯ãƒˆãƒ«ãŒä¸€å…ƒç®¡ç†ã•ã‚Œã¾ã™ã€‚",
        title="çµ±åˆãƒ†ã‚¹ãƒˆæ–‡æ›¸",
        metadata={
            'category': 'test',
            'document_type': 'other',
            'tags': ['test', 'integration'],
            'author': 'System'
        }
    )
    print(f"  âœ… æ–‡æ›¸è¿½åŠ : {test_doc_id}")
    
    # 2. æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®ç§»è¡Œ
    print("\n2. æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®ç§»è¡Œ")
    migration_stats = adapter.migrate_existing_data()
    
    print(f"\n  ç§»è¡Œçµæœ:")
    print(f"    MetadataManager: {migration_stats.get('metadata_migrated', 0)}ä»¶")
    print(f"    Qdrant: {migration_stats.get('qdrant_migrated', 0)}ä»¶")
    print(f"    é‡è¤‡ã‚¹ã‚­ãƒƒãƒ—: {migration_stats.get('duplicates_skipped', 0)}ä»¶")
    print(f"    ã‚¨ãƒ©ãƒ¼: {migration_stats.get('errors', 0)}ä»¶")
    
    # 3. çµ±è¨ˆæƒ…å ±ã®è¡¨ç¤º
    print("\n3. çµ±åˆå¾Œã®çµ±è¨ˆ")
    stats = adapter.unified_store.get_statistics()
    
    print(f"  ç·æ–‡æ›¸æ•°: {stats['total_documents']}")
    print(f"  ãƒ™ã‚¯ãƒˆãƒ«åŒ–æ¸ˆã¿: {stats['vectorized_documents']}")
    print(f"  ãƒ™ã‚¯ãƒˆãƒ«åŒ–ç‡: {stats['vectorization_rate']:.1f}%")
    
    print(f"\n  ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ¥:")
    for status, count in stats['status_counts'].items():
        print(f"    {status}: {count}ä»¶")
    
    print(f"\n  ã‚«ãƒ†ã‚´ãƒªåˆ¥:")
    for category, count in stats['category_counts'].items():
        print(f"    {category}: {count}ä»¶")
    
    # 4. ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    print("\n4. ç§»è¡Œãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ")
    report = adapter.get_migration_report()
    
    report_path = Path("data/unified_metadata/migration_report.json")
    report_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"  âœ… ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_path}")
    
    print("\n" + "=" * 60)
    print("âœ… ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç®¡ç†ã®çµ±åˆãŒå®Œäº†ã—ã¾ã—ãŸ")
    print("=" * 60)
    
    return adapter


if __name__ == "__main__":
    run_metadata_integration()
