#!/usr/bin/env python3
"""
æ—¢å­˜ã®RAGã‚·ã‚¹ãƒ†ãƒ ã«æ°¸ç¶šåŒ–æ©Ÿèƒ½ã‚’çµ±åˆã™ã‚‹ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼
"""

import os
import sys
from pathlib import Path
from typing import List, Dict, Any, Optional
import logging
import json
from datetime import datetime

sys.path.insert(0, "/workspace" if os.path.exists("/workspace") else ".")

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class PersistentRAGAdapter:
    """æ—¢å­˜ã®RAGã‚·ã‚¹ãƒ†ãƒ ã«æ°¸ç¶šåŒ–æ©Ÿèƒ½ã‚’è¿½åŠ ã™ã‚‹ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼"""
    
    def __init__(self):
        # æ°¸ç¶šåŒ–ã‚¹ãƒˆã‚¢ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¨åˆæœŸåŒ–
        from scripts.rag_fixes.fix_data_persistence import PersistentVectorStore
        self.persistent_store = PersistentVectorStore()
        
        # æ—¢å­˜ã®RAGã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®åˆæœŸåŒ–
        self._init_existing_components()
        
        # è‡ªå‹•ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®è¨­å®š
        self.auto_backup_enabled = True
        self.auto_backup_interval = 10  # 10æ–‡æ›¸ã”ã¨ã«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
        self.document_count = 0
        
        logger.info("PersistentRAGAdapter initialized")
    
    def _init_existing_components(self):
        """æ—¢å­˜ã®RAGã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’åˆæœŸåŒ–"""
        try:
            from src.rag.indexing.vector_store import QdrantVectorStore
            from src.rag.indexing.embedding_model import EmbeddingModelFactory
            from src.rag.indexing.metadata_manager import MetadataManager
            
            # æ—¢å­˜ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢
            self.original_vector_store = QdrantVectorStore(
                collection_name="road_design_docs",
                embedding_dim=1024,
                path="./data/qdrant"
            )
            
            # åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«
            self.embedding_model = EmbeddingModelFactory.create(
                model_name="intfloat/multilingual-e5-large",
                device="cuda" if os.path.exists("/usr/local/cuda") else "cpu"
            )
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
            self.metadata_manager = MetadataManager(
                db_path="./metadata/metadata.db"
            )
            
            logger.info("Existing RAG components initialized")
            
        except Exception as e:
            logger.warning(f"Some RAG components could not be initialized: {e}")
    
    def add_document(self,
                    text: str,
                    title: str,
                    metadata: Dict[str, Any],
                    file_path: Optional[str] = None) -> str:
        """æ–‡æ›¸ã‚’æ°¸ç¶šçš„ã«è¿½åŠ ï¼ˆæ—¢å­˜ã®APIã¨äº’æ›ï¼‰"""
        
        try:
            # 1. åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®ç”Ÿæˆ
            embedding = self.embedding_model.encode(text, is_query=False)
            
            # 2. æ°¸ç¶šåŒ–ã‚¹ãƒˆã‚¢ã«è¿½åŠ 
            doc_id = self.persistent_store.add_document_with_persistence(
                text=text,
                title=title,
                embedding=embedding,
                metadata=metadata,
                file_path=file_path
            )
            
            # 3. æ—¢å­˜ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã«ã‚‚è¿½åŠ ï¼ˆäº’æ›æ€§ã®ãŸã‚ï¼‰
            try:
                self.original_vector_store.add_documents(
                    texts=[text],
                    embeddings=[embedding],
                    metadatas=[{**metadata, 'doc_id': doc_id, 'title': title}],
                    ids=[doc_id]
                )
            except Exception as e:
                logger.warning(f"Failed to add to original vector store: {e}")
            
            # 4. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã«ç™»éŒ²
            try:
                from src.rag.indexing.metadata_manager import DocumentMetadata, DocumentType, DocumentStatus
                
                doc_metadata = DocumentMetadata(
                    id=doc_id,
                    title=title,
                    filename=Path(file_path).name if file_path else "inline",
                    file_path=file_path or "",
                    file_hash=self.persistent_store._calculate_hash(text),
                    document_type=DocumentType(metadata.get('document_type', 'OTHER')),
                    category=metadata.get('category', 'general'),
                    status=DocumentStatus.ACTIVE
                )
                
                self.metadata_manager.add_metadata(doc_metadata)
            except Exception as e:
                logger.warning(f"Failed to register metadata: {e}")
            
            # 5. è‡ªå‹•ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®ãƒã‚§ãƒƒã‚¯
            self.document_count += 1
            if self.auto_backup_enabled and self.document_count % self.auto_backup_interval == 0:
                backup_name = f"auto_{datetime.now().strftime('%Y%m%d_%H%M')}_docs{self.document_count}"
                self.create_backup(backup_name)
            
            logger.info(f"Document added with persistence: {doc_id}")
            return doc_id
            
        except Exception as e:
            logger.error(f"Failed to add document: {e}")
            raise
    
    def search(self,
              query: str,
              top_k: int = 5,
              filters: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """æ°¸ç¶šåŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ¤œç´¢"""
        
        try:
            # ã‚¯ã‚¨ãƒªã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆ
            query_embedding = self.embedding_model.encode(query, is_query=True)
            
            # æ°¸ç¶šåŒ–ã‚¹ãƒˆã‚¢ã‹ã‚‰æ¤œç´¢
            results = self.persistent_store.vector_store.search(
                query_embedding=query_embedding,
                top_k=top_k,
                filters=filters
            )
            
            # çµæœã®æ•´å½¢
            formatted_results = []
            for result in results:
                formatted_results.append({
                    'id': result.id,
                    'score': float(result.score),
                    'text': result.text,
                    'metadata': result.metadata
                })
            
            return formatted_results
            
        except Exception as e:
            logger.error(f"Search failed: {e}")
            return []
    
    def create_backup(self, backup_name: Optional[str] = None) -> str:
        """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ä½œæˆ"""
        if not backup_name:
            backup_name = f"manual_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        backup_path = self.persistent_store.create_backup(backup_name)
        logger.info(f"Backup created: {backup_path}")
        return backup_path
    
    def restore_backup(self, backup_name: str) -> bool:
        """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰å¾©å…ƒ"""
        success = self.persistent_store.restore_from_backup(backup_name)
        if success:
            logger.info(f"Successfully restored from backup: {backup_name}")
            # æ—¢å­˜ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚‚å†åˆæœŸåŒ–
            self._init_existing_components()
        else:
            logger.error(f"Failed to restore from backup: {backup_name}")
        return success
    
    def get_persistence_status(self) -> Dict[str, Any]:
        """æ°¸ç¶šåŒ–ã®çŠ¶æ…‹ã‚’å–å¾—"""
        status = self.persistent_store.verify_persistence()
        
        # è¿½åŠ æƒ…å ±
        status['auto_backup_enabled'] = self.auto_backup_enabled
        status['auto_backup_interval'] = self.auto_backup_interval
        status['documents_since_last_backup'] = self.document_count % self.auto_backup_interval
        
        return status
    
    def migrate_existing_data(self) -> int:
        """æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ°¸ç¶šåŒ–ã‚¹ãƒˆã‚¢ã«ç§»è¡Œ"""
        migrated_count = 0
        
        try:
            # æ—¢å­˜ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            import numpy as np
            
            # ãƒ€ãƒŸãƒ¼ã‚¯ã‚¨ãƒªã§å…¨ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆå®Ÿéš›ã¯åˆ¥ã®æ–¹æ³•ãŒå¿…è¦ï¼‰
            dummy_query = np.random.randn(1024).astype(np.float32)
            existing_data = self.original_vector_store.search(
                query_embedding=dummy_query,
                top_k=1000  # å¤§ããªæ•°ã‚’æŒ‡å®š
            )
            
            logger.info(f"Found {len(existing_data)} documents to migrate")
            
            for item in existing_data:
                try:
                    # æ°¸ç¶šåŒ–ã‚¹ãƒˆã‚¢ã«è¿½åŠ 
                    self.persistent_store.add_document_with_persistence(
                        text=item.text,
                        title=item.metadata.get('title', 'Migrated Document'),
                        embedding=np.random.randn(1024),  # å®Ÿéš›ã¯å†è¨ˆç®—ãŒå¿…è¦
                        metadata=item.metadata
                    )
                    migrated_count += 1
                    
                    if migrated_count % 10 == 0:
                        logger.info(f"Migrated {migrated_count} documents...")
                        
                except Exception as e:
                    logger.warning(f"Failed to migrate document {item.id}: {e}")
            
            # ç§»è¡Œå®Œäº†å¾Œã«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
            if migrated_count > 0:
                self.create_backup(f"migration_complete_{migrated_count}_docs")
            
            logger.info(f"Migration completed: {migrated_count} documents")
            
        except Exception as e:
            logger.error(f"Migration failed: {e}")
        
        return migrated_count


def setup_persistent_rag():
    """æ°¸ç¶šåŒ–RAGã‚·ã‚¹ãƒ†ãƒ ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
    print("=" * 60)
    print("ğŸ”§ æ°¸ç¶šåŒ–RAGã‚·ã‚¹ãƒ†ãƒ ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—")
    print("=" * 60)
    
    # 1. ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®åˆæœŸåŒ–
    print("\n1. æ°¸ç¶šåŒ–ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®åˆæœŸåŒ–")
    adapter = PersistentRAGAdapter()
    print("  âœ… ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼åˆæœŸåŒ–å®Œäº†")
    
    # 2. æ°¸ç¶šåŒ–çŠ¶æ…‹ã®ç¢ºèª
    print("\n2. ç¾åœ¨ã®æ°¸ç¶šåŒ–çŠ¶æ…‹")
    status = adapter.get_persistence_status()
    for key, value in status.items():
        print(f"  {key}: {value}")
    
    # 3. ãƒ†ã‚¹ãƒˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®è¿½åŠ 
    print("\n3. ãƒ†ã‚¹ãƒˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®è¿½åŠ ")
    test_docs = [
        {
            'title': 'æ°¸ç¶šåŒ–ãƒ†ã‚¹ãƒˆæ–‡æ›¸1',
            'text': 'ã“ã‚Œã¯æ°¸ç¶šåŒ–ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆæ–‡æ›¸ã§ã™ã€‚ãƒ‡ãƒ¼ã‚¿ãŒæ­£ã—ãä¿å­˜ã•ã‚Œã€ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãŒä½œæˆã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¾ã™ã€‚',
            'metadata': {'category': 'test', 'priority': 'high'}
        },
        {
            'title': 'æ°¸ç¶šåŒ–ãƒ†ã‚¹ãƒˆæ–‡æ›¸2',
            'text': 'ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã¨ãƒªã‚¹ãƒˆã‚¢æ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆã€‚ã‚·ã‚¹ãƒ†ãƒ éšœå®³ãŒç™ºç”Ÿã—ã¦ã‚‚ãƒ‡ãƒ¼ã‚¿ãŒå¾©å…ƒã§ãã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¾ã™ã€‚',
            'metadata': {'category': 'test', 'priority': 'medium'}
        }
    ]
    
    for doc in test_docs:
        doc_id = adapter.add_document(
            text=doc['text'],
            title=doc['title'],
            metadata=doc['metadata']
        )
        print(f"  âœ… è¿½åŠ : {doc['title']} (ID: {doc_id[:8]}...)")
    
    # 4. æ¤œç´¢ãƒ†ã‚¹ãƒˆ
    print("\n4. æ¤œç´¢ãƒ†ã‚¹ãƒˆ")
    results = adapter.search("æ°¸ç¶šåŒ–", top_k=3)
    print(f"  æ¤œç´¢çµæœ: {len(results)}ä»¶")
    for i, result in enumerate(results[:2], 1):
        print(f"    {i}. ã‚¹ã‚³ã‚¢: {result['score']:.3f}, ID: {result['id'][:8]}...")
    
    # 5. ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®ä½œæˆ
    print("\n5. æ‰‹å‹•ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®ä½œæˆ")
    backup_path = adapter.create_backup("setup_complete")
    print(f"  âœ… ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ: {backup_path}")
    
    print("\nâœ… ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†")
    print("\næ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
    print("  1. æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®ç§»è¡Œ: python scripts/rag_fixes/migrate_existing_data.py")
    print("  2. è‡ªå‹•ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®è¨­å®š: python scripts/rag_fixes/setup_auto_backup.py")
    print("  3. ã‚·ã‚¹ãƒ†ãƒ ã®å†èµ·å‹•: ./start_dev_env.sh")
    
    return adapter


if __name__ == "__main__":
    setup_persistent_rag()
