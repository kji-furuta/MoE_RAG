#!/usr/bin/env python3
"""
çµ±åˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ 
MetadataManagerã¨Qdrantã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€å…ƒç®¡ç†
"""

import os
import sys
import json
import sqlite3
import hashlib
from pathlib import Path
from datetime import datetime, timezone, timedelta
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict, field
from enum import Enum
import logging
import uuid

# ãƒ‘ã‚¹è¨­å®š
current_dir = Path(__file__).parent.parent.parent
sys.path.insert(0, str(current_dir))

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# JST ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³
JST = timezone(timedelta(hours=9))


class DocumentType(Enum):
    """æ–‡æ›¸ã‚¿ã‚¤ãƒ—"""
    ROAD_STANDARD = "road_standard"
    DESIGN_GUIDE = "design_guide"
    REGULATION = "regulation"
    TECHNICAL_MANUAL = "technical_manual"
    SPECIFICATION = "specification"
    OTHER = "other"


class DocumentStatus(Enum):
    """æ–‡æ›¸ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹"""
    PENDING = "pending"          # å‡¦ç†å¾…ã¡
    INDEXING = "indexing"        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¸­
    INDEXED = "indexed"          # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å®Œäº†
    FAILED = "failed"            # å¤±æ•—
    ARCHIVED = "archived"        # ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–


@dataclass
class UnifiedDocument:
    """çµ±åˆæ–‡æ›¸ãƒ¢ãƒ‡ãƒ«"""
    # ä¸»ã‚­ãƒ¼ï¼ˆã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã§ä¸€æ„ï¼‰
    id: str = field(default_factory=lambda: str(uuid.uuid4()))
    
    # åŸºæœ¬æƒ…å ±
    title: str = ""
    content: str = ""
    content_hash: str = ""
    
    # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±
    file_path: Optional[str] = None
    file_name: Optional[str] = None
    file_size: Optional[int] = None
    file_type: Optional[str] = None
    
    # åˆ†é¡æƒ…å ±
    document_type: DocumentType = DocumentType.OTHER
    category: str = "general"
    subcategory: Optional[str] = None
    tags: List[str] = field(default_factory=list)
    
    # ãƒ™ã‚¯ãƒˆãƒ«æƒ…å ±
    vector_id: Optional[str] = None  # Qdrantå†…ã®ID
    vector_dims: Optional[int] = None
    embedding_model: Optional[str] = None
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    # ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†
    version: str = "1.0"
    parent_id: Optional[str] = None  # è¦ªæ–‡æ›¸ã®ID
    
    # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
    status: DocumentStatus = DocumentStatus.PENDING
    
    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
    created_at: Optional[datetime] = None
    updated_at: Optional[datetime] = None
    indexed_at: Optional[datetime] = None
    
    def __post_init__(self):
        """åˆæœŸåŒ–å¾Œã®å‡¦ç†"""
        if not self.created_at:
            self.created_at = datetime.now(JST)
        if not self.updated_at:
            self.updated_at = datetime.now(JST)
        if not self.content_hash and self.content:
            self.content_hash = self._calculate_hash(self.content)
    
    def _calculate_hash(self, text: str) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¨ˆç®—"""
        return hashlib.sha256(text.encode()).hexdigest()
    
    def to_dict(self) -> Dict[str, Any]:
        """è¾æ›¸å½¢å¼ã«å¤‰æ›"""
        data = asdict(self)
        # Enumã‚’æ–‡å­—åˆ—ã«å¤‰æ›
        data['document_type'] = self.document_type.value
        data['status'] = self.status.value
        # datetimeã‚’ISOå½¢å¼ã«å¤‰æ›
        for key in ['created_at', 'updated_at', 'indexed_at']:
            if data[key]:
                data[key] = data[key].isoformat()
        return data
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'UnifiedDocument':
        """è¾æ›¸ã‹ã‚‰å¾©å…ƒ"""
        data = data.copy()
        # æ–‡å­—åˆ—ã‚’Enumã«å¤‰æ›
        if 'document_type' in data:
            data['document_type'] = DocumentType(data['document_type'])
        if 'status' in data:
            data['status'] = DocumentStatus(data['status'])
        # ISOå½¢å¼ã‚’datetimeã«å¤‰æ›
        for key in ['created_at', 'updated_at', 'indexed_at']:
            if data.get(key):
                if isinstance(data[key], str):
                    data[key] = datetime.fromisoformat(data[key])
        return cls(**data)


class UnifiedMetadataStore:
    """çµ±åˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆã‚¢"""
    
    def __init__(self, base_path: str = "./data/unified_metadata"):
        self.base_path = Path(base_path)
        self.base_path.mkdir(parents=True, exist_ok=True)
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹
        self.db_path = self.base_path / "unified_store.db"
        
        # åˆæœŸåŒ–
        self._init_database()
        self._init_vector_store()
        
        logger.info(f"UnifiedMetadataStore initialized at {self.base_path}")
    
    def _init_database(self):
        """çµ±åˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®åˆæœŸåŒ–"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # çµ±åˆæ–‡æ›¸ãƒ†ãƒ¼ãƒ–ãƒ«
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS documents (
                -- ä¸»ã‚­ãƒ¼
                id TEXT PRIMARY KEY,
                
                -- åŸºæœ¬æƒ…å ±
                title TEXT NOT NULL,
                content TEXT NOT NULL,
                content_hash TEXT UNIQUE,
                
                -- ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±
                file_path TEXT,
                file_name TEXT,
                file_size INTEGER,
                file_type TEXT,
                
                -- åˆ†é¡æƒ…å ±
                document_type TEXT NOT NULL,
                category TEXT NOT NULL,
                subcategory TEXT,
                tags TEXT,  -- JSONé…åˆ—ã¨ã—ã¦ä¿å­˜
                
                -- ãƒ™ã‚¯ãƒˆãƒ«æƒ…å ±
                vector_id TEXT,
                vector_dims INTEGER,
                embedding_model TEXT,
                
                -- ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆJSONï¼‰
                metadata TEXT,
                
                -- ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†
                version TEXT DEFAULT '1.0',
                parent_id TEXT,
                
                -- ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
                status TEXT DEFAULT 'pending',
                
                -- ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                indexed_at TIMESTAMP,
                
                -- å¤–éƒ¨ã‚­ãƒ¼
                FOREIGN KEY (parent_id) REFERENCES documents(id)
            )
        ''')
        
        # ID ãƒãƒƒãƒ”ãƒ³ã‚°ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆäº’æ›æ€§ã®ãŸã‚ï¼‰
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS id_mappings (
                unified_id TEXT PRIMARY KEY,
                original_id TEXT,
                source_system TEXT,  -- 'qdrant', 'metadata_manager', 'legacy'
                mapped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (unified_id) REFERENCES documents(id)
            )
        ''')
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ†ãƒ¼ãƒ–ãƒ«
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS index_log (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                document_id TEXT,
                action TEXT,  -- 'add', 'update', 'delete', 'reindex'
                status TEXT,  -- 'success', 'failed'
                error_message TEXT,
                performed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (document_id) REFERENCES documents(id)
            )
        ''')
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_doc_status ON documents(status)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_doc_category ON documents(category)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_doc_type ON documents(document_type)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_doc_hash ON documents(content_hash)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_doc_vector ON documents(vector_id)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_mapping_original ON id_mappings(original_id)')
        
        # ãƒˆãƒªã‚¬ãƒ¼: updated_atã®è‡ªå‹•æ›´æ–°
        cursor.execute('''
            CREATE TRIGGER IF NOT EXISTS update_timestamp
            AFTER UPDATE ON documents
            FOR EACH ROW
            BEGIN
                UPDATE documents SET updated_at = CURRENT_TIMESTAMP
                WHERE id = NEW.id;
            END
        ''')
        
        conn.commit()
        conn.close()
        
        logger.info("Unified database initialized")
    
    def _init_vector_store(self):
        """ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®åˆæœŸåŒ–"""
        try:
            from src.rag.indexing.vector_store import QdrantVectorStore
            
            self.vector_store = QdrantVectorStore(
                collection_name="unified_docs",
                embedding_dim=1024,
                path=str(self.base_path / "vectors")
            )
            logger.info("Vector store initialized")
        except ImportError:
            logger.warning("QdrantVectorStore not available")
            self.vector_store = None
    
    def add_document(self, document: UnifiedDocument) -> str:
        """çµ±åˆæ–‡æ›¸ã‚’è¿½åŠ """
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            # é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒãƒƒã‚·ãƒ¥ãƒ™ãƒ¼ã‚¹ï¼‰
            cursor.execute(
                "SELECT id FROM documents WHERE content_hash = ?",
                (document.content_hash,)
            )
            existing = cursor.fetchone()
            
            if existing:
                logger.warning(f"Document already exists: {existing[0]}")
                return existing[0]
            
            # æ–‡æ›¸ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«è¿½åŠ 
            cursor.execute('''
                INSERT INTO documents (
                    id, title, content, content_hash,
                    file_path, file_name, file_size, file_type,
                    document_type, category, subcategory, tags,
                    vector_id, vector_dims, embedding_model,
                    metadata, version, parent_id, status,
                    created_at, updated_at, indexed_at
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                document.id,
                document.title,
                document.content,
                document.content_hash,
                document.file_path,
                document.file_name,
                document.file_size,
                document.file_type,
                document.document_type.value,
                document.category,
                document.subcategory,
                json.dumps(document.tags),
                document.vector_id,
                document.vector_dims,
                document.embedding_model,
                json.dumps(document.metadata),
                document.version,
                document.parent_id,
                document.status.value,
                document.created_at.isoformat() if document.created_at else datetime.now(JST).isoformat(),
                document.updated_at.isoformat() if document.updated_at else datetime.now(JST).isoformat(),
                document.indexed_at.isoformat() if document.indexed_at else None
            ))
            
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ­ã‚°ã«è¨˜éŒ²
            cursor.execute('''
                INSERT INTO index_log (document_id, action, status)
                VALUES (?, ?, ?)
            ''', (document.id, 'add', 'success'))
            
            conn.commit()
            logger.info(f"Document added: {document.id} - {document.title}")
            return document.id
            
        except Exception as e:
            conn.rollback()
            logger.error(f"Failed to add document: {e}")
            
            # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã«è¨˜éŒ²
            cursor.execute('''
                INSERT INTO index_log (document_id, action, status, error_message)
                VALUES (?, ?, ?, ?)
            ''', (document.id, 'add', 'failed', str(e)))
            conn.commit()
            
            raise
        finally:
            conn.close()
    
    def add_document_with_vector(self,
                                 document: UnifiedDocument,
                                 embedding: Any) -> str:
        """æ–‡æ›¸ã¨ãƒ™ã‚¯ãƒˆãƒ«ã‚’åŒæ™‚ã«è¿½åŠ """
        # æ–‡æ›¸ã‚’è¿½åŠ 
        doc_id = self.add_document(document)
        
        # ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã«è¿½åŠ 
        if self.vector_store and embedding is not None:
            try:
                # ãƒ™ã‚¯ãƒˆãƒ«IDã¯æ–‡æ›¸IDã¨åŒã˜ã«ã™ã‚‹ï¼ˆä¸€å¯¾ä¸€ãƒãƒƒãƒ”ãƒ³ã‚°ï¼‰
                self.vector_store.add_documents(
                    texts=[document.content],
                    embeddings=[embedding],
                    metadatas=[{
                        'doc_id': doc_id,
                        'title': document.title,
                        'category': document.category,
                        'document_type': document.document_type.value,
                        **document.metadata
                    }],
                    ids=[doc_id]  # çµ±ä¸€IDä½¿ç”¨
                )
                
                # ãƒ™ã‚¯ãƒˆãƒ«æƒ…å ±ã‚’æ›´æ–°
                self.update_vector_info(
                    doc_id,
                    vector_id=doc_id,
                    vector_dims=len(embedding) if hasattr(embedding, '__len__') else None
                )
                
                logger.info(f"Vector added for document: {doc_id}")
                
            except Exception as e:
                logger.error(f"Failed to add vector: {e}")
        
        return doc_id
    
    def get_document(self, doc_id: str) -> Optional[UnifiedDocument]:
        """IDã§æ–‡æ›¸ã‚’å–å¾—"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute(
            "SELECT * FROM documents WHERE id = ?",
            (doc_id,)
        )
        
        row = cursor.fetchone()
        conn.close()
        
        if row:
            return self._row_to_document(row)
        return None
    
    def get_document_by_original_id(self, original_id: str) -> Optional[UnifiedDocument]:
        """å…ƒã®IDã§æ–‡æ›¸ã‚’å–å¾—ï¼ˆäº’æ›æ€§ã®ãŸã‚ï¼‰"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # IDãƒãƒƒãƒ”ãƒ³ã‚°ã‹ã‚‰æ¤œç´¢
        cursor.execute('''
            SELECT d.* FROM documents d
            JOIN id_mappings m ON d.id = m.unified_id
            WHERE m.original_id = ?
        ''', (original_id,))
        
        row = cursor.fetchone()
        conn.close()
        
        if row:
            return self._row_to_document(row)
        return None
    
    def search_documents(self,
                        query: Optional[str] = None,
                        category: Optional[str] = None,
                        document_type: Optional[DocumentType] = None,
                        status: Optional[DocumentStatus] = None,
                        limit: int = 100) -> List[UnifiedDocument]:
        """æ–‡æ›¸ã‚’æ¤œç´¢"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # ã‚¯ã‚¨ãƒªæ§‹ç¯‰
        conditions = []
        params = []
        
        if query:
            conditions.append("(title LIKE ? OR content LIKE ?)")
            params.extend([f"%{query}%", f"%{query}%"])
        
        if category:
            conditions.append("category = ?")
            params.append(category)
        
        if document_type:
            conditions.append("document_type = ?")
            params.append(document_type.value)
        
        if status:
            conditions.append("status = ?")
            params.append(status.value)
        
        # SQLå®Ÿè¡Œ
        where_clause = " AND ".join(conditions) if conditions else "1=1"
        sql = f"""
            SELECT * FROM documents
            WHERE {where_clause}
            ORDER BY updated_at DESC
            LIMIT ?
        """
        params.append(limit)
        
        cursor.execute(sql, params)
        rows = cursor.fetchall()
        conn.close()
        
        return [self._row_to_document(row) for row in rows]
    
    def update_document(self, document: UnifiedDocument) -> bool:
        """æ–‡æ›¸ã‚’æ›´æ–°"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            document.updated_at = datetime.now(JST)
            
            cursor.execute('''
                UPDATE documents SET
                    title = ?, content = ?, content_hash = ?,
                    file_path = ?, file_name = ?, file_size = ?, file_type = ?,
                    document_type = ?, category = ?, subcategory = ?, tags = ?,
                    vector_id = ?, vector_dims = ?, embedding_model = ?,
                    metadata = ?, version = ?, parent_id = ?, status = ?,
                    updated_at = ?, indexed_at = ?
                WHERE id = ?
            ''', (
                document.title,
                document.content,
                document.content_hash,
                document.file_path,
                document.file_name,
                document.file_size,
                document.file_type,
                document.document_type.value,
                document.category,
                document.subcategory,
                json.dumps(document.tags),
                document.vector_id,
                document.vector_dims,
                document.embedding_model,
                json.dumps(document.metadata),
                document.version,
                document.parent_id,
                document.status.value,
                document.updated_at.isoformat(),
                document.indexed_at.isoformat() if document.indexed_at else None,
                document.id
            ))
            
            # ãƒ­ã‚°è¨˜éŒ²
            cursor.execute('''
                INSERT INTO index_log (document_id, action, status)
                VALUES (?, ?, ?)
            ''', (document.id, 'update', 'success'))
            
            conn.commit()
            logger.info(f"Document updated: {document.id}")
            return True
            
        except Exception as e:
            conn.rollback()
            logger.error(f"Failed to update document: {e}")
            return False
        finally:
            conn.close()
    
    def update_vector_info(self,
                          doc_id: str,
                          vector_id: str,
                          vector_dims: Optional[int] = None,
                          embedding_model: Optional[str] = None) -> bool:
        """ãƒ™ã‚¯ãƒˆãƒ«æƒ…å ±ã‚’æ›´æ–°"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            cursor.execute('''
                UPDATE documents SET
                    vector_id = ?,
                    vector_dims = ?,
                    embedding_model = ?,
                    indexed_at = CURRENT_TIMESTAMP,
                    status = ?
                WHERE id = ?
            ''', (
                vector_id,
                vector_dims,
                embedding_model,
                DocumentStatus.INDEXED.value,
                doc_id
            ))
            
            conn.commit()
            return True
            
        except Exception as e:
            conn.rollback()
            logger.error(f"Failed to update vector info: {e}")
            return False
        finally:
            conn.close()
    
    def add_id_mapping(self,
                      unified_id: str,
                      original_id: str,
                      source_system: str) -> bool:
        """IDãƒãƒƒãƒ”ãƒ³ã‚°ã‚’è¿½åŠ ï¼ˆäº’æ›æ€§ã®ãŸã‚ï¼‰"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            cursor.execute('''
                INSERT OR REPLACE INTO id_mappings
                (unified_id, original_id, source_system)
                VALUES (?, ?, ?)
            ''', (unified_id, original_id, source_system))
            
            conn.commit()
            return True
            
        except Exception as e:
            conn.rollback()
            logger.error(f"Failed to add ID mapping: {e}")
            return False
        finally:
            conn.close()
    
    def get_statistics(self) -> Dict[str, Any]:
        """çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # ç·æ–‡æ›¸æ•°
        cursor.execute("SELECT COUNT(*) FROM documents")
        total_docs = cursor.fetchone()[0]
        
        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ¥
        cursor.execute("""
            SELECT status, COUNT(*) FROM documents
            GROUP BY status
        """)
        status_counts = dict(cursor.fetchall())
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥
        cursor.execute("""
            SELECT category, COUNT(*) FROM documents
            GROUP BY category
        """)
        category_counts = dict(cursor.fetchall())
        
        # æ–‡æ›¸ã‚¿ã‚¤ãƒ—åˆ¥
        cursor.execute("""
            SELECT document_type, COUNT(*) FROM documents
            GROUP BY document_type
        """)
        type_counts = dict(cursor.fetchall())
        
        # ãƒ™ã‚¯ãƒˆãƒ«åŒ–æ¸ˆã¿
        cursor.execute("SELECT COUNT(*) FROM documents WHERE vector_id IS NOT NULL")
        vectorized_count = cursor.fetchone()[0]
        
        conn.close()
        
        return {
            'total_documents': total_docs,
            'status_counts': status_counts,
            'category_counts': category_counts,
            'type_counts': type_counts,
            'vectorized_documents': vectorized_count,
            'vectorization_rate': (vectorized_count / total_docs * 100) if total_docs > 0 else 0
        }
    
    def _row_to_document(self, row: Tuple) -> UnifiedDocument:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¡Œã‚’æ–‡æ›¸ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›"""
        columns = [
            'id', 'title', 'content', 'content_hash',
            'file_path', 'file_name', 'file_size', 'file_type',
            'document_type', 'category', 'subcategory', 'tags',
            'vector_id', 'vector_dims', 'embedding_model',
            'metadata', 'version', 'parent_id', 'status',
            'created_at', 'updated_at', 'indexed_at'
        ]
        
        data = dict(zip(columns, row))
        
        # JSON ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ãƒ‘ãƒ¼ã‚¹
        if data['tags']:
            data['tags'] = json.loads(data['tags'])
        else:
            data['tags'] = []
        
        if data['metadata']:
            data['metadata'] = json.loads(data['metadata'])
        else:
            data['metadata'] = {}
        
        return UnifiedDocument.from_dict(data)
    
    def migrate_from_separate_systems(self) -> Dict[str, int]:
        """æ—¢å­˜ã®åˆ†é›¢ã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’ç§»è¡Œ"""
        migration_stats = {
            'qdrant_migrated': 0,
            'metadata_manager_migrated': 0,
            'conflicts_resolved': 0
        }
        
        # 1. MetadataManagerã‹ã‚‰ã®ç§»è¡Œ
        try:
            from src.rag.indexing.metadata_manager import MetadataManager
            
            old_mm = MetadataManager()
            # å®Ÿè£…ã¯ MetadataManager ã®å®Ÿéš›ã®APIã«ä¾å­˜
            logger.info("Migrating from MetadataManager...")
            
        except Exception as e:
            logger.warning(f"Could not migrate from MetadataManager: {e}")
        
        # 2. æ—¢å­˜Qdrantã‹ã‚‰ã®ç§»è¡Œ
        try:
            from src.rag.indexing.vector_store import QdrantVectorStore
            
            old_qdrant = QdrantVectorStore(
                collection_name="road_design_docs",
                embedding_dim=1024,
                path="./data/qdrant"
            )
            
            logger.info("Migrating from Qdrant...")
            # å®Ÿè£…ã¯å®Ÿéš›ã®Qdrant APIã«ä¾å­˜
            
        except Exception as e:
            logger.warning(f"Could not migrate from Qdrant: {e}")
        
        return migration_stats


def test_unified_store():
    """çµ±åˆã‚¹ãƒˆã‚¢ã®ãƒ†ã‚¹ãƒˆ"""
    print("=" * 60)
    print("ğŸ”§ çµ±åˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆã‚¢ã®ãƒ†ã‚¹ãƒˆ")
    print("=" * 60)
    
    # ã‚¹ãƒˆã‚¢ã®åˆæœŸåŒ–
    store = UnifiedMetadataStore()
    
    # ãƒ†ã‚¹ãƒˆæ–‡æ›¸ã®ä½œæˆ
    test_docs = [
        UnifiedDocument(
            title="é“è·¯è¨­è¨ˆåŸºæº– ç¬¬1ç« ",
            content="é“è·¯è¨­è¨ˆã«ãŠã‘ã‚‹æœ€å°æ›²ç·šåŠå¾„ã®åŸºæº–ã«ã¤ã„ã¦èª¬æ˜ã—ã¾ã™ã€‚",
            document_type=DocumentType.ROAD_STANDARD,
            category="è¨­è¨ˆåŸºæº–",
            subcategory="å¹¾ä½•æ§‹é€ ",
            tags=["é“è·¯", "è¨­è¨ˆ", "æ›²ç·šåŠå¾„"],
            metadata={"author": "å›½åœŸäº¤é€šçœ", "year": 2024}
        ),
        UnifiedDocument(
            title="æ–½å·¥ç®¡ç†ãƒãƒ‹ãƒ¥ã‚¢ãƒ«",
            content="é“è·¯å·¥äº‹ã®æ–½å·¥ç®¡ç†ã«ã¤ã„ã¦è©³ç´°ã«è§£èª¬ã—ã¾ã™ã€‚",
            document_type=DocumentType.TECHNICAL_MANUAL,
            category="æ–½å·¥",
            subcategory="ç®¡ç†",
            tags=["æ–½å·¥", "ç®¡ç†", "å“è³ª"],
            metadata={"version": "2.0", "department": "æ–½å·¥éƒ¨"}
        )
    ]
    
    print("\n1. æ–‡æ›¸ã®è¿½åŠ ")
    for doc in test_docs:
        doc_id = store.add_document(doc)
        print(f"  âœ… è¿½åŠ : {doc.title} (ID: {doc_id[:8]}...)")
        
        # IDãƒãƒƒãƒ”ãƒ³ã‚°ã®è¿½åŠ ï¼ˆäº’æ›æ€§ãƒ†ã‚¹ãƒˆï¼‰
        original_id = f"legacy_{doc_id[:8]}"
        store.add_id_mapping(doc_id, original_id, "legacy")
        print(f"    ãƒãƒƒãƒ”ãƒ³ã‚°: {original_id} -> {doc_id[:8]}...")
    
    print("\n2. æ–‡æ›¸ã®æ¤œç´¢")
    results = store.search_documents(query="é“è·¯", limit=5)
    print(f"  æ¤œç´¢çµæœ: {len(results)}ä»¶")
    for result in results:
        print(f"    - {result.title} ({result.status.value})")
    
    print("\n3. çµ±è¨ˆæƒ…å ±")
    stats = store.get_statistics()
    print(f"  ç·æ–‡æ›¸æ•°: {stats['total_documents']}")
    print(f"  ãƒ™ã‚¯ãƒˆãƒ«åŒ–æ¸ˆã¿: {stats['vectorized_documents']}")
    print(f"  ãƒ™ã‚¯ãƒˆãƒ«åŒ–ç‡: {stats['vectorization_rate']:.1f}%")
    print(f"  ã‚«ãƒ†ã‚´ãƒªåˆ¥:")
    for category, count in stats['category_counts'].items():
        print(f"    - {category}: {count}ä»¶")
    
    print("\nâœ… ãƒ†ã‚¹ãƒˆå®Œäº†")
    return store


if __name__ == "__main__":
    test_unified_store()
