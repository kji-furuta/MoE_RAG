#!/usr/bin/env python3
"""
ãƒ‡ãƒ¼ã‚¿æ°¸ç¶šæ€§å®Ÿè£…ã®æ¤œè¨¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import os
import sys
from pathlib import Path
import json
import sqlite3
from datetime import datetime

sys.path.insert(0, "/workspace" if os.path.exists("/workspace") else ".")

def verify_persistence():
    """æ°¸ç¶šæ€§å®Ÿè£…ã®æ¤œè¨¼"""
    
    print("=" * 60)
    print("ğŸ” ãƒ‡ãƒ¼ã‚¿æ°¸ç¶šæ€§å®Ÿè£…ã®æ¤œè¨¼")
    print("=" * 60)
    
    success_count = 0
    total_tests = 7
    
    # 1. æ°¸ç¶šåŒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç¢ºèª
    print("\n1. æ°¸ç¶šåŒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã®ç¢ºèª")
    persistent_path = Path("data/rag_persistent")
    expected_dirs = ["vectors", "metadata", "backups", "checkpoints"]
    
    if persistent_path.exists():
        existing_dirs = [d.name for d in persistent_path.iterdir() if d.is_dir()]
        missing = set(expected_dirs) - set(existing_dirs)
        
        if not missing:
            print(f"  âœ… ã™ã¹ã¦ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨")
            success_count += 1
        else:
            print(f"  âŒ ä¸è¶³: {missing}")
    else:
        print(f"  âŒ æ°¸ç¶šåŒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“")
    
    # 2. SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ç¢ºèª
    print("\n2. SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ç¢ºèª")
    db_path = persistent_path / "metadata" / "persistent_store.db"
    
    if db_path.exists():
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«ã®ç¢ºèª
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
        tables = [t[0] for t in cursor.fetchall()]
        
        expected_tables = ["documents", "vectors", "index_status", "backup_history"]
        missing_tables = set(expected_tables) - set(tables)
        
        if not missing_tables:
            print(f"  âœ… ã™ã¹ã¦ã®ãƒ†ãƒ¼ãƒ–ãƒ«ãŒå­˜åœ¨")
            success_count += 1
            
            # ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ã®ç¢ºèª
            for table in expected_tables:
                cursor.execute(f"SELECT COUNT(*) FROM {table}")
                count = cursor.fetchone()[0]
                print(f"    - {table}: {count} ãƒ¬ã‚³ãƒ¼ãƒ‰")
        else:
            print(f"  âŒ ä¸è¶³ãƒ†ãƒ¼ãƒ–ãƒ«: {missing_tables}")
        
        conn.close()
    else:
        print(f"  âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
    
    # 3. Qdrantã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®ç¢ºèª
    print("\n3. Qdrantã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®ç¢ºèª")
    qdrant_path = persistent_path / "vectors"
    
    if qdrant_path.exists():
        # meta.jsonã®ç¢ºèª
        meta_path = qdrant_path / "meta.json"
        if meta_path.exists():
            with open(meta_path, 'r') as f:
                meta = json.load(f)
            
            if "persistent_docs" in meta.get("collections", {}):
                print(f"  âœ… persistent_docsã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å­˜åœ¨")
                success_count += 1
                
                collection = meta["collections"]["persistent_docs"]
                if "vectors" in collection:
                    size = collection["vectors"]["size"]
                    print(f"    ãƒ™ã‚¯ãƒˆãƒ«æ¬¡å…ƒ: {size}")
                    if size == 1024:
                        print(f"    âœ… æ¬¡å…ƒãŒæ­£ã—ã„")
                    else:
                        print(f"    âš ï¸  æ¬¡å…ƒãŒç•°ãªã‚‹ï¼ˆæœŸå¾…å€¤: 1024ï¼‰")
            else:
                print(f"  âŒ persistent_docsã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
        else:
            print(f"  âš ï¸  Qdrant meta.jsonãŒå­˜åœ¨ã—ã¾ã›ã‚“ï¼ˆåˆå›ã¯æ­£å¸¸ï¼‰")
            success_count += 1
    else:
        print(f"  âš ï¸  Qdrantãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“ï¼ˆåˆå›ã¯æ­£å¸¸ï¼‰")
    
    # 4. ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—è¨­å®šã®ç¢ºèª
    print("\n4. ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—è¨­å®šã®ç¢ºèª")
    backup_config = Path("config/backup_config.json")
    
    if backup_config.exists():
        with open(backup_config, 'r') as f:
            config = json.load(f)
        
        print(f"  âœ… ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨")
        success_count += 1
        print(f"    æœ‰åŠ¹: {config.get('enabled', False)}")
        print(f"    æ—¥æ¬¡: {config.get('backup_schedule', {}).get('daily', False)}")
        print(f"    é€±æ¬¡: {config.get('backup_schedule', {}).get('weekly', False)}")
    else:
        print(f"  âš ï¸  ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“ï¼ˆåˆå›ã¯æ­£å¸¸ï¼‰")
        success_count += 1
    
    # 5. ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç¢ºèª
    print("\n5. ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç¢ºèª")
    backup_dir = persistent_path / "backups"
    
    if backup_dir.exists():
        backups = list(backup_dir.iterdir())
        print(f"  âœ… ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå­˜åœ¨")
        print(f"    ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ•°: {len(backups)}")
        success_count += 1
        
        if backups:
            # æœ€æ–°ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æƒ…å ±
            latest = max(backups, key=lambda p: p.stat().st_mtime)
            mtime = datetime.fromtimestamp(latest.stat().st_mtime)
            size = sum(f.stat().st_size for f in latest.rglob('*') if f.is_file())
            print(f"    æœ€æ–°: {latest.name}")
            print(f"    æ—¥æ™‚: {mtime.strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"    ã‚µã‚¤ã‚º: {size:,} bytes")
    else:
        print(f"  âš ï¸  ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“")
    
    # 6. æ°¸ç¶šåŒ–APIã®ãƒ†ã‚¹ãƒˆ
    print("\n6. æ°¸ç¶šåŒ–APIã®ãƒ†ã‚¹ãƒˆ")
    try:
        from scripts.rag_fixes.fix_data_persistence import PersistentVectorStore
        
        store = PersistentVectorStore()
        status = store.verify_persistence()
        
        print(f"  âœ… æ°¸ç¶šåŒ–APIæ­£å¸¸")
        success_count += 1
        print(f"    ç·æ–‡æ›¸æ•°: {status['total_documents']}")
        print(f"    ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ¸ˆã¿: {status['indexed_documents']}")
        print(f"    ãƒ™ã‚¯ãƒˆãƒ«æ•°: {status['total_vectors']}")
        print(f"    æ°¸ç¶šåŒ–çŠ¶æ…‹: {status['persistence_status']}")
    except Exception as e:
        print(f"  âŒ æ°¸ç¶šåŒ–APIã‚¨ãƒ©ãƒ¼: {e}")
    
    # 7. çµ±åˆãƒ†ã‚¹ãƒˆ
    print("\n7. çµ±åˆãƒ†ã‚¹ãƒˆ")
    try:
        from scripts.rag_fixes.integrate_persistence import PersistentRAGAdapter
        import numpy as np
        
        adapter = PersistentRAGAdapter()
        
        # ãƒ†ã‚¹ãƒˆæ–‡æ›¸ã®è¿½åŠ 
        test_doc_id = adapter.add_document(
            text="æ°¸ç¶šæ€§ãƒ†ã‚¹ãƒˆæ–‡æ›¸",
            title="Test Document",
            metadata={"test": True, "timestamp": datetime.now().isoformat()}
        )
        
        # æ¤œç´¢ãƒ†ã‚¹ãƒˆ
        results = adapter.search("ãƒ†ã‚¹ãƒˆ", top_k=1)
        
        if results:
            print(f"  âœ… çµ±åˆãƒ†ã‚¹ãƒˆæˆåŠŸ")
            success_count += 1
            print(f"    è¿½åŠ æ–‡æ›¸ID: {test_doc_id[:8]}...")
            print(f"    æ¤œç´¢çµæœ: {len(results)}ä»¶")
        else:
            print(f"  âš ï¸  æ¤œç´¢çµæœãªã—ï¼ˆãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã¯æ­£å¸¸ï¼‰")
            success_count += 1
            
    except Exception as e:
        print(f"  âŒ çµ±åˆãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    
    # çµæœã‚µãƒãƒªãƒ¼
    print("\n" + "=" * 60)
    print(f"ğŸ“Š æ¤œè¨¼çµæœ: {success_count}/{total_tests} ãƒ†ã‚¹ãƒˆæˆåŠŸ")
    
    if success_count == total_tests:
        print("âœ… ã™ã¹ã¦ã®ãƒ†ã‚¹ãƒˆã«åˆæ ¼ã—ã¾ã—ãŸï¼")
        print("\nã‚·ã‚¹ãƒ†ãƒ ã¯æ­£å¸¸ã«å‹•ä½œã—ã¦ã„ã¾ã™ã€‚")
    elif success_count >= total_tests - 2:
        print("âš ï¸  ä¸€éƒ¨ã®ãƒ†ã‚¹ãƒˆãŒå¤±æ•—ã—ã¾ã—ãŸãŒã€åŸºæœ¬æ©Ÿèƒ½ã¯å‹•ä½œã—ã¾ã™ã€‚")
    else:
        print("âŒ è¤‡æ•°ã®ãƒ†ã‚¹ãƒˆãŒå¤±æ•—ã—ã¾ã—ãŸã€‚ä¿®æ­£ãŒå¿…è¦ã§ã™ã€‚")
    
    print("=" * 60)
    
    return success_count == total_tests

if __name__ == "__main__":
    success = verify_persistence()
    sys.exit(0 if success else 1)
