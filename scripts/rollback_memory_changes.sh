#!/bin/bash

# ãƒ¡ãƒ¢ãƒªç®¡ç†æ”¹å–„ã®å¤‰æ›´ã‚’ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯

echo "================================"
echo "ãƒ¡ãƒ¢ãƒªç®¡ç†æ”¹å–„ã®å¤‰æ›´ã‚’ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯"
echo "================================"

cd /home/kjifu/MoE_RAG || exit 1

# 1. è¿½åŠ ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
echo "1. è¿½åŠ ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ä¸­..."

# æ–°è¦ä½œæˆã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
rm -f src/core/memory_manager.py
rm -f src/core/quantization_manager.py
rm -f src/core/docker_memory_patch.py
rm -f app/memory_optimized_loader_v2.py
rm -f app/main_unified_docker.py
rm -f app/main_unified_lightweight.py
rm -f scripts/migrate_memory_management.py
rm -f scripts/test_memory_management.py
rm -f scripts/start_web_interface_docker.sh
rm -f scripts/diagnose_docker_resources.sh
rm -f scripts/check_docker_resources.sh
rm -f scripts/start_highspec.sh
rm -f docker/entrypoint.sh
rm -f docker/entrypoint_lightweight.sh
rm -f docker/docker-compose-highspec.yml
rm -f docker/wslconfig.example
rm -f src/rag/config/lightweight_config.py
rm -f MEMORY_MANAGEMENT_IMPROVEMENT.md

# core ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒç©ºãªã‚‰å‰Šé™¤
if [ -d src/core ] && [ -z "$(ls -A src/core)" ]; then
    rmdir src/core
fi

echo "âœ“ è¿½åŠ ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã—ã¾ã—ãŸ"

# 2. å…ƒã®ãƒ•ã‚¡ã‚¤ãƒ«ã«æˆ»ã™ï¼ˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãŒã‚ã‚‹å ´åˆï¼‰
echo "2. å¤‰æ›´ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’å…ƒã«æˆ»ã™..."

# app/main_unified.py ã‚’å…ƒã«æˆ»ã™ï¼ˆç’°å¢ƒå¤‰æ•°è¨­å®šã‚’å¾©å…ƒï¼‰
if [ -f app/backups/main_unified_*.py ]; then
    # æœ€æ–°ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’å¾©å…ƒ
    latest_backup=$(ls -t app/backups/main_unified_*.py | head -1)
    cp "$latest_backup" app/main_unified.py
    echo "âœ“ app/main_unified.py ã‚’å¾©å…ƒã—ã¾ã—ãŸ"
fi

# docker-compose.yml ã‚’å…ƒã«æˆ»ã™
cat > docker/docker-compose.yml << 'EOF'
version: '3.8'

services:
  ai-ft:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    image: ai-ft-rag:latest
    container_name: ai-ft-container
    hostname: ai-ft
    
    # GPU access - requires nvidia-docker2
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    
    # Environment variables
    environment:
      - CUDA_VISIBLE_DEVICES=0,1
      - PYTHONPATH=/workspace
      - WANDB_API_KEY=${WANDB_API_KEY:-}
      - HF_TOKEN=${HF_TOKEN:-}
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    
    # Port mappings
    ports:
      - "8888:8888"  # Jupyter Lab
      - "6006:6006"  # TensorBoard
      - "8050:8050"  # Web Interface (çµ±åˆ)
      - "8051:8051"  # RAG API (é–‹ç™ºãƒ»ãƒ†ã‚¹ãƒˆç”¨)
      - "11434:11434"  # Ollama API
    
    # Volume mounts
    volumes:
      - ../src:/workspace/src
      - ../config:/workspace/config
      - ../scripts:/workspace/scripts
      - ../notebooks:/workspace/notebooks
      - ../data:/workspace/data
      - ../models:/workspace/models
      - ../tests:/workspace/tests
      - ../app:/workspace/app
      - ../templates:/workspace/templates
      - ../outputs:/workspace/outputs
      - ./logs:/workspace/logs
      - ~/.cache/huggingface:/home/ai-user/.cache/huggingface
      - ~/.wandb:/home/ai-user/.wandb
      # RAGå°‚ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
      - ../temp_uploads:/workspace/temp_uploads
      - ../qdrant_data:/workspace/qdrant_data
      - ../docs:/workspace/docs
      - ../examples:/workspace/examples
      # RAGãƒ‡ãƒ¼ã‚¿æ°¸ç¶šåŒ–
      - ai_ft_rag_metadata:/workspace/metadata
      - ai_ft_rag_processed:/workspace/outputs/rag_index
      # Ollamaãƒ¢ãƒ‡ãƒ«æ°¸ç¶šåŒ–
      - ollama_models:/usr/share/ollama/.ollama/models
    
    # Working directory
    working_dir: /workspace
    
    # Keep container running
    tty: true
    stdin_open: true
    
    # Restart policy
    restart: unless-stopped
    
    # Shared memory size for DataLoader
    shm_size: '32gb'
    
    # Ulimits for better performance
    ulimits:
      memlock:
        soft: -1
        hard: -1
      stack:
        soft: 67108864
        hard: 67108864

  # TensorBoard service (optional separate container)
  tensorboard:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    container_name: ai-ft-tensorboard
    command: tensorboard --logdir=/workspace/logs --host=0.0.0.0 --port=6006
    ports:
      - "6007:6006"  # Different port to avoid conflict
    volumes:
      - ./logs:/workspace/logs
    depends_on:
      - ai-ft
    profiles:
      - tensorboard

  # Jupyter Lab service (optional separate container)
  jupyter:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    container_name: ai-ft-jupyter
    command: jupyter lab --ip=0.0.0.0 --port=8888 --allow-root --no-browser
    ports:
      - "8889:8888"  # Different port to avoid conflict
    volumes:
      - ../notebooks:/workspace/notebooks
      - ../data:/workspace/data
      - ../src:/workspace/src
    depends_on:
      - ai-ft
    profiles:
      - jupyter

  # Qdrant Vector Database Service
  qdrant:
    image: qdrant/qdrant:latest
    container_name: ai-ft-qdrant
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant_storage:/qdrant/storage
    environment:
      - QDRANT__TELEMETRY_DISABLED=true
    restart: unless-stopped

# Networks
networks:
  default:
    name: ai-ft-network

# Volumes for persistent data
volumes:
  models_data:
    driver: local
  logs_data:
    driver: local
  qdrant_storage:
    driver: local
  ai_ft_rag_metadata:
    driver: local
  ai_ft_rag_processed:
    driver: local
  ollama_models:
    driver: local
EOF

echo "âœ“ docker-compose.yml ã‚’å¾©å…ƒã—ã¾ã—ãŸ"

# 3. Dockerfileã‚’å…ƒã«æˆ»ã™ï¼ˆã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆã‚’å‰Šé™¤ï¼‰
# Dockerfileã®æœ€å¾Œã®éƒ¨åˆ†ã‚’å…ƒã«æˆ»ã™
sed -i '/# ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ã‚³ãƒ”ãƒ¼/,/CMD \[\]/d' docker/Dockerfile

# å…ƒã®è¨­å®šã‚’è¿½åŠ 
cat >> docker/Dockerfile << 'EOF'

# ä½œæ¥­ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’åˆ‡ã‚Šæ›¿ãˆ
USER ai-user

# ãƒãƒ¼ãƒˆã‚’å…¬é–‹
EXPOSE 8888 6006 8050 8051

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚³ãƒžãƒ³ãƒ‰ï¼ˆbashã‚·ã‚§ãƒ«ã‚’èµ·å‹•ï¼‰
CMD ["/bin/bash"]
EOF

echo "âœ“ Dockerfile ã‚’å¾©å…ƒã—ã¾ã—ãŸ"

# 4. å…ƒã®èµ·å‹•ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å¾©å…ƒ
cat > scripts/start_web_interface.sh << 'EOF'
#!/bin/bash

# AI_FT_3 çµ±åˆWebã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹èµ·å‹•ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
# ç¶™ç¶šå­¦ç¿’ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã‚’å«ã‚€å…¨æ©Ÿèƒ½ã‚’èµ·å‹•

echo "ðŸš€ AI_FT_3 çµ±åˆWebã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’èµ·å‹•ä¸­..."

# Ollamaã‚µãƒ¼ãƒ“ã‚¹ã‚’èµ·å‹•ï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆï¼‰
if command -v ollama &> /dev/null; then
    echo "ðŸ¤– Ollamaã‚µãƒ¼ãƒ“ã‚¹ã‚’èµ·å‹•ä¸­..."
    nohup ollama serve > /dev/null 2>&1 &
    sleep 3
    echo "âœ… Ollamaã‚µãƒ¼ãƒ“ã‚¹ã‚’èµ·å‹•ã—ã¾ã—ãŸ (port 11434)"
    
    # Ollamaãƒ¢ãƒ‡ãƒ«ã®ç¢ºèªã¨è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    echo "ðŸ“¦ Ollamaãƒ¢ãƒ‡ãƒ«ã‚’ç¢ºèªä¸­..."
    if ! ollama list | grep -q "llama3.2:3b"; then
        echo "ðŸ“¥ llama3.2:3bãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­..."
        ollama pull llama3.2:3b
        echo "âœ… ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒå®Œäº†ã—ã¾ã—ãŸ"
    else
        echo "âœ… llama3.2:3bãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨å¯èƒ½ã§ã™"
    fi
fi

# ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¨­å®š
cd /workspace

# ãƒ‘ãƒ¼ãƒŸãƒƒã‚·ãƒ§ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯ãƒ»ä¿®æ­£
echo "ðŸ” ãƒ‘ãƒ¼ãƒŸãƒƒã‚·ãƒ§ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯ä¸­..."
if [ -f /workspace/scripts/setup_permissions.sh ]; then
    /workspace/scripts/setup_permissions.sh --check
    if [ $? -ne 0 ]; then
        echo "ðŸ”§ ãƒ‘ãƒ¼ãƒŸãƒƒã‚·ãƒ§ãƒ³ã‚’ä¿®æ­£ä¸­..."
        if [ "$EUID" -eq 0 ]; then
            /workspace/scripts/setup_permissions.sh
        else
            sudo /workspace/scripts/setup_permissions.sh 2>/dev/null || /workspace/scripts/setup_permissions.sh
        fi
    fi
else
    echo "âš ï¸ ãƒ‘ãƒ¼ãƒŸãƒƒã‚·ãƒ§ãƒ³è¨­å®šã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬çš„ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    mkdir -p /workspace/outputs
    mkdir -p /workspace/data/continual_learning
    mkdir -p /workspace/logs
    mkdir -p /workspace/temp_uploads
fi

# ç¶™ç¶šå­¦ç¿’ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–
echo "ðŸ“‹ ç¶™ç¶šå­¦ç¿’ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–ä¸­..."

# ç¶™ç¶šå­¦ç¿’ç”¨ã®è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
cat > /workspace/config/continual_learning_config.yaml << 'EOFCONFIG'
# ç¶™ç¶šå­¦ç¿’ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ è¨­å®š
continual_learning:
  enabled: true
  base_models:
    - cyberagent/calm3-22b-chat
    - cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese
    - Qwen/Qwen2.5-14B-Instruct
    - Qwen/Qwen2.5-32B-Instruct
  
  ewc_settings:
    default_lambda: 5000
    fisher_computation_batches: 100
    use_efficient_storage: true
  
  training_settings:
    default_epochs: 3
    default_learning_rate: 2e-5
    default_batch_size: 4
    max_sequence_length: 512

# ãƒ¢ãƒ‡ãƒ«ç®¡ç†è¨­å®š
model_management:
  auto_detect_finetuned: true
  include_base_models: true
  cache_enabled: true
EOFCONFIG

echo "âœ… ç¶™ç¶šå­¦ç¿’ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã®è¨­å®šã‚’å®Œäº†"

# Webã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•
echo "ðŸŒ çµ±åˆWebã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•ä¸­..."
echo "ðŸ“Š åˆ©ç”¨å¯èƒ½ãªæ©Ÿèƒ½:"
echo "  - ãƒ¡ã‚¤ãƒ³ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰: http://localhost:8050/"
echo "  - ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°: http://localhost:8050/finetune"
echo "  - ç¶™ç¶šå­¦ç¿’ç®¡ç†: http://localhost:8050/continual"
echo "  - RAGã‚·ã‚¹ãƒ†ãƒ : http://localhost:8050/rag"
echo "  - ãƒ¢ãƒ‡ãƒ«ç®¡ç†: http://localhost:8050/models"

# çµ±åˆWebã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•
exec python3 -m uvicorn app.main_unified:app --host 0.0.0.0 --port 8050 --reload
EOF

chmod +x scripts/start_web_interface.sh
echo "âœ“ scripts/start_web_interface.sh ã‚’å¾©å…ƒã—ã¾ã—ãŸ"

# 5. app/memory_optimized_loader.py ã®å…ƒã®ç’°å¢ƒå¤‰æ•°è¨­å®šã‚’å¾©å…ƒ
if [ -f app/memory_optimized_loader.py ]; then
    # CUDA_LAUNCH_BLOCKINGã‚’1ã«æˆ»ã™ï¼ˆå…ƒã®è¨­å®šï¼‰
    sed -i 's/os.environ\["CUDA_LAUNCH_BLOCKING"\] = "0"/os.environ["CUDA_LAUNCH_BLOCKING"] = "1"/g' app/memory_optimized_loader.py
    echo "âœ“ app/memory_optimized_loader.py ã®ç’°å¢ƒå¤‰æ•°ã‚’å¾©å…ƒ"
fi

echo ""
echo "================================"
echo "ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Œäº†"
echo "================================"
echo ""
echo "æ¬¡ã®ã‚³ãƒžãƒ³ãƒ‰ã§Dockerã‚’å†èµ·å‹•ã—ã¦ãã ã•ã„ï¼š"
echo ""
echo "cd docker"
echo "docker-compose down"
echo "docker-compose build"
echo "docker-compose up -d"
echo ""
echo "ãã®å¾Œã€ä»¥ä¸‹ã®ã‚³ãƒžãƒ³ãƒ‰ã§èµ·å‹•ï¼š"
echo "docker exec ai-ft-container bash /workspace/scripts/start_web_interface.sh"
echo ""
