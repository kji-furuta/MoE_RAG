#!/usr/bin/env python3
"""
WSLç’°å¢ƒç”¨ï¼šã‚·ãƒ³ãƒ—ãƒ«ãªOllamaã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã¨ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«å¤‰æ›
"""

import os
import subprocess
import sys
from pathlib import Path

def install_ollama_wsl():
    """WSLç’°å¢ƒã§Ollamaã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«"""
    print("WSLç’°å¢ƒã§Ollamaã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­...")
    
    try:
        # Ollamaã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
        subprocess.run(
            "curl -fsSL https://ollama.ai/install.sh | sh",
            shell=True, check=True
        )
        
        # ç’°å¢ƒå¤‰æ•°ã®è¨­å®š
        home = os.environ.get('HOME')
        os.environ['PATH'] = f"{home}/.local/bin:{os.environ.get('PATH')}"
        
        # ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç¢ºèª
        result = subprocess.run(["ollama", "--version"], capture_output=True, text=True)
        if result.returncode == 0:
            print(f"âœ… Ollamaã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æˆåŠŸ: {result.stdout.strip()}")
            return True
        else:
            print("âŒ Ollamaã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return False
            
    except Exception as e:
        print(f"âŒ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚¨ãƒ©ãƒ¼: {e}")
        return False

def convert_finetuned_model(model_path: str, model_name: str = "roadexpert"):
    """ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’Ollamaå½¢å¼ã«å¤‰æ›"""
    print(f"ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®å¤‰æ›é–‹å§‹: {model_path}")
    
    try:
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
        output_dir = Path("ollama_models")
        output_dir.mkdir(exist_ok=True)
        
        # GGUFãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        gguf_path = output_dir / f"{model_name}.gguf"
        
        # pipã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç¢ºèªã¨llama-cpp-pythonã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
        print("pipã¨llama-cpp-pythonã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­...")
        
        # pipã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
        try:
            subprocess.run([
                "sudo", "apt", "update"
            ], check=True)
            subprocess.run([
                "sudo", "apt", "install", "-y", "python3-pip"
            ], check=True)
        except subprocess.CalledProcessError:
            print("pipã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return False
        
        # llama-cpp-pythonã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
        subprocess.run([
            sys.executable, "-m", "pip", "install", "llama-cpp-python", "--break-system-packages"
        ], check=True)
        
        # GGUFå¤‰æ›ï¼ˆHugging Faceãƒ¢ãƒ‡ãƒ«ã‚’ç›´æ¥ä½¿ç”¨ï¼‰
        print("ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’Ollamaç”¨ã«æº–å‚™ä¸­...")
        
        # Ollamaã§ç›´æ¥ä½¿ç”¨ã™ã‚‹ãŸã‚ã®Modelfileã‚’ä½œæˆ
        # GGUFãƒ•ã‚¡ã‚¤ãƒ«ã®ä»£ã‚ã‚Šã«ã€Hugging Faceãƒ¢ãƒ‡ãƒ«ã‚’ç›´æ¥å‚ç…§
        modelfile_content = f"""FROM {model_path}
PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER top_k 40
PARAMETER repeat_penalty 1.1
PARAMETER stop "è³ªå•:"
PARAMETER stop "å›ç­”:"

SYSTEM "ã‚ãªãŸã¯é“è·¯å·¥å­¦ã®å°‚é–€å®¶ã§ã™ã€‚è³ªå•ã«å¯¾ã£ã¦æ­£ç¢ºã§åˆ†ã‹ã‚Šã‚„ã™ã„å›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚"
"""
        
        # Modelfileã‚’ä¿å­˜
        modelfile_path = output_dir / f"{model_name}.Modelfile"
        with open(modelfile_path, "w", encoding="utf-8") as f:
            f.write(modelfile_content)
        
        # æˆåŠŸã¨ã—ã¦æ‰±ã†
        result = type('Result', (), {'returncode': 0, 'stderr': ''})()
        print(f"âœ… ãƒ¢ãƒ‡ãƒ«æº–å‚™å®Œäº†: {modelfile_path}")
        
        if result.returncode == 0:
            # Ollamaãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ
            print("Ollamaãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆä¸­...")
            subprocess.run([
                "ollama", "create", model_name, "-f", str(modelfile_path)
            ], check=True)
            
            print(f"âœ… å¤‰æ›å®Œäº†ï¼ä½¿ç”¨æ–¹æ³•: ollama run {model_name}")
            return True
            
        else:
            print(f"âŒ GGUFå¤‰æ›ã‚¨ãƒ©ãƒ¼: {result.stderr}")
            return False
            
    except subprocess.TimeoutExpired:
        print("âŒ å¤‰æ›ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸ")
        return False
    except Exception as e:
        print(f"âŒ å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
        return False

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("WSLç’°å¢ƒã§ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«å¤‰æ›ã‚’é–‹å§‹ã—ã¾ã™")
    
    # 1. Ollamaã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
    if not install_ollama_wsl():
        print("Ollamaã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return
    
    # 2. ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®å¤‰æ›
    model_path = "/home/kjifuruta/Projects/AT_FT/AI_FT_3/outputs/ãƒ•ãƒ«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°_20250723_041920"
    
    if convert_finetuned_model(model_path):
        print("\n" + "="*50)
        print("ğŸ‰ WSLç’°å¢ƒã§ã®å¤‰æ›ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        print("="*50)
        print("ä½¿ç”¨æ–¹æ³•:")
        print("  ollama run roadexpert 'ç¸¦æ–­æ›²ç·šã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ'")
        print("  ollama run roadexpert 'é“è·¯ã®æ¨ªæ–­å‹¾é…ã®æ¨™æº–å€¤ã¯ï¼Ÿ'")
        print("="*50)
    else:
        print("âŒ å¤‰æ›ã«å¤±æ•—ã—ã¾ã—ãŸ")

if __name__ == "__main__":
    main() 