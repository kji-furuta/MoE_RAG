#!/bin/bash

# é«˜ã‚¹ãƒšãƒƒã‚¯ç’°å¢ƒç”¨èµ·å‹•ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
# ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’å¢—ã‚„ã—ã€ãƒ•ãƒ«ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨

echo "ğŸš€ Starting AI_FT_3 (High-Spec Mode)..."
echo "ğŸ’ª System Resources:"
echo "  - GPU: $(nvidia-smi --query-gpu=count --format=csv,noheader) devices"
echo "  - RAM: $(free -h | grep "^Mem:" | awk '{print $2}')"
echo "  - CPU: $(nproc) cores"

cd /workspace

# é«˜ã‚¹ãƒšãƒƒã‚¯ç’°å¢ƒç”¨ã®è¨­å®š
export PYTHONPATH=/workspace:$PYTHONPATH

# ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ï¼ˆå¤§å®¹é‡å‘ã‘ï¼‰
export PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True,max_split_size_mb:1024"
export OMP_NUM_THREADS=8
export MKL_NUM_THREADS=8

# ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®šã‚’å¤§å¹…ã«å¢—åŠ 
export TRANSFORMERS_TIMEOUT=600  # 10åˆ†
export HF_HUB_DOWNLOAD_TIMEOUT=600
export UVICORN_TIMEOUT_KEEP_ALIVE=300  # 5åˆ†
export REQUEST_TIMEOUT=300

# ãƒãƒƒãƒ•ã‚¡ã¨ãƒ­ã‚°è¨­å®š
export PYTHONUNBUFFERED=1
export TOKENIZERS_PARALLELISM=true  # é«˜ã‚¹ãƒšãƒƒã‚¯ãªã‚‰ä¸¦åˆ—åŒ–OK

# RAGã‚·ã‚¹ãƒ†ãƒ ã¯ãƒ•ãƒ«æ©Ÿèƒ½ã‚’ä½¿ç”¨
unset RAG_DISABLE_AUTO_INIT
unset RAG_USE_CPU
export RAG_DEVICE=cuda
export RAG_BATCH_SIZE=32

# å¿…è¦ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
mkdir -p outputs data logs temp_uploads config

# Ollamaãƒã‚§ãƒƒã‚¯
if command -v ollama &> /dev/null; then
    echo "ğŸ¤– Starting Ollama service..."
    nohup ollama serve > /tmp/ollama.log 2>&1 &
    sleep 2
fi

echo ""
echo "ğŸŒ Starting Full-Feature Web Server..."
echo "ğŸ“Š Available endpoints:"
echo "  - Dashboard: http://localhost:8050/"
echo "  - API Docs: http://localhost:8050/docs"
echo "  - Fine-tuning: http://localhost:8050/static/moe_training.html"
echo "  - RAG System: http://localhost:8050/static/moe_rag_ui.html"
echo ""

# æ—¢å­˜ãƒ—ãƒ­ã‚»ã‚¹ã‚’ã‚¯ãƒªã‚¢
if lsof -i:8050 > /dev/null 2>&1; then
    echo "âš ï¸ Clearing port 8050..."
    kill $(lsof -t -i:8050) 2>/dev/null || true
    sleep 2
fi

# uvicornã‚’é«˜ã‚¹ãƒšãƒƒã‚¯è¨­å®šã§èµ·å‹•
exec python3 -m uvicorn app.main_unified:app \
    --host 0.0.0.0 \
    --port 8050 \
    --workers 1 \
    --loop uvloop \
    --timeout-keep-alive 300 \
    --timeout-notify 300 \
    --limit-max-requests 0 \
    --limit-concurrency 100 \
    --backlog 2048 \
    --log-level info
