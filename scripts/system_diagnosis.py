#!/usr/bin/env python
"""
AI_FT_7 ã‚·ã‚¹ãƒ†ãƒ è¨ºæ–­ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ç¾åœ¨ã®ç’°å¢ƒçŠ¶æ…‹ã‚’è©³ç´°ã«ç¢ºèª
"""

import os
import sys
import torch
import psutil
import subprocess
from pathlib import Path
import json
from datetime import datetime

def check_system_info():
    """ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã®ç¢ºèª"""
    print("=" * 70)
    print("ğŸ–¥ï¸ System Information")
    print("=" * 70)
    
    # CPUæƒ…å ±
    print("\nğŸ“Š CPU Information:")
    print(f"  Physical cores: {psutil.cpu_count(logical=False)}")
    print(f"  Logical cores: {psutil.cpu_count(logical=True)}")
    print(f"  CPU usage: {psutil.cpu_percent(interval=1)}%")
    
    # ãƒ¡ãƒ¢ãƒªæƒ…å ±
    mem = psutil.virtual_memory()
    print(f"\nğŸ’¾ Memory Information:")
    print(f"  Total: {mem.total / (1024**3):.1f} GB")
    print(f"  Available: {mem.available / (1024**3):.1f} GB")
    print(f"  Used: {mem.used / (1024**3):.1f} GB ({mem.percent}%)")
    
    # ãƒ‡ã‚£ã‚¹ã‚¯æƒ…å ±
    print(f"\nğŸ’¿ Disk Information:")
    for partition in psutil.disk_partitions():
        if partition.mountpoint == '/' or '/workspace' in partition.mountpoint:
            usage = psutil.disk_usage(partition.mountpoint)
            print(f"  {partition.mountpoint}:")
            print(f"    Total: {usage.total / (1024**3):.1f} GB")
            print(f"    Free: {usage.free / (1024**3):.1f} GB")
            print(f"    Used: {usage.percent:.1f}%")

def check_gpu_info():
    """GPUæƒ…å ±ã®ç¢ºèª"""
    print("\n" + "=" * 70)
    print("ğŸ® GPU Information")
    print("=" * 70)
    
    if torch.cuda.is_available():
        print(f"\nâœ… CUDA is available")
        print(f"  CUDA version: {torch.version.cuda}")
        print(f"  PyTorch version: {torch.__version__}")
        print(f"  Number of GPUs: {torch.cuda.device_count()}")
        
        for i in range(torch.cuda.device_count()):
            props = torch.cuda.get_device_properties(i)
            print(f"\n  GPU {i}: {props.name}")
            print(f"    Memory: {props.total_memory / (1024**3):.1f} GB")
            print(f"    Compute Capability: {props.major}.{props.minor}")
            
            # ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
            if torch.cuda.is_available():
                allocated = torch.cuda.memory_allocated(i) / (1024**3)
                reserved = torch.cuda.memory_reserved(i) / (1024**3)
                print(f"    Allocated: {allocated:.2f} GB")
                print(f"    Reserved: {reserved:.2f} GB")
    else:
        print("âŒ CUDA is not available")

def check_python_packages():
    """ä¸»è¦ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèª"""
    print("\n" + "=" * 70)
    print("ğŸ“¦ Package Versions")
    print("=" * 70)
    
    packages = [
        "transformers",
        "peft",
        "accelerate",
        "bitsandbytes",
        "deepspeed",
        "fastapi",
        "uvicorn",
    ]
    
    for package in packages:
        try:
            module = __import__(package)
            version = getattr(module, "__version__", "unknown")
            print(f"  {package}: {version}")
        except ImportError:
            print(f"  {package}: âŒ Not installed")

def check_project_structure():
    """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ ã®ç¢ºèª"""
    print("\n" + "=" * 70)
    print("ğŸ“ Project Structure")
    print("=" * 70)
    
    base_path = Path("/home/kjifu/AI_FT_7")
    
    important_dirs = [
        "src/training",
        "src/rag",
        "app",
        "models",
        "outputs",
        "data",
        "configs",
        "docker",
    ]
    
    for dir_path in important_dirs:
        full_path = base_path / dir_path
        if full_path.exists():
            # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚µã‚¤ã‚ºã‚’è¨ˆç®—
            size = sum(f.stat().st_size for f in full_path.rglob('*') if f.is_file())
            size_gb = size / (1024**3)
            print(f"  âœ… {dir_path}: {size_gb:.2f} GB")
        else:
            print(f"  âŒ {dir_path}: Not found")

def check_model_files():
    """ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª"""
    print("\n" + "=" * 70)
    print("ğŸ¤– Model Files")
    print("=" * 70)
    
    models_dir = Path("/home/kjifu/AI_FT_7/models")
    outputs_dir = Path("/home/kjifu/AI_FT_7/outputs")
    
    print("\nğŸ“‚ Models directory:")
    if models_dir.exists():
        for model_path in models_dir.iterdir():
            if model_path.is_dir():
                size = sum(f.stat().st_size for f in model_path.rglob('*') if f.is_file())
                print(f"  - {model_path.name}: {size / (1024**3):.2f} GB")
    else:
        print("  Directory not found")
    
    print("\nğŸ“‚ Outputs directory:")
    if outputs_dir.exists():
        for output_path in outputs_dir.iterdir():
            if output_path.is_dir():
                size = sum(f.stat().st_size for f in output_path.rglob('*') if f.is_file())
                print(f"  - {output_path.name}: {size / (1024**3):.2f} GB")
    else:
        print("  Directory not found")

def check_docker_status():
    """DockerçŠ¶æ…‹ã®ç¢ºèª"""
    print("\n" + "=" * 70)
    print("ğŸ³ Docker Status")
    print("=" * 70)
    
    try:
        # Dockerã‚³ãƒ³ãƒ†ãƒŠã®ç¢ºèª
        result = subprocess.run(['docker', 'ps'], capture_output=True, text=True)
        if result.returncode == 0:
            print("âœ… Docker is running")
            if "ai-ft-container" in result.stdout:
                print("  âœ… AI-FT container is running")
            else:
                print("  âš ï¸ AI-FT container is not running")
        else:
            print("âŒ Docker is not accessible")
    except FileNotFoundError:
        print("âŒ Docker command not found")

def generate_report():
    """è¨ºæ–­ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
    print("\n" + "=" * 70)
    print("ğŸ“‹ Diagnosis Summary")
    print("=" * 70)
    
    # ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ãƒã‚§ãƒƒã‚¯
    workspace_usage = psutil.disk_usage('/home/kjifu/AI_FT_7')
    free_gb = workspace_usage.free / (1024**3)
    
    print("\nğŸ” Key Findings:")
    
    # ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ã®åˆ¤å®š
    if free_gb < 20:
        print(f"  âŒ Critical: Low disk space ({free_gb:.1f} GB free)")
        print("     â†’ Cannot perform full fine-tuning of 32B model")
        print("     â†’ Recommend: Use LoRA/DoRA or clean up space")
    elif free_gb < 85:
        print(f"  âš ï¸ Warning: Limited disk space ({free_gb:.1f} GB free)")
        print("     â†’ Full fine-tuning may fail")
        print("     â†’ Recommend: Use LoRA/DoRA or AWQ quantization")
    else:
        print(f"  âœ… Sufficient disk space ({free_gb:.1f} GB free)")
    
    # GPUçŠ¶æ…‹ã®åˆ¤å®š
    if torch.cuda.is_available():
        total_vram = sum(torch.cuda.get_device_properties(i).total_memory 
                        for i in range(torch.cuda.device_count()))
        total_vram_gb = total_vram / (1024**3)
        
        if total_vram_gb >= 48:
            print(f"  âœ… Sufficient VRAM ({total_vram_gb:.1f} GB total)")
            print("     â†’ Can run 32B model with optimization")
        else:
            print(f"  âš ï¸ Limited VRAM ({total_vram_gb:.1f} GB total)")
            print("     â†’ Recommend: Use quantization or smaller models")
    
    # æ¨å¥¨äº‹é …
    print("\nğŸ’¡ Recommendations:")
    print("  1. Implement DoRA for better accuracy (+3.7%)")
    print("  2. Use vLLM for faster inference (2.5-3x)")
    print("  3. Apply AWQ quantization to save memory (50%)")
    
    # ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜
    report = {
        "timestamp": datetime.now().isoformat(),
        "disk_free_gb": free_gb,
        "gpu_available": torch.cuda.is_available(),
        "gpu_count": torch.cuda.device_count() if torch.cuda.is_available() else 0,
    }
    
    report_path = Path("/home/kjifu/AI_FT_7/system_diagnosis.json")
    with open(report_path, 'w') as f:
        json.dump(report, f, indent=2)
    
    print(f"\nğŸ“ Report saved to: {report_path}")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    print("\n" + "ğŸ”§ AI_FT_7 System Diagnosis Tool ğŸ”§".center(70))
    print("=" * 70)
    
    check_system_info()
    check_gpu_info()
    check_python_packages()
    check_project_structure()
    check_model_files()
    check_docker_status()
    generate_report()
    
    print("\n" + "=" * 70)
    print("âœ… Diagnosis completed!")
    print("=" * 70)

if __name__ == "__main__":
    main()
