#!/usr/bin/env python3
"""
å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ï¼ˆ17B/32Bï¼‰ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ä¾‹
RTX A5000 x2ç’°å¢ƒã§ã®å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
"""

import torch
from src.models.japanese_model import JapaneseModel
from src.training.lora_finetuning import LoRAFinetuningTrainer, LoRAConfig
from src.training.training_utils import TrainingConfig
import logging

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def main():
    """å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    
    print("ğŸš€ å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ï¼ˆ17B/32Bï¼‰ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ä¾‹")
    print("=" * 60)
    
    # åˆ©ç”¨å¯èƒ½ãªå¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã‚’è¡¨ç¤º
    large_models = JapaneseModel.list_large_models()
    print(f"ğŸ“‹ åˆ©ç”¨å¯èƒ½ãªå¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ï¼ˆ17B+ï¼‰: {len(large_models)}å€‹")
    
    for model_name, config in large_models.items():
        print(f"  â€¢ {config['display_name']}")
        print(f"    å¿…è¦VRAM: {config['min_gpu_memory_gb']}GB")
        print(f"    èªè¨¼å¿…è¦: {config.get('requires_auth', False)}")
        print()
    
    # ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºåˆ¥ã®åˆ†é¡ã‚’è¡¨ç¤º
    models_by_size = JapaneseModel.list_models_by_size()
    print("ğŸ“Š ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºåˆ¥åˆ†é¡:")
    for size, models in models_by_size.items():
        if models:
            print(f"  {size}: {len(models)}å€‹")
    
    print("\n" + "=" * 60)
    
    # ä¾‹1: 17Bãƒ¢ãƒ‡ãƒ«ï¼ˆPhi-3.5-17Bï¼‰ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
    print("ğŸ¯ ä¾‹1: Microsoft Phi-3.5-17B ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°")
    print("-" * 40)
    
    # 17Bãƒ¢ãƒ‡ãƒ«ã®æƒ…å ±ã‚’å–å¾—
    model_info = JapaneseModel.get_model_info("microsoft/Phi-3.5-17B-Instruct")
    print(f"ãƒ¢ãƒ‡ãƒ«æƒ…å ±: {model_info['display_name']}")
    print(f"å¿…è¦VRAM: {model_info['min_gpu_memory_gb']}GB")
    print(f"æ¨å¥¨è¨­å®š: {model_info['recommended_training_config']}")
    
    # 17Bãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ï¼ˆQLoRAæ¨å¥¨ï¼‰
    model_17b = JapaneseModel(
        model_name="microsoft/Phi-3.5-17B-Instruct",
        load_in_8bit=True,  # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–
        gradient_checkpointing=True,  # è‡ªå‹•æœ‰åŠ¹åŒ–
        use_flash_attention=True
    )
    
    # QLoRAè¨­å®š
    qlora_config = LoRAConfig(
        r=8,                    # å°ã•ã„ãƒ©ãƒ³ã‚¯ã§ãƒ¡ãƒ¢ãƒªç¯€ç´„
        lora_alpha=16,
        target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
        lora_dropout=0.05,
        use_qlora=True,         # QLoRAæœ‰åŠ¹
        qlora_4bit=False        # 17Bãªã‚‰8bitã§ååˆ†
    )
    
    # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®š
    training_config = TrainingConfig(
        learning_rate=2e-4,     # å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ç”¨ã®ä½ã„å­¦ç¿’ç‡
        batch_size=2,           # å°ã•ã„ãƒãƒƒãƒã‚µã‚¤ã‚º
        gradient_accumulation_steps=8,  # å®ŸåŠ¹ãƒãƒƒãƒã‚µã‚¤ã‚º16
        num_epochs=3,           # å°‘ãªã„ã‚¨ãƒãƒƒã‚¯æ•°
        output_dir="./outputs/phi3.5_17b_qlora",
        gradient_checkpointing=True,
        fp16=True,
        save_steps=100,
        logging_steps=10
    )
    
    print(f"âœ… 17Bãƒ¢ãƒ‡ãƒ«è¨­å®šå®Œäº†")
    print(f"   LoRAãƒ©ãƒ³ã‚¯: {qlora_config.r}")
    print(f"   ãƒãƒƒãƒã‚µã‚¤ã‚º: {training_config.batch_size}")
    print(f"   å®ŸåŠ¹ãƒãƒƒãƒã‚µã‚¤ã‚º: {training_config.batch_size * training_config.gradient_accumulation_steps}")
    
    print("\n" + "=" * 60)
    
    # ä¾‹2: 32Bãƒ¢ãƒ‡ãƒ«ï¼ˆQwen2.5-32Bï¼‰ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
    print("ğŸ¯ ä¾‹2: Qwen 2.5 32B ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°")
    print("-" * 40)
    
    # 32Bãƒ¢ãƒ‡ãƒ«ã®æƒ…å ±ã‚’å–å¾—
    model_info_32b = JapaneseModel.get_model_info("Qwen/Qwen2.5-32B-Instruct")
    print(f"ãƒ¢ãƒ‡ãƒ«æƒ…å ±: {model_info_32b['display_name']}")
    print(f"å¿…è¦VRAM: {model_info_32b['min_gpu_memory_gb']}GB")
    print(f"æ¨å¥¨è¨­å®š: {model_info_32b['recommended_training_config']}")
    
    # 32Bãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ï¼ˆ4bit QLoRAå¿…é ˆï¼‰
    model_32b = JapaneseModel(
        model_name="Qwen/Qwen2.5-32B-Instruct",
        load_in_4bit=True,      # 4bité‡å­åŒ–å¿…é ˆ
        gradient_checkpointing=True,
        use_flash_attention=True
    )
    
    # 4bit QLoRAè¨­å®š
    qlora_config_32b = LoRAConfig(
        r=4,                    # ã•ã‚‰ã«å°ã•ã„ãƒ©ãƒ³ã‚¯
        lora_alpha=8,
        target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
        lora_dropout=0.05,
        use_qlora=True,
        qlora_4bit=True         # 4bité‡å­åŒ–
    )
    
    # 32Bç”¨ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®š
    training_config_32b = TrainingConfig(
        learning_rate=2e-4,
        batch_size=1,           # æœ€å°ãƒãƒƒãƒã‚µã‚¤ã‚º
        gradient_accumulation_steps=16,  # å®ŸåŠ¹ãƒãƒƒãƒã‚µã‚¤ã‚º16
        num_epochs=2,           # ã•ã‚‰ã«å°‘ãªã„ã‚¨ãƒãƒƒã‚¯æ•°
        output_dir="./outputs/qwen2.5_32b_qlora",
        gradient_checkpointing=True,
        fp16=True,
        save_steps=50,
        logging_steps=5
    )
    
    print(f"âœ… 32Bãƒ¢ãƒ‡ãƒ«è¨­å®šå®Œäº†")
    print(f"   LoRAãƒ©ãƒ³ã‚¯: {qlora_config_32b.r}")
    print(f"   4bité‡å­åŒ–: {qlora_config_32b.qlora_4bit}")
    print(f"   ãƒãƒƒãƒã‚µã‚¤ã‚º: {training_config_32b.batch_size}")
    print(f"   å®ŸåŠ¹ãƒãƒƒãƒã‚µã‚¤ã‚º: {training_config_32b.batch_size * training_config_32b.gradient_accumulation_steps}")
    
    print("\n" + "=" * 60)
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
    train_texts = [
        "è³ªå•: æ—¥æœ¬ã®é¦–éƒ½ã¯ã©ã“ã§ã™ã‹ï¼Ÿ\nå›ç­”: æ—¥æœ¬ã®é¦–éƒ½ã¯æ±äº¬ã§ã™ã€‚",
        "è³ªå•: å¯Œå£«å±±ã®é«˜ã•ã¯ä½•ãƒ¡ãƒ¼ãƒˆãƒ«ã§ã™ã‹ï¼Ÿ\nå›ç­”: å¯Œå£«å±±ã®é«˜ã•ã¯3,776ãƒ¡ãƒ¼ãƒˆãƒ«ã§ã™ã€‚",
        "è³ªå•: æ—¥æœ¬ã®å›½èŠ±ã¯ä½•ã§ã™ã‹ï¼Ÿ\nå›ç­”: æ—¥æœ¬ã®å›½èŠ±ã¯æ¡œã§ã™ã€‚",
        "è³ªå•: æ—¥æœ¬ã®äººå£ã¯ä½•äººã§ã™ã‹ï¼Ÿ\nå›ç­”: æ—¥æœ¬ã®äººå£ã¯ç´„1å„„2,500ä¸‡äººã§ã™ã€‚",
        "è³ªå•: æ—¥æœ¬ã®é€šè²¨ã¯ä½•ã§ã™ã‹ï¼Ÿ\nå›ç­”: æ—¥æœ¬ã®é€šè²¨ã¯å††ï¼ˆJPYï¼‰ã§ã™ã€‚"
    ]
    
    print("ğŸ“ ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†")
    print(f"   ãƒ‡ãƒ¼ã‚¿æ•°: {len(train_texts)}ä»¶")
    
    print("\n" + "=" * 60)
    print("ğŸš€ ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œä¾‹")
    print("-" * 40)
    
    # å®Ÿéš›ã®å®Ÿè¡Œã¯ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆï¼ˆæ™‚é–“ãŒã‹ã‹ã‚‹ãŸã‚ï¼‰
    print("""
# 17Bãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œä¾‹:
trainer_17b = LoRAFinetuningTrainer(
    model=model_17b,
    lora_config=qlora_config,
    training_config=training_config
)

# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ
trained_model_17b = trainer_17b.train(train_texts=train_texts)

# 32Bãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œä¾‹:
trainer_32b = LoRAFinetuningTrainer(
    model=model_32b,
    lora_config=qlora_config_32b,
    training_config=training_config_32b
)

# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ
trained_model_32b = trainer_32b.train(train_texts=train_texts)
""")
    
    print("=" * 60)
    print("ğŸ’¡ å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®ãƒã‚¤ãƒ³ãƒˆ:")
    print("â€¢ 17B+ãƒ¢ãƒ‡ãƒ«ã¯QLoRAå¿…é ˆ")
    print("â€¢ 32B+ãƒ¢ãƒ‡ãƒ«ã¯4bité‡å­åŒ–æ¨å¥¨")
    print("â€¢ ãƒãƒƒãƒã‚µã‚¤ã‚ºã¯1-2ã«åˆ¶é™")
    print("â€¢ Gradient Accumulationã§å®ŸåŠ¹ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’ç¢ºä¿")
    print("â€¢ Gradient Checkpointingã§ãƒ¡ãƒ¢ãƒªç¯€ç´„")
    print("â€¢ å­¦ç¿’ç‡ã¯2e-4ç¨‹åº¦ã«è¨­å®š")
    print("â€¢ ã‚¨ãƒãƒƒã‚¯æ•°ã¯2-3å›ç¨‹åº¦")
    
    print("\nğŸ‰ å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æº–å‚™å®Œäº†ï¼")

if __name__ == "__main__":
    main() 