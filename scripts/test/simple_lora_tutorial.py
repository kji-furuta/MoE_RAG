#!/usr/bin/env python3
"""
ç°¡å˜ãªLoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«
"""

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    get_linear_schedule_with_warmup
)
from peft import LoraConfig, get_peft_model, TaskType
import os
from datetime import datetime

class SimpleQADataset(Dataset):
    def __init__(self, texts, tokenizer, max_length=256):
        self.texts = texts
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': encoding['input_ids'].flatten()
        }

def main():
    print("ğŸš€ RTX A5000 x2 ç°¡å˜LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°")
    print("=" * 50)
    
    # GPUç¢ºèª
    device_count = torch.cuda.device_count()
    print(f"âœ… åˆ©ç”¨å¯èƒ½GPU: {device_count}å°")
    
    if device_count >= 2:
        print("   GPU 0: RTX A5000 (24GB)")
        print("   GPU 1: RTX A5000 (24GB)")
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"âœ… ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")
    
    # Step 1: ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®æº–å‚™
    print("\nğŸ“š Step 1: ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿")
    model_name = "rinna/japanese-gpt-neox-3.6b-instruction-sft"
    
    print(f"ãƒ¢ãƒ‡ãƒ«: {model_name}")
    print("èª­ã¿è¾¼ã¿ä¸­...")
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto",  # è‡ªå‹•çš„ã«GPUã«é…ç½®
            trust_remote_code=True
        )
        
        print("âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")
        
    except Exception as e:
        print(f"âŒ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        print("ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨...")
        
        model_name = "rinna/japanese-gpt-neox-small"
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        print("âœ… ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")
    
    # Step 2: LoRAè¨­å®š
    print("\nâš™ï¸ Step 2: LoRAè¨­å®š")
    
    peft_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        inference_mode=False,
        r=16,
        lora_alpha=32,
        lora_dropout=0.1,
        target_modules=["query_key_value", "dense", "dense_h_to_4h", "dense_4h_to_h"]
    )
    
    model = get_peft_model(model, peft_config)
    model.print_trainable_parameters()
    
    print("âœ… LoRAè¨­å®šå®Œäº†")
    
    # Step 3: è¨“ç·´ãƒ‡ãƒ¼ã‚¿æº–å‚™
    print("\nğŸ“ Step 3: è¨“ç·´ãƒ‡ãƒ¼ã‚¿æº–å‚™")
    
    train_texts = [
        "ãƒ¦ãƒ¼ã‚¶ãƒ¼: æ—¥æœ¬ã®é¦–éƒ½ã¯ã©ã“ã§ã™ã‹ï¼Ÿ\nã‚·ã‚¹ãƒ†ãƒ : æ—¥æœ¬ã®é¦–éƒ½ã¯æ±äº¬ã§ã™ã€‚",
        "ãƒ¦ãƒ¼ã‚¶ãƒ¼: å¯Œå£«å±±ã®é«˜ã•ã¯ä½•ãƒ¡ãƒ¼ãƒˆãƒ«ã§ã™ã‹ï¼Ÿ\nã‚·ã‚¹ãƒ†ãƒ : å¯Œå£«å±±ã®é«˜ã•ã¯3,776ãƒ¡ãƒ¼ãƒˆãƒ«ã§ã™ã€‚",
        "ãƒ¦ãƒ¼ã‚¶ãƒ¼: ä»Šæ—¥ã¯ã„ã„å¤©æ°—ã§ã™ã­ã€‚\nã‚·ã‚¹ãƒ†ãƒ : ãã†ã§ã™ã­ã€‚ã¨ã¦ã‚‚è‰¯ã„å¤©æ°—ã§æ°—æŒã¡ãŒã„ã„ã§ã™ã€‚",
        "ãƒ¦ãƒ¼ã‚¶ãƒ¼: ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸã€‚\nã‚·ã‚¹ãƒ†ãƒ : ã©ã†ã„ãŸã—ã¾ã—ã¦ã€‚ãŠå½¹ã«ç«‹ã¦ã¦å¬‰ã—ã„ã§ã™ã€‚",
        "ãƒ¦ãƒ¼ã‚¶ãƒ¼: ãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™ã€‚\nã‚·ã‚¹ãƒ†ãƒ : ãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™ã€‚ä»Šæ—¥ã‚‚ä¸€æ—¥ã‚ˆã‚ã—ããŠé¡˜ã„ã—ã¾ã™ã€‚",
    ]
    
    dataset = SimpleQADataset(train_texts, tokenizer)
    dataloader = DataLoader(dataset, batch_size=1, shuffle=True)
    
    print(f"âœ… è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(train_texts)}ä»¶")
    
    # Step 4: è¨“ç·´è¨­å®š
    print("\nğŸ“Š Step 4: è¨“ç·´è¨­å®š")
    
    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)
    num_epochs = 3
    
    print(f"âœ… ã‚¨ãƒãƒƒã‚¯æ•°: {num_epochs}")
    print(f"âœ… å­¦ç¿’ç‡: 3e-4")
    print(f"âœ… ãƒãƒƒãƒã‚µã‚¤ã‚º: 1")
    
    # Step 5: è¨“ç·´å®Ÿè¡Œ
    print("\nğŸ‹ï¸ Step 5: è¨“ç·´å®Ÿè¡Œ")
    
    model.train()
    total_loss = 0
    step = 0
    
    for epoch in range(num_epochs):
        print(f"\nã‚¨ãƒãƒƒã‚¯ {epoch + 1}/{num_epochs}")
        epoch_loss = 0
        
        for batch in dataloader:
            batch = {k: v.to(device) for k, v in batch.items()}
            
            optimizer.zero_grad()
            
            outputs = model(**batch)
            loss = outputs.loss
            
            loss.backward()
            optimizer.step()
            
            epoch_loss += loss.item()
            total_loss += loss.item()
            step += 1
            
            if step % 2 == 0:
                print(f"  ã‚¹ãƒ†ãƒƒãƒ— {step}: Loss = {loss.item():.4f}")
        
        avg_epoch_loss = epoch_loss / len(dataloader)
        print(f"ã‚¨ãƒãƒƒã‚¯ {epoch + 1} å¹³å‡Loss: {avg_epoch_loss:.4f}")
    
    print("\nâœ… è¨“ç·´å®Œäº†ï¼")
    
    # Step 6: ãƒ†ã‚¹ãƒˆç”Ÿæˆ
    print("\nğŸ§ª Step 6: ãƒ†ã‚¹ãƒˆç”Ÿæˆ")
    
    model.eval()
    test_input = "ãƒ¦ãƒ¼ã‚¶ãƒ¼: ã“ã‚“ã«ã¡ã¯\nã‚·ã‚¹ãƒ†ãƒ :"
    
    inputs = tokenizer(test_input, return_tensors="pt").to(device)
    
    with torch.no_grad():
        outputs = model.generate(
            inputs['input_ids'],
            max_new_tokens=30,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(f"å…¥åŠ›: {test_input}")
    print(f"å‡ºåŠ›: {response}")
    
    # Step 7: ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    print("\nğŸ’¾ Step 7: ãƒ¢ãƒ‡ãƒ«ä¿å­˜")
    
    output_dir = f"./lora_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)
    
    print(f"âœ… ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†: {output_dir}")
    
    return True

def show_next_steps():
    print("\nğŸ¯ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
    print("=" * 30)
    print("1. ã‚ˆã‚Šå¤§ããªãƒ¢ãƒ‡ãƒ«ã‚’è©¦ã™:")
    print("   - elyza/Llama-3-ELYZA-JP-8B")
    print("   - stabilityai/japanese-stablelm-3b-4e1t-instruct")
    
    print("\n2. QLoRAã§å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«:")
    print("   - 13Bä»¥ä¸Šã®ãƒ¢ãƒ‡ãƒ«")
    print("   - 4bité‡å­åŒ–ä½¿ç”¨")
    
    print("\n3. ãƒãƒ«ãƒGPUè¨­å®š:")
    print("   - device_map='auto'")
    print("   - Model Parallelism")
    
    print("\n4. ç‹¬è‡ªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ:")
    print("   - CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿")
    print("   - ã‚ˆã‚Šå¤šãã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿")

if __name__ == "__main__":
    try:
        success = main()
        if success:
            show_next_steps()
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()