#!/usr/bin/env python3
"""
LoRAãƒ¢ãƒ‡ãƒ«ãƒ‰ãƒ­ãƒƒãƒ—ãƒ€ã‚¦ãƒ³ãƒ†ã‚¹ãƒˆ
APIãŒæ­£ã—ããƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’è¿”ã™ã‹ç¢ºèª
"""

import json
import requests
from pathlib import Path

def test_api_response():
    """APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ†ã‚¹ãƒˆ"""
    
    print("="*60)
    print("LoRAãƒ¢ãƒ‡ãƒ«ãƒ‰ãƒ­ãƒƒãƒ—ãƒ€ã‚¦ãƒ³APIãƒ†ã‚¹ãƒˆ")
    print("="*60)
    
    # APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
    url = "http://localhost:8050/rag/list-lora-models"
    
    try:
        print(f"\nAPIå‘¼ã³å‡ºã—: {url}")
        response = requests.get(url, timeout=10)
        
        if response.status_code == 200:
            data = response.json()
            
            print("\nâœ… APIãƒ¬ã‚¹ãƒãƒ³ã‚¹æˆåŠŸ")
            print(f"ç·ãƒ¢ãƒ‡ãƒ«æ•°: {data.get('count', 0)}")
            
            # ã‚µãƒãƒªãƒ¼æƒ…å ±
            if data.get('summary'):
                print("\n[ã‚µãƒãƒªãƒ¼]")
                summary = data['summary']
                print(f"  ç·æ•°: {summary.get('total', 0)}")
                print(f"  ä½¿ç”¨å¯èƒ½: {summary.get('ready_to_use', 0)}")
                print(f"  LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼: {summary.get('lora_adapters', 0)}")
                print(f"  ãƒãƒ¼ã‚¸æ¸ˆã¿: {summary.get('merged_models', 0)}")
                print(f"  ç¶™ç¶šå­¦ç¿’: {summary.get('continual_models', 0)}")
                print(f"  GGUF: {summary.get('gguf_models', 0)}")
            
            # ãƒ¢ãƒ‡ãƒ«è©³ç´°
            if data.get('models'):
                print("\n[æ¤œå‡ºã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«]")
                
                # ã‚¿ã‚¤ãƒ—ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã¦è¡¨ç¤º
                model_by_type = {}
                for model in data['models']:
                    model_type = model.get('type', 'unknown')
                    if model_type not in model_by_type:
                        model_by_type[model_type] = []
                    model_by_type[model_type].append(model)
                
                type_labels = {
                    'ollama_ready': 'âœ… Ollamaç™»éŒ²æ¸ˆã¿',
                    'gguf_model': 'ğŸ“¦ GGUFå½¢å¼',
                    'merged_model': 'ğŸ”€ ãƒãƒ¼ã‚¸æ¸ˆã¿',
                    'lora_adapter': 'ğŸ¯ LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼',
                    'continual_model': 'ğŸ“š ç¶™ç¶šå­¦ç¿’',
                    'auto': 'ğŸ” è‡ªå‹•æ¤œå‡º'
                }
                
                for model_type, models in model_by_type.items():
                    print(f"\n{type_labels.get(model_type, model_type)}:")
                    for model in models[:3]:  # å„ã‚¿ã‚¤ãƒ—æœ€å¤§3å€‹è¡¨ç¤º
                        print(f"  - {model.get('display_name', model.get('name', 'Unknown'))}")
                        print(f"    ãƒ‘ã‚¹: {model.get('path', 'Unknown')}")
                        if model.get('base_model'):
                            print(f"    ãƒ™ãƒ¼ã‚¹: {model['base_model']}")
                        if model.get('needs_processing'):
                            print(f"    è¦å‡¦ç†: {model['needs_processing']}")
                        if model.get('recommended'):
                            print(f"    â­ æ¨å¥¨")
            else:
                print("\nâš ï¸ ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                print("  è‡ªå‹•æ¤œå‡ºãƒ¢ãƒ¼ãƒ‰ãŒä½¿ç”¨ã•ã‚Œã¾ã™")
            
            # JSONãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
            debug_file = Path("debug_lora_api_response.json")
            with open(debug_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
            print(f"\nãƒ‡ãƒãƒƒã‚°ç”¨JSONã‚’ä¿å­˜: {debug_file}")
            
        else:
            print(f"\nâŒ APIã‚¨ãƒ©ãƒ¼: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ {response.status_code}")
            print(f"ãƒ¬ã‚¹ãƒãƒ³ã‚¹: {response.text[:500]}")
            
    except requests.exceptions.ConnectionError:
        print("\nâŒ æ¥ç¶šã‚¨ãƒ©ãƒ¼")
        print("ã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„:")
        print("docker exec ai-ft-container python -m uvicorn app.main_unified:app --host 0.0.0.0 --port 8050")
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼: {e}")

def create_test_models():
    """ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ"""
    
    print("\n" + "="*60)
    print("ãƒ†ã‚¹ãƒˆç”¨ãƒ¢ãƒ‡ãƒ«ä½œæˆ")
    print("="*60)
    
    outputs_dir = Path("/workspace/outputs")
    
    # ãƒ†ã‚¹ãƒˆç”¨LoRAãƒ¢ãƒ‡ãƒ«
    test_lora = outputs_dir / "lora_test_deepseek"
    test_lora.mkdir(parents=True, exist_ok=True)
    
    adapter_config = {
        "base_model_name_or_path": "cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese",
        "r": 16,
        "lora_alpha": 32,
        "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"]
    }
    
    with open(test_lora / "adapter_config.json", 'w') as f:
        json.dump(adapter_config, f, indent=2)
    
    print(f"âœ… ãƒ†ã‚¹ãƒˆLoRAãƒ¢ãƒ‡ãƒ«ä½œæˆ: {test_lora}")
    
    # ãƒ†ã‚¹ãƒˆç”¨ãƒãƒ¼ã‚¸æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
    test_merged = outputs_dir / "merged_model"
    test_merged.mkdir(parents=True, exist_ok=True)
    
    config = {
        "model_type": "qwen2",
        "hidden_size": 4096,
        "num_hidden_layers": 32
    }
    
    with open(test_merged / "config.json", 'w') as f:
        json.dump(config, f, indent=2)
    
    print(f"âœ… ãƒ†ã‚¹ãƒˆãƒãƒ¼ã‚¸ãƒ¢ãƒ‡ãƒ«ä½œæˆ: {test_merged}")
    
    # ãƒ†ã‚¹ãƒˆç”¨GGUFãƒ•ã‚¡ã‚¤ãƒ«
    test_gguf = outputs_dir / "test_model.gguf"
    test_gguf.touch()
    print(f"âœ… ãƒ†ã‚¹ãƒˆGGUFãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ: {test_gguf}")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "--create-test":
        create_test_models()
    
    test_api_response()