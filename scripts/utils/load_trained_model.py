#!/usr/bin/env python3
"""
ä¿å­˜ã•ã‚ŒãŸLoRAãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã¨åˆ©ç”¨ä¾‹
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import os

def load_and_use_trained_model():
    print("ğŸ”„ ä¿å­˜ã•ã‚ŒãŸLoRAãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã¨åˆ©ç”¨")
    print("=" * 50)
    
    # ä¿å­˜ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹
    base_model_name = "distilgpt2"
    adapter_path = "/workspace/lora_demo_20250626_074248"
    
    print(f"ğŸ“š ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«: {base_model_name}")
    print(f"ğŸ“ ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ãƒ‘ã‚¹: {adapter_path}")
    
    try:
        # Step 1: ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿
        print("\nğŸ”§ Step 1: ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿")
        tokenizer = AutoTokenizer.from_pretrained(base_model_name)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        print("âœ… ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")
        
        # Step 2: LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®èª­ã¿è¾¼ã¿
        print("\nğŸ¯ Step 2: LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼èª­ã¿è¾¼ã¿")
        model = PeftModel.from_pretrained(base_model, adapter_path)
        print("âœ… LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼èª­ã¿è¾¼ã¿å®Œäº†")
        
        # Step 3: ãƒ†ã‚¹ãƒˆç”Ÿæˆ
        print("\nğŸ§ª Step 3: ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã§ãƒ†ã‚¹ãƒˆ")
        
        test_inputs = [
            "Hello, how are",
            "The weather is",
            "I love machine",
            "RTX A5000 is",
            "Fine-tuning is"
        ]
        
        model.eval()
        
        for test_input in test_inputs:
            inputs = tokenizer(test_input, return_tensors="pt").to(model.device)
            
            with torch.no_grad():
                outputs = model.generate(
                    inputs['input_ids'],
                    max_new_tokens=20,
                    do_sample=True,
                    temperature=0.7,
                    pad_token_id=tokenizer.eos_token_id
                )
            
            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
            print(f"å…¥åŠ›: '{test_input}'")
            print(f"å‡ºåŠ›: '{generated_text}'")
            print("-" * 40)
        
        # Step 4: ãƒ¢ãƒ‡ãƒ«æƒ…å ±è¡¨ç¤º
        print("\nğŸ“Š Step 4: ãƒ¢ãƒ‡ãƒ«æƒ…å ±")
        model.print_trainable_parameters()
        
        # Step 5: ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ†ã‚¹ãƒˆ
        print("\nğŸ’¬ Step 5: ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ†ã‚¹ãƒˆ")
        print("ï¼ˆ'quit'ã§çµ‚äº†ï¼‰")
        
        while True:
            try:
                user_input = input("\nå…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ: ")
                if user_input.lower() == 'quit':
                    break
                
                inputs = tokenizer(user_input, return_tensors="pt").to(model.device)
                
                with torch.no_grad():
                    outputs = model.generate(
                        inputs['input_ids'],
                        max_new_tokens=30,
                        do_sample=True,
                        temperature=0.8,
                        pad_token_id=tokenizer.eos_token_id
                    )
                
                generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
                print(f"ç”Ÿæˆ: {generated_text}")
                
            except KeyboardInterrupt:
                break
        
        print("\nâœ… ãƒ¢ãƒ‡ãƒ«åˆ©ç”¨å®Œäº†ï¼")
        return True
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return False

def compare_base_vs_finetuned():
    """ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¨ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒ"""
    print("\nğŸ” ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ« vs ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ")
    print("=" * 60)
    
    base_model_name = "distilgpt2"
    adapter_path = "/workspace/lora_demo_20250626_074248"
    
    try:
        # ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«
        print("ğŸ“š ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿...")
        tokenizer = AutoTokenizer.from_pretrained(base_model_name)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        
        # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
        print("ğŸ¯ ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿...")
        finetuned_model = PeftModel.from_pretrained(base_model, adapter_path)
        
        # æ¯”è¼ƒãƒ†ã‚¹ãƒˆ
        test_input = "Hello, how are"
        inputs = tokenizer(test_input, return_tensors="pt").to(base_model.device)
        
        print(f"\nğŸ§ª æ¯”è¼ƒãƒ†ã‚¹ãƒˆ: '{test_input}'")
        print("-" * 40)
        
        # ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã§ã®ç”Ÿæˆ
        base_model.eval()
        with torch.no_grad():
            outputs = base_model.generate(
                inputs['input_ids'],
                max_new_tokens=20,
                do_sample=True,
                temperature=0.7,
                pad_token_id=tokenizer.eos_token_id
            )
        
        base_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        print(f"ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«: {base_text}")
        
        # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã§ã®ç”Ÿæˆ
        finetuned_model.eval()
        with torch.no_grad():
            outputs = finetuned_model.generate(
                inputs['input_ids'],
                max_new_tokens=20,
                do_sample=True,
                temperature=0.7,
                pad_token_id=tokenizer.eos_token_id
            )
        
        finetuned_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        print(f"ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿: {finetuned_text}")
        
        print("\nâœ… æ¯”è¼ƒå®Œäº†")
        
    except Exception as e:
        print(f"âŒ æ¯”è¼ƒã‚¨ãƒ©ãƒ¼: {e}")

def show_model_files():
    """ä¿å­˜ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®è©³ç´°"""
    print("\nğŸ“ ä¿å­˜ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®è©³ç´°")
    print("=" * 50)
    
    adapter_path = "/workspace/lora_demo_20250626_074248"
    
    try:
        import json
        
        # adapter_config.jsonã®å†…å®¹
        config_path = f"{adapter_path}/adapter_config.json"
        if os.path.exists(config_path):
            with open(config_path, 'r') as f:
                config = json.load(f)
            
            print("âš™ï¸ LoRAè¨­å®š:")
            for key, value in config.items():
                print(f"   {key}: {value}")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºæƒ…å ±
        print("\nğŸ“Š ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º:")
        import subprocess
        result = subprocess.run(['ls', '-lh', adapter_path], capture_output=True, text=True)
        print(result.stdout)
        
    except Exception as e:
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")

def main():
    print("ğŸ¯ ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®æ´»ç”¨ã‚¬ã‚¤ãƒ‰")
    print("=" * 60)
    
    # ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±
    show_model_files()
    
    # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã¨ä½¿ç”¨
    success = load_and_use_trained_model()
    
    if success:
        # ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¨ã®æ¯”è¼ƒ
        compare_base_vs_finetuned()
    
    print("\n" + "=" * 60)
    print("ğŸ’¡ æ´»ç”¨æ–¹æ³•ã¾ã¨ã‚")
    print("=" * 60)
    print("1. ä¿å­˜å ´æ‰€: /workspace/lora_demo_YYYYMMDD_HHMMSS/")
    print("2. èª­ã¿è¾¼ã¿: PeftModel.from_pretrained(base_model, adapter_path)")
    print("3. åˆ©ç”¨: é€šå¸¸ã®Transformersãƒ¢ãƒ‡ãƒ«ã¨åŒæ§˜")
    print("4. é…å¸ƒ: adapter_model.safetensorsã®ã¿é€ä»˜å¯èƒ½ï¼ˆç´„1.6MBï¼‰")
    print("5. æœ¬ç•ªé‹ç”¨: APIã‚µãƒ¼ãƒãƒ¼ã‚„Webã‚¢ãƒ—ãƒªã«çµ±åˆå¯èƒ½")

if __name__ == "__main__":
    main()