#!/usr/bin/env python3
"""
DeepSeek-R1-Distill-Qwen-32B-Japanese é‡å­åŒ–æ¤œè¨¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
32Bãƒ¢ãƒ‡ãƒ«ã®é‡å­åŒ–ãƒ—ãƒ­ã‚»ã‚¹ã‚’è©³ç´°ã«æ¤œè¨¼
"""

import os
import sys
import json
import torch
import gc
import psutil
import subprocess
from pathlib import Path
from datetime import datetime
import logging

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹ã‚’è¿½åŠ 
sys.path.append('/workspace')
from app.memory_optimized_loader import get_optimal_quantization_config

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class DeepSeek32BValidator:
    """DeepSeek-R1-Distill-Qwen-32Bé‡å­åŒ–æ¤œè¨¼"""
    
    def __init__(self):
        self.model_name = "cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese"
        self.report = {
            "model": self.model_name,
            "timestamp": datetime.now().isoformat(),
            "checks": [],
            "memory_usage": {},
            "recommendations": []
        }
    
    def check_system_resources(self):
        """ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ã‚’ãƒã‚§ãƒƒã‚¯"""
        print("\n" + "="*60)
        print("1. ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ãƒã‚§ãƒƒã‚¯")
        print("="*60)
        
        # CPU/ãƒ¡ãƒ¢ãƒªæƒ…å ±
        cpu_count = psutil.cpu_count()
        ram_gb = psutil.virtual_memory().total / (1024**3)
        available_ram_gb = psutil.virtual_memory().available / (1024**3)
        
        print(f"CPU ã‚³ã‚¢æ•°: {cpu_count}")
        print(f"RAM ç·å®¹é‡: {ram_gb:.1f} GB")
        print(f"RAM åˆ©ç”¨å¯èƒ½: {available_ram_gb:.1f} GB")
        
        # GPUæƒ…å ±
        gpu_info = []
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                props = torch.cuda.get_device_properties(i)
                total_gb = props.total_memory / (1024**3)
                free_gb = (props.total_memory - torch.cuda.memory_allocated(i)) / (1024**3)
                gpu_info.append({
                    "id": i,
                    "name": props.name,
                    "total_gb": total_gb,
                    "free_gb": free_gb
                })
                print(f"GPU {i}: {props.name} - {total_gb:.1f} GB (ç©ºã: {free_gb:.1f} GB)")
        else:
            print("GPU: åˆ©ç”¨ä¸å¯")
        
        # 32Bãƒ¢ãƒ‡ãƒ«è¦ä»¶ãƒã‚§ãƒƒã‚¯
        print("\n[32Bãƒ¢ãƒ‡ãƒ«è¦ä»¶]")
        print("- FP16: 64GB VRAMï¼ˆæ¨å¥¨ï¼‰")
        print("- 4bité‡å­åŒ–: 16GB VRAMï¼ˆæœ€å°ï¼‰")
        print("- GGUFå¤‰æ›: 64GB ãƒ‡ã‚£ã‚¹ã‚¯ç©ºãå®¹é‡")
        
        # åˆ¤å®š
        total_gpu_memory = sum(g["total_gb"] for g in gpu_info)
        check_result = {
            "ram_gb": ram_gb,
            "available_ram_gb": available_ram_gb,
            "gpu_count": len(gpu_info),
            "total_gpu_memory_gb": total_gpu_memory,
            "gpu_details": gpu_info
        }
        
        self.report["memory_usage"]["system"] = check_result
        
        # æ¨å¥¨äº‹é …
        if total_gpu_memory < 48:
            self.report["recommendations"].append(
                "âš ï¸ GPU VRAMãŒ48GBæœªæº€: 4bité‡å­åŒ–ï¼ˆQLoRAï¼‰ãŒå¿…é ˆã§ã™"
            )
        
        if available_ram_gb < 32:
            self.report["recommendations"].append(
                "âš ï¸ RAMç©ºãå®¹é‡ãŒ32GBæœªæº€: ã‚¹ãƒ¯ãƒƒãƒ—ä½¿ç”¨ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™"
            )
        
        return check_result
    
    def check_quantization_config(self):
        """é‡å­åŒ–è¨­å®šã‚’ãƒã‚§ãƒƒã‚¯"""
        print("\n" + "="*60)
        print("2. é‡å­åŒ–è¨­å®šãƒã‚§ãƒƒã‚¯")
        print("="*60)
        
        # ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–è¨­å®šã‚’å–å¾—
        config, device_map = get_optimal_quantization_config(self.model_name)
        
        print("\n[è‡ªå‹•é¸æŠã•ã‚ŒãŸè¨­å®š]")
        print(f"é‡å­åŒ–ã‚¿ã‚¤ãƒ—: {'4bit' if hasattr(config, 'load_in_4bit') and config.load_in_4bit else '8bit'}")
        
        if hasattr(config, 'load_in_4bit') and config.load_in_4bit:
            print(f"- compute_dtype: {config.bnb_4bit_compute_dtype}")
            print(f"- quant_type: {config.bnb_4bit_quant_type}")
            print(f"- double_quant: {config.bnb_4bit_use_double_quant}")
            print(f"- cpu_offload: {config.llm_int8_enable_fp32_cpu_offload}")
        
        print(f"Device map: {device_map}")
        
        check_result = {
            "quantization": "4bit" if hasattr(config, 'load_in_4bit') and config.load_in_4bit else "8bit",
            "config_details": str(config),
            "device_map": str(device_map)
        }
        
        self.report["checks"].append({
            "name": "quantization_config",
            "status": "âœ… 32Bãƒ¢ãƒ‡ãƒ«ç”¨ã«4bité‡å­åŒ–ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã™" if hasattr(config, 'load_in_4bit') and config.load_in_4bit else "âš ï¸ 8bité‡å­åŒ–ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã™",
            "details": check_result
        })
        
        return check_result
    
    def check_lora_workflow(self):
        """LoRAãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’ãƒã‚§ãƒƒã‚¯"""
        print("\n" + "="*60)
        print("3. LoRAãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãƒã‚§ãƒƒã‚¯")
        print("="*60)
        
        workflow_steps = [
            {
                "step": "ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°",
                "description": "QLoRAï¼ˆ4bitï¼‰ã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°",
                "memory_requirement": "16-20GB VRAM",
                "status": "OK"
            },
            {
                "step": "LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ä¿å­˜",
                "description": "ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®ã¿ä¿å­˜ï¼ˆãƒãƒ¼ã‚¸ãªã—ï¼‰",
                "memory_requirement": "< 1GB",
                "status": "OK"
            },
            {
                "step": "ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«å†ãƒ­ãƒ¼ãƒ‰",
                "description": "4bité‡å­åŒ–ã§ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰",
                "memory_requirement": "16GB VRAM",
                "status": "OK"
            },
            {
                "step": "ãƒãƒ¼ã‚¸å‡¦ç†",
                "description": "LoRA + ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ« â†’ FP16",
                "memory_requirement": "32-40GB RAM (CPUå‡¦ç†)",
                "status": "æ³¨æ„"
            },
            {
                "step": "GGUFå¤‰æ›",
                "description": "FP16 â†’ GGUF F16",
                "memory_requirement": "64GB ãƒ‡ã‚£ã‚¹ã‚¯",
                "status": "OK"
            },
            {
                "step": "é‡å­åŒ–",
                "description": "GGUF F16 â†’ Q4_K_M",
                "memory_requirement": "CPUå‡¦ç†",
                "status": "OK"
            },
            {
                "step": "Ollamaç™»éŒ²",
                "description": "Q4_K_Mãƒ¢ãƒ‡ãƒ«ã‚’ç™»éŒ²",
                "memory_requirement": "16GB ãƒ‡ã‚£ã‚¹ã‚¯",
                "status": "OK"
            }
        ]
        
        for i, step in enumerate(workflow_steps, 1):
            status_icon = "âœ…" if step["status"] == "OK" else "âš ï¸"
            print(f"\n[Step {i}] {step['step']}")
            print(f"  {status_icon} {step['description']}")
            print(f"  ãƒ¡ãƒ¢ãƒªè¦ä»¶: {step['memory_requirement']}")
        
        self.report["checks"].append({
            "name": "lora_workflow",
            "status": "âœ… ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ç¢ºèªå®Œäº†",
            "workflow": workflow_steps
        })
        
        return workflow_steps
    
    def check_script_compatibility(self):
        """ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®äº’æ›æ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
        print("\n" + "="*60)
        print("4. ã‚¹ã‚¯ãƒªãƒ—ãƒˆäº’æ›æ€§ãƒã‚§ãƒƒã‚¯")
        print("="*60)
        
        scripts_to_check = [
            {
                "path": "/workspace/scripts/qlora_to_ollama.py",
                "purpose": "QLoRA â†’ Ollamaå¤‰æ›",
                "32b_support": None
            },
            {
                "path": "/workspace/scripts/unified_model_processor.py",
                "purpose": "çµ±åˆãƒ¢ãƒ‡ãƒ«å‡¦ç†",
                "32b_support": None
            },
            {
                "path": "/workspace/scripts/proper_merge_flow.py",
                "purpose": "FP16ãƒãƒ¼ã‚¸å‡¦ç†",
                "32b_support": None
            }
        ]
        
        for script in scripts_to_check:
            if Path(script["path"]).exists():
                # ã‚¹ã‚¯ãƒªãƒ—ãƒˆå†…å®¹ã‚’ãƒã‚§ãƒƒã‚¯
                with open(script["path"], 'r') as f:
                    content = f.read()
                
                # 32Bãƒ¢ãƒ‡ãƒ«å¯¾å¿œãƒã‚§ãƒƒã‚¯
                has_32b_check = "32b" in content.lower() or "32B" in content
                has_4bit_fallback = "load_in_4bit" in content
                has_memory_optimization = "memory_optimized_loader" in content or "get_optimal_quantization_config" in content
                
                script["32b_support"] = {
                    "exists": True,
                    "has_32b_check": has_32b_check,
                    "has_4bit_fallback": has_4bit_fallback,
                    "has_memory_optimization": has_memory_optimization,
                    "compatible": has_32b_check or has_4bit_fallback or has_memory_optimization
                }
                
                status = "âœ…" if script["32b_support"]["compatible"] else "âš ï¸"
                print(f"\n{status} {Path(script['path']).name}")
                print(f"  ç”¨é€”: {script['purpose']}")
                print(f"  32Bå¯¾å¿œ: {'ã‚ã‚Š' if has_32b_check else 'ãªã—'}")
                print(f"  4bitãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {'ã‚ã‚Š' if has_4bit_fallback else 'ãªã—'}")
                print(f"  ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–: {'ã‚ã‚Š' if has_memory_optimization else 'ãªã—'}")
            else:
                script["32b_support"] = {"exists": False}
                print(f"\nâŒ {Path(script['path']).name} - ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
        
        self.report["checks"].append({
            "name": "script_compatibility",
            "scripts": scripts_to_check
        })
        
        return scripts_to_check
    
    def check_gguf_conversion(self):
        """GGUFå¤‰æ›ã®äº’æ›æ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
        print("\n" + "="*60)
        print("5. GGUFå¤‰æ›äº’æ›æ€§ãƒã‚§ãƒƒã‚¯")
        print("="*60)
        
        # llama.cppã®å­˜åœ¨ç¢ºèª
        llama_cpp_path = Path("/workspace/llama.cpp")
        
        if llama_cpp_path.exists():
            print("âœ… llama.cpp: ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿")
            
            # convert_hf_to_gguf.pyã®ç¢ºèª
            convert_script = llama_cpp_path / "convert_hf_to_gguf.py"
            if convert_script.exists():
                print("âœ… convert_hf_to_gguf.py: å­˜åœ¨")
                
                # ã‚¹ã‚¯ãƒªãƒ—ãƒˆå†…å®¹ç¢ºèª
                with open(convert_script, 'r') as f:
                    content = f.read()[:1000]  # æœ€åˆã®1000æ–‡å­—
                
                # Qwenãƒ¢ãƒ‡ãƒ«ã®ã‚µãƒãƒ¼ãƒˆç¢ºèª
                supports_qwen = "qwen" in content.lower()
                print(f"  Qwenã‚µãƒãƒ¼ãƒˆ: {'ã‚ã‚Š' if supports_qwen else 'è¦ç¢ºèª'}")
            else:
                print("âŒ convert_hf_to_gguf.py: å­˜åœ¨ã—ãªã„")
            
            # llama-quantizeã®ç¢ºèª
            quantize_bin = llama_cpp_path / "build" / "bin" / "llama-quantize"
            if quantize_bin.exists():
                print("âœ… llama-quantize: ãƒ“ãƒ«ãƒ‰æ¸ˆã¿")
            else:
                print("âš ï¸ llama-quantize: æœªãƒ“ãƒ«ãƒ‰")
        else:
            print("âŒ llama.cpp: æœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«")
            self.report["recommendations"].append(
                "llama.cppã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå¿…è¦ã§ã™"
            )
        
        self.report["checks"].append({
            "name": "gguf_conversion",
            "llama_cpp_exists": llama_cpp_path.exists(),
            "convert_script_exists": (llama_cpp_path / "convert_hf_to_gguf.py").exists() if llama_cpp_path.exists() else False,
            "quantize_binary_exists": (llama_cpp_path / "build" / "bin" / "llama-quantize").exists() if llama_cpp_path.exists() else False
        })
    
    def estimate_processing_time(self):
        """å‡¦ç†æ™‚é–“ã‚’æ¨å®š"""
        print("\n" + "="*60)
        print("6. å‡¦ç†æ™‚é–“æ¨å®š")
        print("="*60)
        
        estimates = {
            "QLoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°": "2-4æ™‚é–“ (3ã‚¨ãƒãƒƒã‚¯)",
            "LoRAãƒãƒ¼ã‚¸ï¼ˆ4bitâ†’FP16ï¼‰": "30-60åˆ†",
            "GGUFå¤‰æ›ï¼ˆFP16ï¼‰": "20-30åˆ†",
            "é‡å­åŒ–ï¼ˆQ4_K_Mï¼‰": "15-20åˆ†",
            "Ollamaç™»éŒ²": "1-2åˆ†",
            "åˆè¨ˆ": "ç´„3-6æ™‚é–“"
        }
        
        for step, time in estimates.items():
            print(f"{step}: {time}")
        
        self.report["estimates"] = estimates
        
        return estimates
    
    def generate_report(self):
        """æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        print("\n" + "="*60)
        print("æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆ")
        print("="*60)
        
        # ç·åˆåˆ¤å®š
        all_checks_ok = True
        critical_issues = []
        
        # GPU ãƒ¡ãƒ¢ãƒªãƒã‚§ãƒƒã‚¯
        if self.report["memory_usage"]["system"]["total_gpu_memory_gb"] < 48:
            critical_issues.append("GPU VRAM < 48GB: QLoRAï¼ˆ4bitï¼‰å¿…é ˆ")
        
        # RAM ãƒã‚§ãƒƒã‚¯
        if self.report["memory_usage"]["system"]["available_ram_gb"] < 32:
            critical_issues.append("RAMç©ºã < 32GB: ãƒãƒ¼ã‚¸å‡¦ç†ãŒé…ã„å¯èƒ½æ€§")
        
        if critical_issues:
            print("\nâš ï¸ æ³¨æ„äº‹é …:")
            for issue in critical_issues:
                print(f"  - {issue}")
        else:
            print("\nâœ… å…¨ãƒã‚§ãƒƒã‚¯åˆæ ¼")
        
        print("\n[æ¨å¥¨è¨­å®š]")
        print("1. QLoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆ4bité‡å­åŒ–ï¼‰")
        print("2. ãƒãƒ¼ã‚¸æ™‚ã¯CPUå‡¦ç†ã‚’æ´»ç”¨")
        print("3. GGUFå¤‰æ›ã¯ååˆ†ãªãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ã‚’ç¢ºä¿")
        print("4. æœ€çµ‚çš„ã«Q4_K_Må½¢å¼ã§é‡å­åŒ–")
        
        # ãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        report_path = Path("/workspace/outputs/32b_validation_report.json")
        report_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(report_path, 'w') as f:
            json.dump(self.report, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“„ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_path}")
        
        return self.report
    
    def run_validation(self):
        """å®Œå…¨ãªæ¤œè¨¼ã‚’å®Ÿè¡Œ"""
        print("="*60)
        print("DeepSeek-R1-Distill-Qwen-32B-Japanese é‡å­åŒ–æ¤œè¨¼")
        print("="*60)
        
        # å„ãƒã‚§ãƒƒã‚¯ã‚’å®Ÿè¡Œ
        self.check_system_resources()
        self.check_quantization_config()
        self.check_lora_workflow()
        self.check_script_compatibility()
        self.check_gguf_conversion()
        self.estimate_processing_time()
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        report = self.generate_report()
        
        print("\n" + "="*60)
        print("æ¤œè¨¼å®Œäº†")
        print("="*60)
        
        return report


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    validator = DeepSeek32BValidator()
    report = validator.run_validation()
    
    # çµè«–
    print("\nã€çµè«–ã€‘")
    print("DeepSeek-R1-Distill-Qwen-32B-Japaneseã®é‡å­åŒ–ã¯ä»¥ä¸‹ã®æ¡ä»¶ã§å¯èƒ½:")
    print("1. QLoRAï¼ˆ4bitï¼‰ã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° âœ…")
    print("2. ãƒãƒ¼ã‚¸æ™‚ã«4bité‡å­åŒ–ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨ âœ…")
    print("3. FP16ã§ä¸€æ™‚ä¿å­˜å¾Œã€GGUFå¤‰æ› âœ…")
    print("4. Q4_K_Må½¢å¼ã§æœ€çµ‚é‡å­åŒ– âœ…")
    print("\nç¾åœ¨ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯32Bãƒ¢ãƒ‡ãƒ«ã«å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚")


if __name__ == "__main__":
    main()