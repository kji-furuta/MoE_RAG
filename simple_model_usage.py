#!/usr/bin/env python3
"""
ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ç°¡å˜ãªåˆ©ç”¨ä¾‹
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

def simple_usage_example():
    print("ğŸ¯ ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ç°¡å˜åˆ©ç”¨ä¾‹")
    print("=" * 50)
    
    # ãƒ‘ã‚¹è¨­å®š
    base_model_name = "distilgpt2"
    adapter_path = "/workspace/lora_demo_20250626_074248"
    
    # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
    print("ğŸ“š ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­...")
    tokenizer = AutoTokenizer.from_pretrained(base_model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_name,
        torch_dtype=torch.float16,
        device_map="auto"
    )
    
    # LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼èª­ã¿è¾¼ã¿
    model = PeftModel.from_pretrained(base_model, adapter_path)
    print("âœ… èª­ã¿è¾¼ã¿å®Œäº†")
    
    # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
    def generate_text(prompt, max_length=50):
        inputs = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True).to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                inputs['input_ids'],
                attention_mask=inputs['attention_mask'],
                max_new_tokens=max_length,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                repetition_penalty=1.1,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id
            )
        
        return tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # å®Ÿè¡Œä¾‹
    prompts = [
        "Hello, how are you",
        "The weather today is",
        "I think that",
        "Technology is",
        "In the future"
    ]
    
    print("\nğŸ§ª ç”Ÿæˆãƒ†ã‚¹ãƒˆ:")
    for prompt in prompts:
        result = generate_text(prompt, 30)
        print(f"å…¥åŠ›: {prompt}")
        print(f"å‡ºåŠ›: {result}")
        print("-" * 40)

def batch_generation_example():
    """ãƒãƒƒãƒç”Ÿæˆã®ä¾‹"""
    print("\nğŸ“¦ ãƒãƒƒãƒç”Ÿæˆä¾‹")
    print("=" * 30)
    
    base_model_name = "distilgpt2"
    adapter_path = "/workspace/lora_demo_20250626_074248"
    
    tokenizer = AutoTokenizer.from_pretrained(base_model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_name,
        torch_dtype=torch.float16,
        device_map="auto"
    )
    
    model = PeftModel.from_pretrained(base_model, adapter_path)
    
    # è¤‡æ•°ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä¸€åº¦ã«å‡¦ç†
    prompts = [
        "Hello",
        "Good morning",
        "Thank you"
    ]
    
    print("ğŸš€ è¤‡æ•°ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆåŒæ™‚å‡¦ç†:")
    for i, prompt in enumerate(prompts):
        inputs = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True).to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                inputs['input_ids'],
                attention_mask=inputs['attention_mask'],
                max_new_tokens=20,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                repetition_penalty=1.1,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id
            )
        
        result = tokenizer.decode(outputs[0], skip_special_tokens=True)
        print(f"{i+1}. {prompt} â†’ {result}")

if __name__ == "__main__":
    simple_usage_example()
    batch_generation_example()
    
    print("\n" + "=" * 50)
    print("ğŸ’¡ å®Ÿç”¨åŒ–ã®ãƒã‚¤ãƒ³ãƒˆ")
    print("=" * 50)
    print("1. ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã¯ç´„6.2MBï¼ˆä¸»ã«ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ï¼‰")
    print("2. LoRAé‡ã¿ã¯1.6MBã®ã¿ï¼ˆè»½é‡ï¼ï¼‰")
    print("3. GPUä½¿ç”¨: RTX A5000 x2ã§è‡ªå‹•æœ€é©åŒ–")
    print("4. æ¨è«–é€Ÿåº¦: é«˜é€Ÿï¼ˆFP16 + ãƒãƒ«ãƒGPUï¼‰")
    print("5. ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: ç´„2-4GB")
    
    print("\nğŸš€ æœ¬ç•ªé‹ç”¨ã§ã®æ´»ç”¨:")
    print("â€¢ WebAPIã‚µãƒ¼ãƒãƒ¼ã«çµ„ã¿è¾¼ã¿")
    print("â€¢ ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã®ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰")
    print("â€¢ ãƒãƒƒãƒå‡¦ç†ã§ã®å¤§é‡ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ")
    print("â€¢ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å¯¾è©±ã‚·ã‚¹ãƒ†ãƒ ")