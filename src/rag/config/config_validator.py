"""
è¨­å®šãƒãƒªãƒ‡ãƒ¼ã‚¿ãƒ¼
è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ã¨è‡ªå‹•ä¿®æ­£
"""

import logging
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass

from .rag_config import RAGConfig
from .model_path_resolver import ModelPathResolver

logger = logging.getLogger(__name__)


@dataclass
class ValidationIssue:
    """æ¤œè¨¼å•é¡Œ"""
    level: str  # error, warning, info
    component: str  # llm, embedding, vector_store, etc.
    message: str
    suggested_fix: Optional[str] = None
    auto_fixable: bool = False


class ConfigValidator:
    """è¨­å®šãƒãƒªãƒ‡ãƒ¼ã‚¿ãƒ¼"""
    
    def __init__(self, config: RAGConfig):
        """
        Args:
            config: æ¤œè¨¼å¯¾è±¡ã®è¨­å®š
        """
        self.config = config
        self.issues: List[ValidationIssue] = []
        
    def validate_all(self) -> List[ValidationIssue]:
        """å…¨è¨­å®šã‚’æ¤œè¨¼"""
        
        self.issues.clear()
        
        # å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’æ¤œè¨¼
        self._validate_llm_config()
        self._validate_embedding_config()
        self._validate_vector_store_config()
        self._validate_document_processing_config()
        self._validate_retrieval_config()
        
        return self.issues
        
    def _validate_llm_config(self):
        """LLMè¨­å®šã®æ¤œè¨¼"""
        
        llm_config = self.config.llm
        
        # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹æ¤œè¨¼
        if llm_config.use_finetuned:
            path_issue = self._validate_model_path(
                llm_config.finetuned_model_path,
                "finetuned_model_path"
            )
            if path_issue:
                self.issues.append(path_issue)
                
        # ãƒ¡ãƒ¢ãƒªè¨­å®šã®æ¤œè¨¼
        if llm_config.max_memory:
            total_memory = sum(
                int(mem.replace('GB', '')) 
                for mem in llm_config.max_memory.values() 
                if isinstance(mem, str) and 'GB' in mem
            )
            
            if total_memory > 128:  # 128GBä»¥ä¸Šã¯è­¦å‘Š
                self.issues.append(ValidationIssue(
                    level="warning",
                    component="llm",
                    message=f"Large memory configuration: {total_memory}GB",
                    suggested_fix="Consider reducing max_memory if you don't have enough VRAM"
                ))
                
        # ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ¤œè¨¼
        if not 0.0 <= llm_config.temperature <= 2.0:
            self.issues.append(ValidationIssue(
                level="error",
                component="llm", 
                message=f"Invalid temperature: {llm_config.temperature}",
                suggested_fix="Set temperature between 0.0 and 2.0",
                auto_fixable=True
            ))
            
    def _validate_model_path(self, model_path: str, config_key: str) -> Optional[ValidationIssue]:
        """ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã®æ¤œè¨¼"""
        
        resolver = ModelPathResolver()
        validation = resolver.validate_model_path(model_path)
        
        if not validation['is_valid']:
            # ä»£æ›¿ãƒ¢ãƒ‡ãƒ«ã‚’æ¢ã™
            alternative = resolver.find_latest_model()
            
            if alternative:
                return ValidationIssue(
                    level="warning",
                    component="llm",
                    message=f"Invalid model path: {model_path}. Issues: {', '.join(validation['issues'])}",
                    suggested_fix=f"Use detected model: {alternative}",
                    auto_fixable=True
                )
            else:
                return ValidationIssue(
                    level="error",
                    component="llm",
                    message=f"Invalid model path: {model_path}. No alternative models found.",
                    suggested_fix="Train a model first or set use_finetuned to false"
                )
                
        return None
        
    def _validate_embedding_config(self):
        """åŸ‹ã‚è¾¼ã¿è¨­å®šã®æ¤œè¨¼"""
        
        embedding_config = self.config.embedding
        
        # ãƒãƒƒãƒã‚µã‚¤ã‚ºã®æ¤œè¨¼
        if embedding_config.batch_size <= 0:
            self.issues.append(ValidationIssue(
                level="error",
                component="embedding",
                message=f"Invalid batch_size: {embedding_config.batch_size}",
                suggested_fix="Set batch_size to a positive integer",
                auto_fixable=True
            ))
            
        # æœ€å¤§é•·ã®æ¤œè¨¼
        if not 128 <= embedding_config.max_length <= 2048:
            self.issues.append(ValidationIssue(
                level="warning",
                component="embedding",
                message=f"Unusual max_length: {embedding_config.max_length}",
                suggested_fix="Consider using 512 for most cases"
            ))
            
    def _validate_vector_store_config(self):
        """ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢è¨­å®šã®æ¤œè¨¼"""
        
        vs_config = self.config.vector_store
        
        # ãƒ‘ã‚¹ã®æ¤œè¨¼
        vs_path = Path(vs_config.path)
        parent_dir = vs_path.parent
        
        if not parent_dir.exists():
            self.issues.append(ValidationIssue(
                level="warning",
                component="vector_store",
                message=f"Parent directory does not exist: {parent_dir}",
                suggested_fix="Parent directory will be created automatically"
            ))
            
    def _validate_document_processing_config(self):
        """æ–‡æ›¸å‡¦ç†è¨­å®šã®æ¤œè¨¼"""
        
        doc_config = self.config.document_processing
        
        # DPIè¨­å®šã®æ¤œè¨¼
        if not 150 <= doc_config.dpi <= 600:
            self.issues.append(ValidationIssue(
                level="warning",
                component="document_processing",
                message=f"Unusual DPI setting: {doc_config.dpi}",
                suggested_fix="Consider using 300 DPI for good quality"
            ))
            
    def _validate_retrieval_config(self):
        """æ¤œç´¢è¨­å®šã®æ¤œè¨¼"""
        
        retrieval_config = self.config.retrieval
        
        # é‡ã¿ã®æ¤œè¨¼
        total_weight = retrieval_config.vector_weight + retrieval_config.keyword_weight
        
        if abs(total_weight - 1.0) > 0.01:  # 1.0ã‹ã‚‰0.01ä»¥ä¸Šé›¢ã‚Œã¦ã„ã‚‹
            self.issues.append(ValidationIssue(
                level="warning",
                component="retrieval",
                message=f"Vector and keyword weights don't sum to 1.0: {total_weight}",
                suggested_fix="Adjust weights to sum to 1.0",
                auto_fixable=True
            ))
            
        # top_kè¨­å®šã®æ¤œè¨¼
        if retrieval_config.top_k <= 0:
            self.issues.append(ValidationIssue(
                level="error",
                component="retrieval",
                message=f"Invalid top_k: {retrieval_config.top_k}",
                suggested_fix="Set top_k to a positive integer",
                auto_fixable=True
            ))
            
    def auto_fix_issues(self) -> int:
        """è‡ªå‹•ä¿®æ­£å¯èƒ½ãªå•é¡Œã‚’ä¿®æ­£"""
        
        fixed_count = 0
        
        for issue in self.issues:
            if not issue.auto_fixable:
                continue
                
            try:
                if issue.component == "llm":
                    if "temperature" in issue.message:
                        # æ¸©åº¦ã®ä¿®æ­£
                        self.config.llm.temperature = max(0.0, min(2.0, self.config.llm.temperature))
                        fixed_count += 1
                        logger.info(f"Auto-fixed: {issue.message}")
                        
                    elif "model path" in issue.message.lower():
                        # ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã®ä¿®æ­£
                        resolver = ModelPathResolver()
                        alternative = resolver.find_latest_model()
                        if alternative:
                            self.config.llm.finetuned_model_path = alternative
                            fixed_count += 1
                            logger.info(f"Auto-fixed model path: {alternative}")
                            
                elif issue.component == "embedding":
                    if "batch_size" in issue.message:
                        # ãƒãƒƒãƒã‚µã‚¤ã‚ºã®ä¿®æ­£
                        self.config.embedding.batch_size = max(1, self.config.embedding.batch_size)
                        fixed_count += 1
                        
                elif issue.component == "retrieval":
                    if "weights" in issue.message:
                        # é‡ã¿ã®æ­£è¦åŒ–
                        total = self.config.retrieval.vector_weight + self.config.retrieval.keyword_weight
                        if total > 0:
                            self.config.retrieval.vector_weight /= total
                            self.config.retrieval.keyword_weight /= total
                            fixed_count += 1
                            
                    elif "top_k" in issue.message:
                        # top_kã®ä¿®æ­£
                        self.config.retrieval.top_k = max(1, self.config.retrieval.top_k)
                        fixed_count += 1
                        
            except Exception as e:
                logger.warning(f"Failed to auto-fix issue: {issue.message} - {e}")
                
        return fixed_count
        
    def generate_report(self) -> str:
        """æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        
        if not self.issues:
            return "âœ… è¨­å®šã«å•é¡Œã¯ã‚ã‚Šã¾ã›ã‚“"
            
        report_lines = ["ğŸ” è¨­å®šæ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆ", "=" * 50]
        
        # ãƒ¬ãƒ™ãƒ«åˆ¥ã«é›†è¨ˆ
        errors = [i for i in self.issues if i.level == "error"]
        warnings = [i for i in self.issues if i.level == "warning"]
        infos = [i for i in self.issues if i.level == "info"]
        
        # ã‚µãƒãƒªãƒ¼
        report_lines.append(f"ã‚¨ãƒ©ãƒ¼: {len(errors)}ä»¶")
        report_lines.append(f"è­¦å‘Š: {len(warnings)}ä»¶") 
        report_lines.append(f"æƒ…å ±: {len(infos)}ä»¶")
        report_lines.append("")
        
        # è©³ç´°
        for level_name, issues in [("ã‚¨ãƒ©ãƒ¼", errors), ("è­¦å‘Š", warnings), ("æƒ…å ±", infos)]:
            if not issues:
                continue
                
            report_lines.append(f"ã€{level_name}ã€‘")
            for issue in issues:
                icon = "âŒ" if issue.level == "error" else "âš ï¸" if issue.level == "warning" else "â„¹ï¸"
                report_lines.append(f"{icon} [{issue.component}] {issue.message}")
                
                if issue.suggested_fix:
                    fix_icon = "ğŸ”§" if issue.auto_fixable else "ğŸ’¡"
                    report_lines.append(f"   {fix_icon} {issue.suggested_fix}")
                    
                report_lines.append("")
                
        return "\n".join(report_lines)


def validate_config(config: RAGConfig, auto_fix: bool = True) -> Tuple[List[ValidationIssue], int]:
    """è¨­å®šã‚’æ¤œè¨¼ï¼ˆä¾¿åˆ©é–¢æ•°ï¼‰
    
    Args:
        config: æ¤œè¨¼å¯¾è±¡ã®è¨­å®š
        auto_fix: è‡ªå‹•ä¿®æ­£ã‚’å®Ÿè¡Œã™ã‚‹ã‹
        
    Returns:
        (æ¤œè¨¼å•é¡Œã®ãƒªã‚¹ãƒˆ, ä¿®æ­£ã•ã‚ŒãŸå•é¡Œã®æ•°)
    """
    
    validator = ConfigValidator(config)
    issues = validator.validate_all()
    
    fixed_count = 0
    if auto_fix:
        fixed_count = validator.auto_fix_issues()
        
    return issues, fixed_count


def print_validation_report(config: RAGConfig, auto_fix: bool = True):
    """æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆã‚’å‡ºåŠ›ï¼ˆä¾¿åˆ©é–¢æ•°ï¼‰"""
    
    validator = ConfigValidator(config)
    issues = validator.validate_all()
    
    if auto_fix:
        fixed_count = validator.auto_fix_issues()
        if fixed_count > 0:
            print(f"ğŸ”§ {fixed_count}ä»¶ã®å•é¡Œã‚’è‡ªå‹•ä¿®æ­£ã—ã¾ã—ãŸ")
            
        # ä¿®æ­£å¾Œã«å†æ¤œè¨¼
        issues = validator.validate_all()
        
    print(validator.generate_report())