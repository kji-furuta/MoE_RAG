#!/usr/bin/env python3
"""
ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import sys
import torch
import logging
from pathlib import Path

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def test_training_imports():
    """ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–¢é€£ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ†ã‚¹ãƒˆ"""
    print("\n=== Import Test ===")
    
    try:
        from src.training.training_utils import TrainingConfig, TextDataset
        from src.training.full_finetuning import FullFinetuningTrainer
        from src.training.lora_finetuning import LoRAFinetuningTrainer, LoRAConfig
        from src.training.quantization import QuantizationOptimizer
        from src.models.japanese_model import JapaneseModel
        print("âœ“ All training modules imported successfully")
        return True
    except Exception as e:
        print(f"âœ— Import failed: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_lora_config():
    """LoRAè¨­å®šã®ãƒ†ã‚¹ãƒˆ"""
    print("\n=== LoRA Config Test ===")
    
    try:
        from src.training.lora_finetuning import LoRAConfig
        
        # æ¨™æº–çš„ãªLoRAè¨­å®š
        lora_config = LoRAConfig(
            r=16,
            lora_alpha=32,
            target_modules=["q_proj", "v_proj"],
            lora_dropout=0.05,
            use_qlora=False
        )
        
        print(f"âœ“ LoRA Config created:")
        print(f"  - Rank: {lora_config.r}")
        print(f"  - Alpha: {lora_config.lora_alpha}")
        print(f"  - Target modules: {lora_config.target_modules}")
        print(f"  - QLoRA: {lora_config.use_qlora}")
        
        # QLoRAè¨­å®š
        qlora_config = LoRAConfig(
            r=8,
            lora_alpha=16,
            use_qlora=True,
            qlora_4bit=True
        )
        
        print(f"\nâœ“ QLoRA Config created:")
        print(f"  - 4-bit quantization: {qlora_config.qlora_4bit}")
        
        return True
    except Exception as e:
        print(f"âœ— LoRA config test failed: {e}")
        return False


def test_training_config():
    """ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®šã®ãƒ†ã‚¹ãƒˆ"""
    print("\n=== Training Config Test ===")
    
    try:
        from src.training.training_utils import TrainingConfig
        
        config = TrainingConfig(
            learning_rate=2e-5,
            batch_size=2,  # å°ã•ã„ãƒãƒƒãƒã‚µã‚¤ã‚ºã§ãƒ†ã‚¹ãƒˆ
            gradient_accumulation_steps=4,
            num_epochs=1,
            output_dir="./test_outputs",
            fp16=True,
            gradient_checkpointing=True
        )
        
        print(f"âœ“ Training Config created:")
        print(f"  - Learning rate: {config.learning_rate}")
        print(f"  - Batch size: {config.batch_size}")
        print(f"  - Gradient accumulation: {config.gradient_accumulation_steps}")
        print(f"  - FP16: {config.fp16}")
        print(f"  - Gradient checkpointing: {config.gradient_checkpointing}")
        
        return True
    except Exception as e:
        print(f"âœ— Training config test failed: {e}")
        return False


def test_dataset_creation():
    """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆã®ãƒ†ã‚¹ãƒˆ"""
    print("\n=== Dataset Test ===")
    
    try:
        from src.training.training_utils import TextDataset
        from transformers import AutoTokenizer
        
        # ç°¡å˜ãªãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã§ãƒ†ã‚¹ãƒˆ
        tokenizer = AutoTokenizer.from_pretrained("gpt2")
        tokenizer.pad_token = tokenizer.eos_token
        
        # ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆ
        sample_texts = [
            "ã“ã‚Œã¯ãƒ†ã‚¹ãƒˆãƒ†ã‚­ã‚¹ãƒˆã§ã™ã€‚",
            "æ—¥æœ¬èªã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ãƒ†ã‚¹ãƒˆã—ã¦ã„ã¾ã™ã€‚",
            "ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãŒã†ã¾ãã„ãã“ã¨ã‚’é¡˜ã£ã¦ã„ã¾ã™ã€‚"
        ]
        
        dataset = TextDataset(sample_texts, tokenizer, max_length=128)
        
        print(f"âœ“ Dataset created with {len(dataset)} samples")
        
        # ã‚µãƒ³ãƒ—ãƒ«ã‚¢ã‚¤ãƒ†ãƒ ã‚’ãƒ†ã‚¹ãƒˆ
        sample = dataset[0]
        print(f"  - Input IDs shape: {sample['input_ids'].shape}")
        print(f"  - Attention mask shape: {sample['attention_mask'].shape}")
        print(f"  - Labels shape: {sample['labels'].shape}")
        
        return True
    except Exception as e:
        print(f"âœ— Dataset test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_quantization_optimizer():
    """é‡å­åŒ–ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã®ãƒ†ã‚¹ãƒˆ"""
    print("\n=== Quantization Optimizer Test ===")
    
    try:
        from src.training.quantization import QuantizationOptimizer
        
        # å°ã•ã„ãƒ¢ãƒ‡ãƒ«ã§ãƒ†ã‚¹ãƒˆ
        optimizer = QuantizationOptimizer("gpt2")
        
        print("âœ“ QuantizationOptimizer created")
        print(f"  - Model: {optimizer.model_name_or_path}")
        print(f"  - Device: {optimizer.device}")
        
        return True
    except Exception as e:
        print(f"âœ— Quantization optimizer test failed: {e}")
        return False


def test_model_preparation():
    """ãƒ¢ãƒ‡ãƒ«æº–å‚™ã®ãƒ†ã‚¹ãƒˆ"""
    print("\n=== Model Preparation Test ===")
    
    try:
        from src.models.japanese_model import JapaneseModel
        from src.training.full_finetuning import FullFinetuningTrainer
        from src.training.training_utils import TrainingConfig
        
        # å°ã•ã„ãƒ¢ãƒ‡ãƒ«ã§ãƒ†ã‚¹ãƒˆ
        model = JapaneseModel(
            model_name="cyberagent/open-calm-small",
            load_in_8bit=True
        )
        
        config = TrainingConfig(
            batch_size=1,
            num_epochs=1,
            output_dir="./test_outputs"
        )
        
        trainer = FullFinetuningTrainer(
            model=model,
            config=config,
            use_accelerate=True
        )
        
        print("âœ“ FullFinetuningTrainer created")
        print(f"  - Model: {model.model_name}")
        print(f"  - Device: {trainer.device}")
        
        return True
    except Exception as e:
        print(f"âœ— Model preparation test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_gpu_setup():
    """GPUè¨­å®šã®ãƒ†ã‚¹ãƒˆ"""
    print("\n=== GPU Setup Test ===")
    
    print(f"PyTorch version: {torch.__version__}")
    print(f"CUDA available: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        
        # ãƒ¡ãƒ¢ãƒªãƒ†ã‚¹ãƒˆ
        x = torch.randn(1000, 1000, device='cuda')
        print(f"GPU memory test: âœ“ (allocated {x.element_size() * x.numel() / 1024**2:.1f} MB)")
        del x
        torch.cuda.empty_cache()
    else:
        print("No GPU available - CPU mode")
    
    return True


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print("ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆ")
    print("=" * 60)
    
    tests = [
        ("GPU Setup", test_gpu_setup),
        ("Imports", test_training_imports),
        ("Training Config", test_training_config),
        ("LoRA Config", test_lora_config),
        ("Dataset Creation", test_dataset_creation),
        ("Quantization Optimizer", test_quantization_optimizer),
        ("Model Preparation", test_model_preparation)
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        try:
            success = test_func()
            if success:
                passed += 1
                print(f"\nâœ“ {test_name} test passed")
            else:
                print(f"\nâœ— {test_name} test failed")
        except Exception as e:
            print(f"\nâœ— {test_name} test failed with exception: {e}")
    
    print(f"\n" + "=" * 60)
    print(f"Test Results: {passed}/{total} tests passed")
    
    if passed == total:
        print("\nğŸ‰ All tests passed! ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ©Ÿèƒ½ã®å®Ÿè£…ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
        print("\nä½¿ç”¨å¯èƒ½ãªæ©Ÿèƒ½:")
        print("- ãƒ•ãƒ«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° (å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°)")
        print("- LoRA ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° (ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠ¹ç‡çš„)")
        print("- QLoRA ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° (4bit/8bité‡å­åŒ–)")
        print("- ãƒãƒ«ãƒGPUå¯¾å¿œ (DataParallel/DistributedDataParallel)")
        print("- é‡å­åŒ–æœ€é©åŒ– (æ¨è«–é€Ÿåº¦å‘ä¸Š)")
    else:
        print(f"\nâš ï¸  {total - passed} tests failed. å®Ÿè£…ã«å•é¡ŒãŒã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
    
    return passed == total


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)