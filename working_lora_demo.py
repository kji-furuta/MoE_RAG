#!/usr/bin/env python3
"""
å‹•ä½œç¢ºèªæ¸ˆã¿LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¢
RTX A5000 x2ç’°å¢ƒ
"""

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model, TaskType
import os
from datetime import datetime

# WandBã‚’ç„¡åŠ¹åŒ–
os.environ["WANDB_DISABLED"] = "true"

class SimpleTextDataset(Dataset):
    def __init__(self, texts, tokenizer, max_length=128):
        self.encodings = []
        for text in texts:
            encoding = tokenizer(
                text,
                truncation=True,
                padding='max_length',
                max_length=max_length,
                return_tensors='pt'
            )
            self.encodings.append({
                'input_ids': encoding['input_ids'].flatten(),
                'attention_mask': encoding['attention_mask'].flatten(),
                'labels': encoding['input_ids'].flatten()
            })
    
    def __len__(self):
        return len(self.encodings)
    
    def __getitem__(self, idx):
        return self.encodings[idx]

def main():
    print("ğŸš€ RTX A5000 x2 å‹•ä½œç¢ºèªæ¸ˆã¿LoRAãƒ‡ãƒ¢")
    print("=" * 50)
    
    # GPUç¢ºèª
    if torch.cuda.is_available():
        device_count = torch.cuda.device_count()
        print(f"âœ… GPU: {device_count}å°åˆ©ç”¨å¯èƒ½")
        
        for i in range(device_count):
            props = torch.cuda.get_device_properties(i)
            memory_gb = props.total_memory / (1024**3)
            print(f"   GPU {i}: {props.name} ({memory_gb:.1f}GB)")
    
    # Step 1: ãƒ¢ãƒ‡ãƒ«æº–å‚™
    print("\nğŸ“š Step 1: ãƒ¢ãƒ‡ãƒ«æº–å‚™")
    model_name = "distilgpt2"
    print(f"ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {model_name}")
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        print("âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")
        
    except Exception as e:
        print(f"âŒ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return False
    
    # Step 2: LoRAè¨­å®š
    print("\nâš™ï¸ Step 2: LoRAè¨­å®š")
    peft_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        inference_mode=False,
        r=8,
        lora_alpha=16,
        lora_dropout=0.1,
        target_modules=["c_attn", "c_proj"]
    )
    
    model = get_peft_model(model, peft_config)
    print("âœ… LoRAé©ç”¨å®Œäº†")
    model.print_trainable_parameters()
    
    # Step 3: è¨“ç·´ãƒ‡ãƒ¼ã‚¿
    print("\nğŸ“ Step 3: è¨“ç·´ãƒ‡ãƒ¼ã‚¿æº–å‚™")
    train_texts = [
        "Hello, how are you today?",
        "The weather is beautiful.",
        "I love machine learning.",
        "RTX A5000 is powerful.",
        "Fine-tuning is exciting."
    ]
    
    dataset = SimpleTextDataset(train_texts, tokenizer)
    print(f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {len(train_texts)}ä»¶")
    
    # Step 4: è¨“ç·´è¨­å®š
    print("\nğŸ“Š Step 4: è¨“ç·´è¨­å®š")
    output_dir = f"./lora_demo_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=2,
        per_device_train_batch_size=1,
        gradient_accumulation_steps=4,
        warmup_steps=5,
        learning_rate=3e-4,
        fp16=True,
        logging_steps=1,
        save_steps=100,
        save_total_limit=1,
        prediction_loss_only=True,
        remove_unused_columns=False,
        report_to=[],  # WandBç„¡åŠ¹åŒ–
    )
    
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,
    )
    
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=dataset,
        data_collator=data_collator,
    )
    
    print("âœ… ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼æº–å‚™å®Œäº†")
    
    # Step 5: è¨“ç·´å‰ãƒ†ã‚¹ãƒˆ
    print("\nğŸ§ª Step 5: è¨“ç·´å‰ãƒ†ã‚¹ãƒˆ")
    test_input = "Hello, how are"
    inputs = tokenizer(test_input, return_tensors="pt").to(model.device)
    
    model.eval()
    with torch.no_grad():
        outputs = model.generate(
            inputs['input_ids'],
            max_new_tokens=15,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id
        )
    
    before_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(f"å…¥åŠ›: {test_input}")
    print(f"è¨“ç·´å‰å‡ºåŠ›: {before_text}")
    
    # Step 6: è¨“ç·´å®Ÿè¡Œ
    print("\nğŸ‹ï¸ Step 6: è¨“ç·´å®Ÿè¡Œ")
    print("è¨“ç·´é–‹å§‹...")
    
    try:
        trainer.train()
        print("âœ… è¨“ç·´å®Œäº†ï¼")
        
    except Exception as e:
        print(f"âŒ è¨“ç·´ã‚¨ãƒ©ãƒ¼: {e}")
        return False
    
    # Step 7: è¨“ç·´å¾Œãƒ†ã‚¹ãƒˆ
    print("\nğŸ¯ Step 7: è¨“ç·´å¾Œãƒ†ã‚¹ãƒˆ")
    model.eval()
    with torch.no_grad():
        outputs = model.generate(
            inputs['input_ids'],
            max_new_tokens=15,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id
        )
    
    after_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(f"å…¥åŠ›: {test_input}")
    print(f"è¨“ç·´å¾Œå‡ºåŠ›: {after_text}")
    
    # Step 8: ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    print("\nğŸ’¾ Step 8: ãƒ¢ãƒ‡ãƒ«ä¿å­˜")
    try:
        model.save_pretrained(output_dir)
        tokenizer.save_pretrained(output_dir)
        print(f"âœ… ä¿å­˜å®Œäº†: {output_dir}")
        
    except Exception as e:
        print(f"âŒ ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    print("\nğŸ‰ LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†ï¼")
    
    return True

def show_summary():
    """å®Œäº†å¾Œã®è¦ç´„"""
    print("\n" + "=" * 60)
    print("ğŸ“‹ å®Ÿè¡Œå®Œäº† - ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ‰‹é †ã¾ã¨ã‚")
    print("=" * 60)
    
    print("\nâœ… ä»Šå›å®Ÿè¡Œã—ãŸå†…å®¹:")
    print("â€¢ DistilGPT-2ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿")
    print("â€¢ LoRA (Low-Rank Adaptation) ã®é©ç”¨")
    print("â€¢ å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®è¨“ç·´")
    print("â€¢ è¨“ç·´å‰å¾Œã®ç”Ÿæˆãƒ†ã‚¹ãƒˆ")
    print("â€¢ ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜")
    
    print("\nğŸš€ RTX A5000 x2ã§ã®æœ€é©åŒ–ãƒã‚¤ãƒ³ãƒˆ:")
    print("â€¢ device_map='auto' ã§ãƒãƒ«ãƒGPUæ´»ç”¨")
    print("â€¢ fp16=True ã§ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–")
    print("â€¢ gradient_accumulation_steps ã§ãƒãƒƒãƒã‚µã‚¤ã‚ºèª¿æ•´")
    print("â€¢ LoRAã§è¨“ç·´å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’0.5%ã«å‰Šæ¸›")
    
    print("\nğŸ¯ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
    print("1. ã‚ˆã‚Šå¤§ããªãƒ¢ãƒ‡ãƒ« (3B-8B) ã«æŒ‘æˆ¦")
    print("2. æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°")
    print("3. QLoRA ã§13B+ãƒ¢ãƒ‡ãƒ«ã‚’è©¦ã™")
    print("4. ç‹¬è‡ªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®å­¦ç¿’")
    
    print("\nğŸ’¡ å®Ÿç”¨çš„ãªå¿œç”¨ä¾‹:")
    print("â€¢ Q&Aã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰")
    print("â€¢ å¯¾è©±ã‚·ã‚¹ãƒ†ãƒ ã®ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º")
    print("â€¢ æ–‡æ›¸è¦ç´„ã‚·ã‚¹ãƒ†ãƒ ")
    print("â€¢ ç‰¹å®šãƒ‰ãƒ¡ã‚¤ãƒ³ã®è¨€èªãƒ¢ãƒ‡ãƒ«")

if __name__ == "__main__":
    try:
        success = main()
        if success:
            show_summary()
    except Exception as e:
        print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()